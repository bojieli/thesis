\section{Related Work}
\label{kvdirect:sec:related}

Driven by performance, the research and development of distributed key-value storage systems, an important infrastructure, has received considerable attention. A large number of distributed key-value storages are based on CPU. To reduce computational costs, Masstree~\cite{mao2012cache}, MemC3~\cite{fan2013memc3}, and libcuckoo~\cite{li2014algorithmic} optimize locks, caches, hashes, and memory allocation algorithms, while \oursys{} comes with a new hash table and memory management mechanism specifically designed for FPGA to minimize PCIe traffic. MICA~\cite{lim2014mica} partitions the hash table to each core, thus completely avoiding synchronization. However, this method introduces core imbalance for skewed workloads.

To get rid of the overhead of the operating system kernel, Netmap~\cite{rizzo2012netmap} and DPDK~\cite{intel2014data} directly poll network packets from network cards, while mTCP~\cite{jeong2014mtcp} and SandStorm~\cite{marinos2014network} use user-mode lightweight network stacks to handle these packets. Key-value storage systems \cite{kapoor2012chronos,ousterhout2010case,ousterhout2015ramcloud,lim2014mica,li2016full} benefit from such high-performance optimization. As another step in this direction, recent works \cite{infiniband2000infiniband,kalia2014using,kalia2016design,kalia2014using,kalia2016design} leverage the hardware-based network stack of RDMA network cards, using two-sided RDMA as the RPC mechanism between key-value storage clients and servers to further increase per-core throughput and reduce latency. Nevertheless, these systems are still CPU-bound (\S \ref{kvdirect:sec:state-of-the-art-kvs}).

A different approach is to utilize one-sided RDMA. Pilaf~\cite{mitchell2013using} and FaRM~\cite{dragojevic2014farm} adopt one-sided RDMA reads for GET operations, with FaRM achieving network-saturating throughput. Nessie~\cite{szepesi2014designing}, DrTM~\cite{wei2015fast}, DrTM+R~\cite{chen2016fast}, and FaSST~\cite{kalia2016fasst} utilize distributed transactions to implement one-sided RDMA GET and PUT. However, the performance of PUT operations is inevitably affected by the synchronization overhead of consistency guarantees, and is limited by RDMA primitives \cite{kalia2016design}. In addition, the client CPU is involved in key-value processing, limiting the throughput per core to about 10~Mops on the client side. In contrast, \oursys{} extends RDMA primitives to key-value operations, ensuring server-side consistency, making key-value storage clients completely transparent, and achieving high throughput and low latency, even for PUT operations.

As a flexible and customizable hardware, FPGA is now widely deployed at data center scale \cite{putnam2014reconfigurable,caulfield2016cloud}, and significant improvements have been made for programmability \cite{bacon2013fpga,li2016clicknp}. Some early works have explored building key-value storage systems on FPGA. However, some of them only use on-chip data storage (about a few MB of memory) \cite{liang16fpl} or on-board DRAM (e.g., 8 GB of memory) \cite{istvan2013flexible,chalamalasetti2013fpga,istvan2015hash}, thus the storage capacity is limited. The work \cite{blott2015scaling} focuses on increasing system capacity rather than throughput, and uses SSD as a secondary storage for on-board DRAM. The work \cite{liang16fpl,chalamalasetti2013fpga} can only store fixed-size key-value pairs, such key-value storage systems can only be used for some specific applications, and are not general enough. The work \cite{blott13hotcloud,lavasani2014fpga} uses host DRAM to store hash tables, and the work \cite{tokusashi2016multilevel} uses network card DRAM as a cache for host DRAM, but they do not optimize for network and PCIe DMA bandwidth, resulting in poor performance. \oursys{} makes full use of network card DRAM and host DRAM, making FPGA-based key-value storage systems general and capable of large-scale deployment. In addition, careful hardware and software co-design, as well as optimization for PCIe and network, push the performance of this paper to the physical limit.

The use of programmable switches supporting P4 \cite{bosshart2014p4} to accelerate key-value storage systems has been a hot research topic in recent years \cite{barefoot-tofino}. SwitchKV~\cite{li2016fast} uses content-based routing to route requests to backend nodes based on cached keys, while NetCache~\cite{netcache-sosp17} further caches frequently accessed key-values in the switch. NetChain~\cite{jin2018netchain} implements a highly consistent, fault-tolerant key-value store in network switches.

Secondary indexing in data storage systems, which allows data retrieval using keys other than the primary key, is an important feature \cite{escriva2012hyperdex,kejriwal2016slik}. SLIK~\cite{kejriwal2016slik} supports multiple secondary keys in key-value storage systems using the B+ tree algorithm. Exploring how to support secondary indexing to help \oursys{} move towards a general data storage system would be interesting. SwitchKV~\cite{li2016fast} uses content-based routing to route requests to backend nodes based on cached keys, while NetCache~\cite{netcache-sosp17} further caches key-values in the switch. This load balancing and caching will also benefit the system. Eris~\cite{eris} uses a network sequence generator to implement efficient distributed transactions, which can bring new life to the one-sided RDMA methods for client synchronization.

%\textbf{TODO}

%SSD or HDD for persistence~\cite{marmol2014nvmkv, debnath2010flashstore, papagiannis2016tucana}
%network switch~\cite{li2016fast}


%main limitations:

%~\cite{liang16fpl} only uses on-chip BRAM.
%~\cite{istvan2013flexible,chalamalasetti2013fpga,istvan2015hash} only use on-board DRAM, These work don't make use of host RAM, result in small system capacity.

%~\cite{lavasani2014fpga, blott13hotcloud} leverages host RAM, however they didn't have optimization for PCIe DMA which becomes the bottleneck.

%~\cite{liang16fpl, lavasani2014fpga} only support fixed-size Key-value pair, not pratical.


%all works lack of optimization for network like batching and only achieves low encoding efficiency, resulting in low network bandwidth utilization.
%all works lack of optimization for hashtable, which leads to higher average search in heavy load.
