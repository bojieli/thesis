%!TEX root=main.tex
\chapter{数据中心与可编程网卡概论}

\section{数据中心硬件}

数据中心是互联网的 ``大脑''，也是人类绝大部分数据存储和计算运行的地方。如今的一个大型数据中心可以占据几个足球场的面积，容纳数十万台服务器，消耗几十兆瓦的电力，相当于一个小型核电站的发电量。据统计，全球电力的 1/5（需要核实）都被数据中心所使用。

\subsection{数据中心应用对硬件的影响}

从 20 世纪 50 年代计算机的诞生到 90 年代互联网兴起前，人类的算力大部分用于高性能计算、数字化企业和个人计算机（PC），其产生的数据大多存储在一个个信息孤岛上。需要处理大量信息的大型机乃至超级计算机一般采用软硬件一体化系统，即软硬件是由同一个公司团队开发的。由于采用了高速硬件互连、硬件冗余，这些系统往往同时具备高性能和高可靠性，但成本随系统规模的扩大急剧增加。为了充分利用昂贵的计算机，虚拟化的概念应运而生。其中 20 世纪 50 至 60 年代的分时系统 \cite{strachey1959time,amdahl1964architecture} 实现了多个用户任务分时复用硬件，发展成为 70 年代 UNIX 等现代操作系统 \cite{bach1986design} 的前身。20 世纪 70 至 90 年代，虚拟机监视器（Virtual Machine Monitor，VMM）进一步实现了多个操作系统分时复用硬件 \cite{popek1974formal,agesen2010evolution}，为云计算的发展做了技术准备。

20 世纪末，连接信息孤岛的搜索引擎拉开了互联网时代的序幕。数据中心不仅需要搜集、处理和索引海量信息，还需要实时响应大量用户的信息检索请求。传统的超级计算机、大型机和企业级存储不仅成本高昂，也无法满足海量信息存储和用户请求处理的可扩放性。为此，Google 提出用普通商用服务器组建可扩放的数据中心，用软件来实现存储的分区和冗余、用户请求的分派，在相对不可靠的硬件基础上组建起容错的系统 \cite{dean2008mapreduce}。

这些普通商用服务器之所以成本较低，是因为它的架构与大量商用的个人计算机（PC）类似，CPU、内存、主板、硬盘、网卡等组成部分都是标准组件，由各个专业公司独立设计实现。操作系统、数据库、Web 服务器等软件也是标准化的，或者由专业公司开发，或者是开源软件。产量大的标准组件能更好地平摊研发、流片等一次性工程费用（Non-Recurring Engineering，NRE），从而降低标准组件的价格。由标准组件构成的系统虽然降低了数据中心的软硬件成本，但也给标准组件的开发者施加了限制：大家需要遵守标准组件之间的接口和协议，只能在自己的边界内创新。数据中心系统的搭建者则只能像搭积木一样组合标准组件，而很难通盘考虑、全局优化。

21 世纪的第一个十年，互联网的发展让越来越多的企业需要提供 24 小时运行的网络服务，互联网数据中心（Internet Data Center，IDC）托管服务逐渐兴起。然而，IDC 托管需要客户事先购买服务器硬件，并需要运维人员维护，有较高的资产成本（capex）和运营成本（opex）。很多企业的网络服务一方面具有较高的季节性（如亚马逊的黑五促销），因此在闲时大量的计算资源被闲置；另一方面数据和用户规模的扩张很快，给购买硬件和 IDC 选址带来了时间压力。为此，虚拟机托管服务提供了按需租用的虚拟机资源，实现了服务器资源按 CPU 核的切片和在不同客户间的分时复用，也便于客户内部运维人员的管理和调度。

云计算是虚拟机托管服务的升级版，标志性的变革是计算和存储的解耦。虚拟主机托管服务把一台主机上的计算和存储资源分片成多个虚拟机，一旦主机的硬件或虚拟化软件（hypervisor）发生故障，虚拟机也就停机，其中的数据还有丢失的风险。在云计算中，虚拟机的存储资源在分布式存储系统中有多个副本，从而计算节点发生故障时可以从其他计算节点重启虚拟机，存储节点的故障则一般对客户透明。计算和存储的解耦不仅大大提高了服务可用性和数据安全性，也方便了虚拟化软件升级和虚拟机热迁移。

云的趋势。摩尔定律。


大规模数据处理。

5 年前，深度学习开始兴起。

\textbf{从简单可并行应用（搜索引擎）到复杂分布式计算应用（类似超级计算机）}

\textbf{计算粒度：从物理机到虚拟机到函数 Granular Computing}

\textbf{数据中心操作系统：为应用提供虚拟化抽象，一虚多，多虚一}

\textbf{云计算和 5G 需要虚拟化，需要灵活性、可编程性和可调试性}

\textbf{计算虚拟化、存储虚拟化、网络虚拟化}




计算机已经是目前人类生活中不可或缺的一项技术。从米粒大小的微型控制器，到占用几个足球场面积的云计算中心，无处不在的计算机系统在过去的半个多世纪中取得了令人难以想象的长足进步。当我们随时用手机订餐下单，用智能电视点播节目，从知乎或Wikipedia上获取知识的时候，我们已经很难想象没有计算机的世界是怎样的了。

计算机体系结构的进步是和半导体集成电路的发展分不开的。可以毫不夸张地说，计算机的发展是集成电路技术最大的推动力，也是最大的获益者。过去半个多世纪以来，集成电路一直大致保持着摩尔定律预测的速度发展，高度集成的芯片带来一次又一次计算机体系结构的革命，从巨型机（MainFrame）到个人电脑（Desktop），从服务器到云计算平台，从笔记本电脑到移动计算，集成电路使得各种形态的计算机快速地渗透到我们生活的方方面面。而另一方面，正是计算的需求，推动了中央处理器 (CPU)，内存 (DRAM)，图形加速器 (GPU) 以及高速网络芯片的发展。芯片技术和计算机体系结构相辅相成，共同推动了计算机系统软件和计算机应用的发展，使信息技术在短短的几十年中获得了如此重要的社会和经济地位。

目前信息科学技术依然处于一个高速发展的阶段，新的应用层出不穷：人工智能、物联网、虚拟现实、区块链，新兴的方向让人目不暇接，每一项技术都有无穷的潜力，可能推动人类社会进入全新的未来。而另一方面，基于半导体硅的CMOS集成电路芯片技术由于受到物理规律的限制，已经开始达到瓶颈。一方面芯片的线宽（Feature Size）在5纳米以下已经很难进一步缩小，而另一方面功耗和散热已经成为现代芯片技术难以绕过的难题。在这种情况下，计算机技术会何去何从？未来的计算机如何满足不断增长的计算能力的需求？

借助这篇短文，我们想探讨一下我们认为计算机系统在未来几年中发展的可能方向，尤其是集成电路技术如何与计算机体系结构继续相互推动，提高计算的速度和效率，满足不断增长的算力需求。目前学术界和工业界的共识是，科学家们看来不太可能找到短期内能代替CMOS的技术，计算机技术性能的提升在短期内必须主要依靠计算机体系结构的创新。在这篇短文里，我们着重探讨一下云计算平台的体系结构可能的发展趋势和方向。在可以预见的未来，云计算会是非常重要的计算平台，它会提供人类社会主要的算力，推动整个世界的数据处理能力。当然，移动计算、物联网以及其它“端”也是非常重要的发展方向，但限于篇幅，在这篇短文中我们就不详细探讨这些方面的发展。

背景

云计算平台：变革来临

在21世纪初期，云计算的概念刚刚起步的时候，云计算的平台基本上就是一个普通的企业数据中心，只不过规模比较大而已。当时的云计算与传统意义上科学计算用的超算平台在硬件上有很大区别。科学计算用的超级计算机往往使用定制的芯片，定制的网络，不惜成本地追求最高的性能。而云平台的特点就是把普通的低端商用服务器规模化，用软件实现高容错、高可靠性，降低成本，提供高性能的服务。

随着云计算商业模式得到用户的认可，特别是公有云的概念被广泛接受，经过近二十年的发展，目前主要的云计算平台提供商已经取得了巨大的商业成功。云计算平台的规模已经大大扩张。业界领先的云平台已经拥有遍布世界各地数以百计的数据中心以及数以百万计的服务器。这对计算机的体系结构带来了非常多新的挑战。建设一个大规模的云计算平台需要巨大的投资，任何能够减少单机成本的创新都能带来极高回报；云计算需要大量电力维持运转，提高系统的效率，降低功耗是重要的指标；云计算平台的一个大卖点就是弹性和灵活的配置，如何提供弹性同时尽可能减少系统空闲是一个复杂的问题；云计算需要保证可靠性，需要易于管理，需要保护私有信息，这里有非常多的挑战都需要解决。

在计算机发展的早期，软硬件是由同一个公司团队开发的，许多黑科技在这个大环境下被发明创造出来，对后世产生了深远影响。随着计算机的发展，设计分工变得越来越细，CPU、内存、网络、操作系统等等计算机的组成部分逐渐由各个专业公司独立完成，这使得工程师们必须遵守现有的体系框架，跨界创新变得非常困难。虽然在某些专用领域（例如游戏机）设计师们还可能做系统全盘考虑，但在通用计算领域这几乎成了不可能的任务。

随着云计算的发展，工程师们重新获得了在计算机全栈（full-stack）上创新的机会。在云计算平台中，软件和硬件的环境都由服务商控制，在达到了一定的规模以后，各种形式的定制（customization）都成为可能。只要能够提高性能，降低价格，增强竞争力，云服务商有足够的动力去定制芯片，改变网络协议，改变服务器架构，更改操作系统，甚至重新编写应用程序。下一代云平台想要保持竞争优势，除了完善的软件栈外，还必须在体系结构上创新。像过去一样完全使用现成部件（off-the-shelf components）就可以构建有竞争力的公有云平台的时代已经一去不复返了。而在不远的未来，这些云端体系结构的创新必然会向下流动，对未来私有云、企业服务器甚至个人计算机和移动计算的体系结构都会带来深远影响。

可编程逻辑阵列FPGA 和专用集成电路 ASIC 

为了更好了解集成电路和计算机体系结构，需要在这里简单介绍一下两个重要的集成电路技术。

ASIC（Application Specific Integrated Circuit，专用集成电路）是为某些应用专门开发的集成电路芯片。ASIC开发门槛比较高，研发周期也比较长。在目前的技术水平下，中等复杂度的ASIC前期投入的一次性开发成本（NRE， Non-Recurring Engineering）会在数百万到一两千万美元左右，并且需要一两年的开发周期。

FPGA（Field Programmable Gate Array，可编程门阵列）是一种可以重新定制 (reconfigurable) 的集成电路元器件。直观上来说，FPGA就是一个可以用编程的方法重新组合的一大堆电子元器件。这些元器件包括逻辑门（如与，或，非门），寄存器 (Register)，加法器，静态内存（SRAM）等等，用户可以定制它们之间的连接从而组成不同的电路。如今的FPGA除了基本元件，还加入了越来越多的DSP和硬核（hard IP），以提高乘法、浮点运算和访问外围设备的性能。FPGA的优点是技术比较成熟，开发门槛相对其它集成电路（如ASIC）较低，部署后依然可以修改，缺点是性能比专用芯片差。
FPGA传统上被广泛应用于原型设计，逻辑电路模拟，以及高端路由器等等的领域。最近几年FPGA开始被在数据中心中得到越来越广泛的应用。在数据中心中，FPGA主要被用在两个方面，一方面是用于部署后依然可能需要改变的应用上，比如网卡。由于公有云的网卡上经常需要调整或添加协议，需要经常对硬件重新编程。另一种应用是计算加速：对于一些特殊的计算，可以利用FPGA可以高度并行化的的特性加速。在数据中心中用FPGA加速应用的一个大概的规则是10，100，1000规则：应用需要可以被加速十倍以上，如果加速比太小就不值得做硬件的实现了；被加速的部分核心大约相当于100行以下的代码，更复杂的逻辑用硬件实现难度就会比较大；而计算在数据中心应该需要至少1000台左右的服务器，如果服务器数目远远小于1000台，可能就不值得开发硬件方案了。当然，这些数字都是非常粗略的估计，应该根据FPGA的部署情况，应用本身的价值，以及开发人员的实际情况做相应调整。

在过去，开发ASIC往往是专业硬件芯片公司才能做到的事情。但随着云计算平台规模的不断扩大，云计算系统公司也开始尝试针对自己的云独立设计专用芯片。

未来云计算平台的展望

现代的计算机系统大概可以被划分成四个主要部分：计算系统（Compute）、内存系统（Memory）、存储系统（Storage）以及网络和互连系统（Networking and Interconnect）。接下来，我们就简单谈一下近期云计算平台体系结构上可能的一些进展，希望能够起到抛砖引玉的作用。

\subsection{计算系统}

计算系统是整个计算机的核心，通常是由中央处理器CPU和一些为特殊应用服务的加速器构成。从整个计算机体系结构的发展来看，计算系统上的创新可能是最受关注的一个领域了。

在过去，计算系统基本上就等同于CPU，而CPU上的创新是由Intel、IBM等少数几个大公司主导的。传统上计算力是计算机系统中相对比较富裕的资源。一方面得益于摩尔定律及CPU体系结构的进步，CPU的速度和效率得到了长足的进步；另一方面其它的子系统例如硬盘，内存和网络因为相对缓慢的进展而往往成为实际应用的瓶颈。这种情况在最近几年发生了很大改变：随着新的硬件如固态硬盘和超高速光通信网络的出现， I/O往往不再是瓶颈；而新的应用如深度学习和内存数据库等对算力有极大需求，传统的CPU已经越来越难以支撑这些应用。正因为如此，在计算系统上创新，也就是大家通常所说的异构系统成为近年来计算机体系结构研究的一个热点。

深度学习加速器

最近深度学习的热潮，使得计算加速成为一个热点。从大学到工业界，从初创企业到业界巨头，从传统的半导体公司到传统的应用软件开发商都在尝试设计专门针对深度学习应用的硬件加速器，为小到低功耗的IoT设备，大到整个数据中心设计解决方案。在云计算领域，谷歌作为利用集成电路ASIC加速深度学习的先行者，为此设计的专用张量处理器（TPU）目前已经发展到第3代，无论从技术创新，吸引眼球，还是商业运作方面都取得了极大的成功。微软利用FPGA做深度神经网络（DNN）加速，也已经在Azure云平台上线。随着深度学习应用的普及，这个领域在未来几年一定会有非常激烈的竞争。

深度学习包括训练和推理两种场景。训练所需的内存大、运算复杂，大多部署在云数据中心，NVIDIA的GPU具有巨大的生态系统优势，仅有第三代TPU、Graphcore等少数深度学习加速器正在挑战其垄断地位。推理的应用场景则较为异构。移动计算场景下低功耗是最重要的需求，如寒武纪神经网络处理器以知识产权（IP）授权的形式集成进入华为麒麟970处理器；搜索推荐、自动驾驶和虚拟现实场景下，既需要毫秒甚至微秒级的低延迟，又需要较高的吞吐量。推理的多种场景下不仅有不同的性能指标，所需的神经网络运算也不尽相同。加速器除了支持循环神经网络、卷积、矩阵乘法、正则化等经典神经网络运算，还需要与场景相关的算法共同演进，利用稀疏化（sparsity）、量化（quantization）等技术在不降低精度的前提下提高算力，并支持动态控制流等新兴的神经网络结构。因此，深度学习加速器的差异化和定制化可能成为趋势。

FPGA和可重构硬件

用FPGA做计算加速在学术界已经被研究多年，在某些特定的领域也得到了一些应用。而将FPGA大规模部署在通用服务器上却是最近几年才开始取得的突破。很多公司都开始做一些这方面的尝试，特别是微软和百度在有效利用FPGA做云计算加速方面做了很多工作。例如微软在Azure云平台上已经全面部署了FPGA模块。FPGA在云平台上可以被广泛用于数据压缩、数据加密、图像处理、网络功能、大数据处理以及深度学习的加速。目前云计算一个探索的方向是将FPGA作为服务提供给第三方开发者。随着云端FPGA开发环境和开发工具的完善，越来越多的应用会利用FPGA的重构能力获得加速。

收购Altera后，Intel推出集成了FPGA的Xeon芯片，这将大大提高FPGA作为计算加速器的效率；而Xilinx则推出了全新的代号为Everest的可重构硬件构架，专门针对服务器的计算加速。这些技术必将对未来几年的云计算平台架构带来深远影响。除此之外，一些研究机构和初创企业开始再次尝试设计新一代粗粒度可重构硬件。粗粒度可重构硬件对于一些合适的应用可以兼顾FPGA可重构和ASIC高效率的优点，虽然过去的一些类似尝试最终没有取得商业成功，但云计算蓬勃发展的大环境很可能让这一技术焕发新的生命。

通用可编程加速器

得益于深度学习的高速发展，传统的计算加速器比如NVIDIA的Tesla以及Intel的Xeon Phi近年来也在数据中心和云平台上取得了很大的成功。除了深度学习，这些加速器还可以被用于加速科学计算，数据库检索，机器学习，图像处理等许多不同领域的应用。这些可编程通用加速器主要是利用计算的并行性来获得比CPU更高的性能和效率。但是，目前已有的这些加速器主要是为高性能计算设计的，还没有公司专门为云计算平台的一些主流应用比如数据压缩，网络包处理，加密解密等等设计一个通用的加速器。这很可能是一个有一定潜力值得挖掘的新方向。

对加速器来说，一大挑战是普通的内存无法满足并行计算需要的数据吞吐带宽。利用3D封装技术实现的HBM（High Bandwidth Memory ）最近开始被广泛应用在这些加速器中。随着集成电路工艺的改进，集成计算和内存于同一芯片的PIM（Processing In Memory）方法开始重新受到重视，在这个技术上的突破可能给可编程加速器带来性能的飞跃。

通用处理器

在可以预见的未来，云计算中的大部分代码，特别是不易并行的代码，仍将主要在CPU上执行。为了隐藏访存延迟，提高流水线利用率，CPU使用乱序和推测执行来动态调度指令的执行顺序。过去的推测执行在微体系结构方面的副作用导致了Spectre等安全漏洞，需要未来的CPU体系结构设计做出重大的改变。

乱序和推测执行受到指令窗口的限制，只能隐藏百纳秒级的延迟（基本对应于内存访问的延迟）。对于访问网络、存储、加速器等带来的微秒级延迟，还没有很好的解决方法。简单地增大乱序执行指令窗口是不现实的，而操作系统现有的线程切换机制开销过高。编程语言现有的协程（coroutine）机制一方面使编程变得复杂，另一方面在访问网络、存储、加速器等外设时仍然依赖开销较高的内存屏障。未来计算机体系结构的一大挑战是解决对程序员透明的微秒级细粒度并发。这需要CPU体系结构、编程语言和操作系统的协同设计。

通用处理器并不仅仅只存在于服务器的中央处理器。加速器、硬盘、网卡等设备上一般也有用于控制和通信的处理器。此类处理器往往需要根据领域需求定制，而使用MIPS或ARM需要不菲的授权费。最近，加州大学伯克利分校提出了RISC-V，一种开放的指令集架构（ISA），以缩短定制化处理器架构的周期和成本。RISC-V以及其相对应的软件生态的出现和完善使得Intel，ARM之外的公司和个人也可以在CPU体系结构上创新，而不用被编译器，操作系统，及上层的应用生态限制。RISC-V这一开放指令集架构的运动必然会对未来CPU的生态产生非常大的影响。很多公司会有机会完全抛弃Intel和ARM另起炉灶，为自己的应用设计和生产自己的CPU。让我们拭目以待。

软件的挑战

对所有提高计算系统效率和性能的技术来说，可编程性永远是一个无法绕过的坎。近年来，硬件的高层次综合技术和软件并行编程技术都有了很大的进展，这降低了利用新的计算资源的门槛。尽管如此，如何方便而有效地利用这些非传统的计算资源依然是一个长期的难题，需要大量的持续投入。

不同于PC时代分工明确、由各个公司分别掌控的标准化模块，云计算中从硬件到软件全栈优化和定制化的需求是开源生态系统近十年来蓬勃发展的主要驱动力之一。未来的异构计算软件框架也将形成开放的软硬件接口标准和开源生态系统。

\subsection{内存系统}

内存系统用来存放程序运行时的代码和数据。目前几乎所有的计算机系统上，内存系统都是由少量基于静态内存（SRAM）的缓存（Cache）和大量的动态内存（DRAM）组成的。除了极少数为科学计算或超大型数据库设计的高性能计算机外，绝大部分计算机上的内存只能被本地同一主板上的一颗到几颗CPU芯片访问。在过去的半个世纪以来，内存系统的主要特性都一直没有什么变化，非常令人振奋的是，最近几年内出现的几项新技术可能会改变这一现状。

高速非易失性内存（Non Volatile Memory，NVM)：

目前被广泛使用的动态内存（DRAM）制造工艺已经接近物理极限，价格一直居高不下，另外，DRAM需要不断刷新来保持数据，功耗较高。长期以来科学家们一直在寻找能够代替DRAM的存储器件。通过科学家们多年以来不懈努力，一直声称有希望取代DRAM的高速非易失性内存目前看来终于快要进入实际应用阶段了。相比传统的NAND Flash，新一代的高速非易失性内存存取速度要快得多。虽然它们短期内还不能完全取代DRAM，但至少希望能够在不久的将来会代替一部分普通内存。目前看来，Intel和Micron合作的3D XPoint技术会是最快进入市场的非易失性内存，另外一些基于忆阻器件（ReRAM）和磁阻器件（MRAM）的非易失性内存可能会紧随其后进入市场。

非易失性内存相比DRAM有价格低，容量大，功耗小，断电可以保持数据的优点，但同时又有存取速度慢，写入周期有限等等限制。如何有效地利用非易失性内存目前是一个重要研究方向。

内存解聚（Memory Disaggregation）：

传统的服务器上的CPU、内存、存储等资源聚合在一起成为独立的单元，一台服务器需要通过网络API访问远程的资源，使用上非常不便和低效。目前一个计算机体系结构研究的热点是资源解聚，也就是让资源可以不受服务器物理边界的限制被透明地远程访问。内存解聚指的是计算机的计算系统（CPU、GPU等）可以自由而透明地高效共享远程计算机的内存，这样可以大大增加内存的利用率，降低云计算平台的成本。

要想实现这一目标，计算机系统的软硬件都需要做很大的改变。从硬件上来看，如何能让远程访问满足内存所需的带宽和延迟要求是一个巨大挑战。幸运的是，利用内存访问的局部性，如果一部分热数据仍在本地，剩余的数据通过远程访问，则远程内存的带宽和延迟要求能比本地内存大大降低。加州大学伯克利分校的研究指出，要把内存解聚后的系统性能与全部使用本地内存的差距控制在5\%以内，带宽需要达到40 Gbps，端到端往返延迟需要不超过3~5微秒。现代数据中心的带宽已能满足大多数应用的需求，但延迟仍有一个数量级的差距。

从系统软件上来说，如何有效共享远程的内存，让应用克服远程内存访问的延迟和带宽限制也有很多问题需要解决。与传统的swap不同，把一块内存放在远程会减少对应远程机器的本地内存，因此一块内存放在本地还是远程、放在远程的哪台机器上，需要根据集群全局的信息来决策。另外，远程内存的延迟远大于本地内存，当等待远程内存时，如果不希望CPU核闲置，就需要调度其他任务。前面“通用处理器”一节已经谈到，这种微秒级的延迟隐藏仍是一个没有很好解决的问题。

HBM（High Bandwidth Memory ）：

HBM使用2.5D集成电路封装技术，将内存和逻辑芯片封装在同一硅片衬底中，从而提供更高的带宽。前面已经提到HBM已经开始被用到计算加速器上，但目前这一技术还没有用在通用的计算CPU上。一方面普通外置DRAM已经可以满足通用CPU带宽的需求，不需要高成本的HBM，另一方面现代应用通常需要很大的内存容量，目前的HBM还无法满足。但是从系统整体设计的角度来看，计算加速器独占高速内存是不正确的设计。CPU应该和加速器共享高速内存，这样不仅能够更充分地利用HBM，同时CPU和加速器之间的协同也更加简单高效。但是目前由于CPU和加速器是由不同公司设计制造的，所以只能采用一个权宜的设计。如果单个公司控制整个系统的设计，可能结果就不一样了。在微软的XBOX One X 中，CPU就是和GPU共享高速的GDDR5的，而AMD的APU也一直在推动CPU和GPU共享内存。随着计算加速的普及，在以后的云平台中CPU可能会与加速芯片共享HBM。从CPU角度看HBM就是一块高速内存，而普通DRAM就会成为一个容量的扩展，变成内存层次（Memory Hierarchy）中的另外一层。

软件的挑战：

传统的内存层次（Memory Hierarchy）通常有三层。cache是透明的，用户的程序一般不需要直接操作；而可以按照字节寻址的DRAM的性能是统一的，这是应用程序需要控制的内存区域；对用户完全透明的虚拟内存（Page Swap）虽然理论上非常优美，但由于内存与磁盘之间的延迟相差多个数量级，实际使用中很小比例的内存不命中就会极大影响应用的性能，所以一般是万不得已的最后选择。新一代的内存系统势必需要在传统的内存层次中再增加一个层次。用户程序是需要显式调用专用的API去存取这片内存，还是应该对应用透明？这个层次的内存应该如何能够被高效使用目前还没有定论。

\subsection{存储系统}

存储系统过去主要是由基于磁介质的硬盘和磁带组成，近年来基于NAND  Flash的固态硬盘被广泛应用于数据中心，而磁带已经开始淡出历史舞台。与内存系统不同，存储系统主要用来存放需要长期保存的数据。

SCM（Storage Class Memory）

SCM实际上是上一小节中讨论过的高速非易失性内存的另一种描述方式。在传统的计算机体系结构中，基于DRAM的内存和基于硬盘的存储是完全不同的两个系统，它们分别由操作系统中的内存管理系统和文件系统分别管理，基本上井水不犯河水。可是随着技术的发展，高速非易失性内存有潜力成为可以同时代替内存以及硬盘的一种器件，这时文件系统和内存系统的边界就开始模糊起来了。将高速非易失性内存用文件系统管理起来，利用它的非易失特性储存需要长期保存的数据就是SCM。SCM的出现对存储系统提出了新的挑战，如何有效地利用SCM高带宽、低延迟、可字节寻址的特性加速应用程序是一个需要解决的问题。

首先，机械硬盘和基于NAND Flash的固态硬盘是以“块”为基本单位来读写的，从而随机读写的性能远低于连续读写。SCM可以按照字节寻址和读写，随机与连续读写的性能差距较小，从而降低了应用对数据排布的要求。很多应用可以把SCM文件映射到内存后直接访问，省去在文件存储格式和内存数据结构间来回转换的开销。

其次，传统系统为了在断电或系统崩溃后能够恢复，往往需要日志和快照技术来把一致的内存状态显式写入存储系统。利用SCM的非易失特性，把DRAM作为SCM的写通（write-through）缓存，应用就可以直接操作其上的数据结构而无需额外的日志和快照。

存储解聚（Storage Disaggregation）

类似于内存解聚，存储系统同样可以通过解聚来提高利用效率和性能。微软研究院将存储系统的解聚分为四个层次。一是配置解聚，即标准的机柜（rack）离线配置成存储阵列或计算阵列。二是故障解聚，即计算节点发生故障时把其硬盘分配给新的计算节点，无需搬运数据即可恢复服务。三是动态弹性解聚，即根据计算和存储的需求比例动态调整每块硬盘属于哪个服务器。前三个层次中，每块硬盘在每个时刻只属于一个服务器。四是完全解聚，即每个服务器都可以访问任意设备上的任意文件，还可以在IO操作的细粒度上做负载均衡，当然这对IO控制器的负载要求较高。

与内存系统相比，由于存储系统本身的带宽较低、延迟较高，存储系统解聚面临的挑战相对要小一些，一个主要的问题是如何能在资源共享的情况下能够减少用户之间的相互干扰（即性能隔离）以及保证服务的质量（QoS）。

开放通道（Open Channel）SSD

如今服务器里的固态硬盘大多基于NAND Flash。NAND Flash的每个闪存块需要先擦除再写入，每次擦除会造成一定的磨损，多次写入有的闪存块就会损坏；闪存块读取次数多了、闲置时间长了，上面的数据也会流失。因此，固态硬盘中的闪存转换层（FTL）除了从逻辑地址到物理地址的映射，还需要做一系列的处理。这些处理需要预留一部分存储空间、占用带宽来做垃圾回收和冗余，还会导致写操作时搬运周围数据带来额外开销。如果考虑到应用的读写特性和数据中心的外部冗余，FTL的很多操作在云计算数据中心中是不必要的。因此，开放通道（Open Channel）SSD在数据中心开始流行，应用直接管理闪存阵列，FTL的功能与应用程序就可以协同设计以提升性能。

软件的挑战

传统存储系统操作通常需要操作系统的参与。过去，硬盘的速度较慢，操作系统的开销往往可以忽略不计。随着高性能存储介质的出现，应用程序与操作系统之间的上下文切换以及操作系统内文件系统和块设备层的开销逐渐成为性能瓶颈。为此，Intel 提出的SPDK提供了绕过内核的应用直接访问存储接口。然而，当多个应用需要共享文件系统时，应用间的权限管理、并发控制和QoS仍是有待研究的问题。

\subsection{网络和互连}

网络（Networking）系统通常指用来连接多个计算机使它们之间能够相互通信的系统，而互连（Interconnect）一般指计算机内部的部件之间的连接。通常情况下这两种连接是有着巨大差别的，属于两个完全不同的学术领域。一个典型的网络，比如数据中心网络，通常需要跨越较长距离、对延迟不敏感的传输，需要多层路由和交换来连接为数众多的节点，丢包是靠软件检测和恢复的，且一个组件的故障一般不会影响网络其他部分的正常运行；而一个典型的互连，比如连接内存和CPU的总线，位宽和带宽会比较大，延迟很低，丢包会由硬件检测和恢复，但一个组件的故障可能导致整个系统的崩溃。

在互连网发展的早期，以搜索引擎为代表的应用比较容易并行化，因此对网络通信的需求不是很高，数据中心通常使用现成（off-the-shelf）的服务器通过以太网互连。最近，大数据处理、分布式机器学习、内存和存储等资源的解聚需要数据中心主机间低延迟、高带宽的通信。GPU和深度学习处理器为了做分布式训练和推理，也需要建立可扩放（scalable）的互连。由于高速可扩放通信需求的推动，有一个很有意思的趋势就是融合传统的网络和元器件之间的互连。比如常见的PCI-Express，开始设计的时候是为了连接单一机器内的不同部件，但是现在，PCI-E也被设计为可以交换的一个协议；而以太网的开始时是设计为连接局域网内计算机的一个网络，而如今一些器件间的连接也开始使用以太网的连接方式。

低延迟无丢包的可扩放互连

理想的网络和互连需要既能扩放到数据中心内数以百万计的设备和组件，又有低延迟、高带宽、无丢包的特性，还能容忍部分组件的故障甚至恶意攻击。为此，网络和互连的设计者一方面可以从很多历史设计中吸取经验，如超级计算机、片上网络、核心路由器的线卡间互连、电路交换网络、时分复用网络等；另一方面可以利用新的物理层技术，如光交换芯片、激光通信和60G无线网络等。

数据中心内从发送端网卡到接收端网卡的网络延迟主要包括光纤上的传播延迟、交换机的处理延迟和交换机内的排队延迟，其中排队延迟占了主要部分。为了降低排队延迟，数据中心交换机普遍使用ECN、RED等机制来把拥塞情况反馈给发送端，但这个反馈延迟往往较长，且不能消除来自不同发送端的数据包恰好撞在一起导致的偶发排队。为此，近期一系列研究在重新思考现有的拥塞控制算法，交换机也需要更灵活的动态控制来尽可能降低队列长度、保证服务质量。更深层的问题是Internet的端到端原则简化了设计、提高了鲁棒性，但也缩小了很多全局优化的空间。在端到端原则的指导下，很多数据中心的网络和主机是独立运维的，这也增加了协同设计的沟通成本。在网络和互连融合的未来数据中心，端到端原则是否仍然适用是一个值得思考的问题。

随着主机内异构计算和存储设备数量的增加，CPU的PCIe接口数量不足，PCIe交换机开始被引入主机；由于PCIe的带宽不能满足需求，GPU之间采用NVLink互连，随着互连GPU数量的增加也出现了交换机。为了解决异构计算和内存设备之间的互连，工业界成立了CCIX、Gen-Z和OpenCAPI三个开放标准组织，其中OpenCAPI最先商业化，Gen-Z的应用范围最广。数据中心网络的发展历史表明，异构计算设备之间的可扩放互连必须考虑容错性和安全性。

可编程交换机

可编程交换机是由需求和硬件两方面的趋势所驱动的。从需求方面来讲，网络的自动运维（self-driving network）变得越来越重要。要实现自动化的网络故障检测、诊断和恢复，就必须在网络中加入智能，而不能把网络看成一个黑盒子。为此，交换机中需要加入可编程的抓包和统计功能。不仅如此，利用交换机的高速数据包处理能力，可以加速分布式系统中的缓存、聚合、同步、事务处理等，而低延迟无丢包网络也需要可编程交换机硬件的支持。很多人认为在交换机中增加可编程性会增加芯片面积和功耗，其实并不尽然。交换机芯片中约有30\%的面积用于串行IO通信，50\%的面积用于存储查找表和数据包缓冲的内存，只有20\%的面积用于数据包处理逻辑。随着交换机的带宽不断增长，芯片面积也在增加，此时用于数据包处理的20\%面积就有空闲，可以放入更多逻辑。

可编程交换机按照可编程性从低到高，可分为三个层次。一是符合OpenFlow标准的交换机，数据包处理逻辑是一条由若干个匹配-执行表串接而成的流水线，但每个表的匹配和执行都有一定的限制；二是符合P4标准的交换机，在流水线结构的基础上，每个表项的匹配规则、所执行的操作和数据包头的解析规则都可以定制；三是网络处理器，即使用为网络处理特别设计的众核CPU处理每个数据包，可以达到最大的灵活性，但单条网络连接的吞吐量受限于CPU频率。事实上，固定功能流水线、通用可编程流水线和网络处理器并不是泾渭分明的，在数据中心交换机中已经可以看到逐渐融合的趋势，例如采用交叉开关或片上网络来灵活互连各个数据包处理模块和片上内存。

为了简化交换机运维和管理，微软发起了SONiC白盒交换机开源项目。首先，定义了一组交换机芯片API，交换机的硬件和软件就可以独立演进而无需担心兼容性问题。其次，设计了基于容器的模块化交换机软件架构，通过把持久状态独立于容器存储，实现了细粒度的故障恢复和零服务中断时间的在线升级。最后，提供了监控和诊断能力，以支持网络的自动运维。

可编程网卡

传统以太网网卡的功能相对简单，网络协议栈在操作系统内核里实现，软件中间件又给应用程序提供了RPC、消息队列等更高层的抽象。在云计算场景下，还需要虚拟交换机软件来实现网络虚拟化和防火墙等网络功能。因此，数据中心网络的端到端往返延迟平均情况就高达上百微秒，极端情况甚至可达毫秒级，远远不能满足低延迟分布式计算的需求。其中，软件的延迟占了主要部分。高性能计算中常用的Infiniband网络把大部分网络协议栈实现在网卡里，可以实现端到端微秒级的延迟，因此其中的远程直接内存访问（RDMA）技术开始在数据中心中流行。为了与现有数据中心网络兼容，数据中心的RDMA网卡通常使用以太网和UDP/IP，再在其上封装RDMA可靠传输协议。但云计算数据中心的RDMA部署比高性能计算复杂很多：首先，数据中心的规模大于高性能计算集群；共享同一物理主机的虚拟机需要隔离和QoS，也就是网络虚拟化；虚拟机需要热迁移和故障恢复能力；另外，数据中心应用种类繁多、通信模式复杂，需要兼容性和高层抽象。

为解决网络虚拟化需求，很多云数据中心大规模部署了可编程网卡，以把软件中的网络功能卸载到网卡硬件。主流的可编程网卡有两种结构，一种是由众核处理器构成，网络数据包被分派到不同的核上处理，每个核依次对每个数据包执行各个网络功能的处理指令。市面上的大多数可编程网卡采用这种网络处理器结构。另一种的核心是FPGA，在FPGA上构建类似交换机芯片的流水线，每个数据包依次通过流水线上的各个处理模块。微软在Azure云大规模部署了基于FPGA的可编程网卡，达到了比其他云平台更低、更稳定的网络延迟。

软件的挑战

与存储系统类似，网络系统传统上被视为慢速的外设，再加上交换机排队、网络协议栈和中间件的高延迟，目前很多分布式系统并没有充分利用数据中心网络的高带宽和低延迟。近年来，绕过内核访问网络的用户态协议栈如雨后春笋般出现，但在兼容性、安全性和性能间往往存在一定的折中。利用应用访问网络特征的流调度（flow scheduling）和流量工程（traffic engineering）也是近年的研究热点。我们相信，打破主机边界的软硬件协同设计将是网络和互连的趋势。

系统创新

把整个世界看作一台大型计算机是微软CEO萨蒂亚·纳德拉的愿景，也是很多系统研究者的梦想。云计算的成功使数据中心吸纳了人类世界大部分的计算和存储，而数据中心可以看作是由计算、内存、存储和网络及互连四部分组成的一台大型计算机。系统就是从全局的角度考虑各种软硬件组件如何高效而可靠地协同工作，以及给用户提供怎样的抽象。从大型机、PC、传统数据中心到云计算数据中心，每个时代的系统结构都随着应用的需求而变化。未来十年，系统结构又会出现哪些创新呢？

Rack Level Computing

目前的数据中心的基本组件是由固定数量的CPU、GPU、内存、存储等资源构成的服务器。一方面，一台服务器中能容纳的硬件资源数量较少，只能同时运行少量任务，每个任务对不同种类资源的需求不均衡，这就会导致服务器中有的资源短缺，有的资源闲置。另一方面，CPU、内存和存储技术在成本、性能和功耗方面的演进趋势大不相同，要增加一种新硬件，往往需要重新设计服务器主板和机箱，需要较高的成本和较长的上市周期。前文提到的内存解聚和存储解聚通过访问远程内存和存储，只能解决第一个问题，可以认为是资源解聚的一个过渡期方案。完全的资源解聚需要重新思考以服务器为中心的数据中心设计。目前的趋势是把数据中心的基本组件从服务器变成机柜（rack）。机柜由若干个资源刀片（resource blade）和高速互连组成，每个资源刀片是一个装满同一种硬件资源的物理容器。在目前的数据中心里，存储阵列就像是这样的资源刀片。这样，CPU、GPU、内存、存储等不同种类的计算资源分别装在不同的资源刀片里，可以独立演进。当然，GPU和CPU仍然有本地DRAM或HBM内存作为缓存，而内存刀片则由高达数TB的DRAM或NVM组成，可以存储相对冷的数据。

边缘数据中心（Edge Data Center）

目前的云计算数据中心大多规模庞大，部署在电力成本低、网络条件好的地区。用户访问最近的数据中心，也往往需要数毫秒到数十毫秒的延迟，带宽也受到一定限制。随着物联网、机器视觉、虚拟现实、自动驾驶等技术的发展，终端数据产生了越来越多需要实时处理的数据，此时把数据全部上传到云端在带宽和延迟上都是不可行的。为此，边缘数据中心开始兴起，把云计算搬到距离用户更近的位置。一方面，边缘数据中心可以降低用户访问互连网服务的延迟，例如现在的内容分发网络（CDN）服务。另一方面，边缘数据中心可以把终端设备上的部分计算和存储卸载到云端，降低移动设备的功耗、体积和成本，方便应用的部署和更新。

边缘数据中心将给云计算带来前所未有的挑战。首先，数据中心的数量将非常庞大。目前一家云服务约有几十个数据中心，开发者可以人工决定把服务部署在哪些数据中心。但对深入到每个城市、小区、楼宇的边缘数据中心，手工选址和部署就完全不现实了，因此需要全自动的方案。一个服务的数据被拆分到星罗棋布的数据中心，如何维护数据的一致性又不损失性能也是一个难题。其次，规模不大的边缘数据中心需要支持数量繁多的应用，这就需要细粒度的资源共享。目前云计算的粒度已经越来越细，从虚拟机到容器（container）再到函数（serverless function），边缘数据中心将延续这一趋势。

互连就是计算机（Fabric is the Computer）

传统计算机架构中CPU是中心，所有外设之间的通信都要通过CPU。近年来，CPU性能的提升放缓了，GPU、FPGA、TPU等异构计算设备和网络、存储的性能却突飞猛进，因此CPU日渐成为瓶颈。网卡、GPU、NVMe SSD等设备迫切需要绕过CPU的直接互连互通，因此出现了NVLink这样的专用互连，以及GPU-Direct和NVMe Over Fabrics等直接访问远程主机设备的技术。Broadcom也推出了可以通过以太网连接的PCIe交换机，使PCIe跨越了主机的边界。

然而，现有的技术大多只能互连一定范围内的特定设备。为了让数据中心内的所有异构计算和存储设备都能跨越主机的边界直接互连，微软研究院提出了Terminus项目，将互连（Fabric）作为一台服务器的控制中心，Fabric 控制器之间通过数据中心网络互连。通过在Fabric中加入智能，可以在不同厂商的计算和存储设备之间做 “翻译”，使它们在统一的命名空间中能够互相通信；Terminus也可以实现不同设备间的访问控制、通信性能隔离和服务质量保证；另外，Terminus可以支持资源虚拟化，既可以把多个物理的计算或存储资源虚拟成一个对用户透明的逻辑资源，又可以把一个物理资源虚拟成多个逻辑资源来实现多个用户间的资源共享；最后，Terminus 上的FPGA和可编程的数据中心网络可以在数据传输过程中进行一些处理，加速一系列的计算任务。Terminus项目目前使用FPGA作为原型，在未来也可以实现为智能的PCIe交换机。它与智能网卡、智能交换机一道，构建起数据中心的智能互连（Intelligent Fabric）。此时的数据中心可以被真正看作一台大型计算机，CPU、GPU等异构的计算资源和SSD、SCM等存储资源通过智能互连相连接，把分布式系统的通信、调度、容错等机制隐藏在统一的抽象之下，给用户提供取之不尽的计算资源和前所未有的编程便利。

结论

云计算提供了信息时代的人类社会主要的算力。本文从计算、内存、存储以及网络和互连四个方面探讨了云计算数据中心硬件和系统未来的发展趋势，及其对软件的挑战。云计算使工程师们重新获得了在计算机全栈上创新的机会，也推动了开源软件、开放标准和生态系统的发展。尽管摩尔定律即将终结，通用处理器的性能提升也如挤牙膏一样困难，但这远远不意味着硬件创新的停滞。相反，异构计算硬件和新型内存、存储和互连的故事才刚刚开始。软硬件结合将成为云计算系统的主流，系统的重要性将愈发凸显。数据中心和服务器的体系结构创新也会向下流动，对个人计算设备带来深远影响。在硬件、软件和系统的协同创新下，云计算数据中心对用户提供的抽象将越来越强大和易用，为人工智能，物联网，虚拟现实等重要应用的发展提供算力。



\section{可编程网卡的架构}
\label{smartnic-architecture}

现在的 SmartNIC 不是单个芯片，而是片上系统（SoC）。例如 Mellanox ConnectX-5 firmware。根据数据面来区分架构分类。

\subsection{专用芯片（ASIC）}
\label{smartnic-asic}

TCP Offload Engines

Mellanox ConnectX-5: programmable match-action tables for OVS, QoS, virtual switch, RDMA / RoCE...

Programmable, but not Turing complete. Not flexible.

学术界提出的可编程网卡架构：FlexNIC, Emu, SENIC, sNICh, Uno, Your Programmable NIC Should be a Programmable Switch，HotNets'18


\subsection{网络处理器（NP）}
\label{smartnic-np}

Queues + Flow Processing cores + accelerator ASIC + controller core

Mellanox NP-5, Netronome NFP-32xx, Cavium OCTEON

在服务质量（QoS）方面，硬件调度器比软件调度器不仅 \cite{kaffes2019shinjuku,ousterhout2019shenango}


\subsection{通用处理器（SoC）}
\label{smartnic-soc}

Switching ASIC + CPU

Mellanox BlueField: vSwitch interconnect ConnectX-5 and multi-core ARM CPU, similar to ServerSwitch

BlueField family SoC devices combine 64-bit Arm v8 A72 cores

Broadcom BCM5880X

\subsection{可重构硬件（FPGA）}
\label{smartnic-fpga}

ASIC + FPGA

Mellanox Innova-2 Flex, Microsoft Catapult


\subsection{架构对比}
\label{smartnic-comparison}

性能逐渐降低，可编程性逐渐提高。

如何选择，考虑几个方面：

\section{可编程网卡在数据中心的应用}

\subsection{微软 Azure 云}


一、数据中心系统原语的性能瓶颈
如今大多数的网络服务和大数据处理部署在云计算数据中心里。不同于传统高性能计算集群，一方面，云计算数据中心的资源是在多个租户间共享的，因此需要对资源进行虚拟化，在多个租户间提供访问隔离和性能隔离；另一方面，云计算数据中心的计算任务是多种多样的，大量租户使用现有的开源或商业软件，要求租户手动修改现有软件是比较困难的，因此云计算数据中心的加速方案需要对用户应用透明。

云计算数据中心同时提供了不同于传统分布式系统的加速机遇。首先，数据中心的网络拓扑是较为规则的，且由单一的运营者管理，方便部署软件定义的集中式策略；其次，数据中心网络设备的可编程性较强，可以进行一些灵活的数据包处理。


图1.1：数据中心网络和存储性能的快速提升（纵坐标为对数坐标）

近年来，尽管CPU的频率提升和核数增加都日渐放缓，网络和存储的性能却仍在不断提高。如图1.1所示，近10年来，网络和存储的吞吐量都提升了100倍左右。然而，现有通用操作系统和虚拟机的设计认为网络、存储等I/O设备属于低速设备，因而没有对访问这些外设进行优化，在访问网络和存储的系统原语中存在大量开销。因此，高速数据中心网络和存储往往并不能得到充分利用。

我们分三种场景来讨论数据中心系统原语的开销：进程间通信、跨主机通信、远程访问共享数据结构。


图1.2：进程间通信

如图1.2，在同一台主机的两个进程之间通信时，应用程序调用Linux标准运行库（libc）的通信接口（如send、recv），标准运行库再调用对应的系统调用进入内核，在内核中唤醒对面的进程。在此过程中，系统调用的开销不可忽略。特别是2018年的Spectre和Meltdown漏洞使用户态和内核态之间需要增加额外的指令来保证安全性，从而使系统调用的开销比补丁前提高到了4倍。此外，由于内核中的状态是多个处理器共享的，为了保证并发访问时的一致性，内核需要加锁或使用同步机制（如RCU），进一步增加了系统原语的开销。

多个进程共享内核状态不仅增加了访问开销，同时也使进程的迁移和故障恢复变得复杂，因为进程的内核状态难以从源主机中完整提取出来，也难以在目标主机上完整恢复。现有的进程迁移方案都需要大量修改内核，给内核各个子系统增加提取和恢复进程状态的接口，不仅工程量大，还难以保证正确性。



图1.3：跨主机通信

如图1.3所示，当两个主机之间需要通信时，发送端需要依次通过标准运行库、客户机内核、虚拟机监视器到达网卡，接收端则需要通过相反的路径通知到应用程序。其中标准运行库的系统调用开销与进程间通信的场景相同。

客户机内核的开销，或称网络协议栈开销，主要分为三部分：接口抽象、多进程并发协调、可靠传输协议。其中接口抽象是指Linux的文件描述符、事件通知机制等。多进程并发协调在进程间通信的场景中讨论过，是指访问内核共享数据结构时的锁和同步。可靠传输协议则是为了处理网络丢包，校验数据包正确性，并用拥塞控制算法在多条并发连接间公平分配网络带宽。

网络虚拟化开销，或称网络功能开销，是指云计算环境下为了提供虚拟网络的抽象和隔离性，而在虚拟机监视器中部署的网络隧道封装和解封装、加密解密、防火墙、负载均衡、QoS服务质量保证等网络功能。


图1.4：远程访问共享数据结构

如图1.4所示，在远程访问共享数据结构的场景下，多个客户端访问一个共享的数据结构。通常的架构是由一台主机上的应用程序负责接收和处理来自所有客户端的请求。除了前面讨论过的多主机通信开销，应用程序访问共享数据结构的处理开销也不容忽视。通用CPU的单核计算并行度和随机访问内存并行度都很有限，因此通用CPU的单核哈希表访问性能只有大约5M次每秒。在一台Xeon E5标准处理器中，即使把所有CPU核都利用起来，也远远不足以利用所有内存通道的随机访问性能和网络带宽。


图1.5：操作系统内核占网络应用的CPU时间比例

综上，操作系统原语在多种数据中心通信密集型计算场景下开销明显。下面分析内核和网络虚拟化的开销对应用性能的重大影响。如图1.5所示，使用常见网络应用的标准性能测试，可以发现75%~90%的CPU时间在内核态，也就是用户程序的处理时间只占10%~25%。这意味着在理想情况下，如果系统内核开销可以被完全消除，用户程序的性能就可以提升到4~10倍。


图1.6：40 Gbps线速网络下公有云的网络虚拟化开销

除了内核，公有云的网络虚拟化是系统原语的另一个重要开销。如图1.6所示，对常见网络功能而言，如果用软件实现，为了达到40 Gbps线速的吞吐量，在正常情况下需要数个到数十个CPU核来进行处理，而在极端的最坏情况下甚至需要数百个CPU核进行处理。巨大的网络功能开销不仅增加了云服务商的运营成本，使很多云服务商不得不限制虚拟机的网络带宽，造成物理网络带宽的浪费。

此外，软件网络功能还有较高且不稳定的延迟。软件网络功能往往需要数十微秒到数毫秒不等的延迟。数据中心网络的硬件转发延迟只有数微秒到数十微秒，软件网络功能增加的延迟使端到端的应用延迟增加了数倍。另一方面，当负载较高时，延迟可能增加到数毫秒，这对延迟敏感的应用程序而言是不可接受的。

二、可编程硬件
1.1 FPGA的优势
众所周知，通用处理器（CPU）的摩尔定律已入暮年，而机器学习和 Web服务的规模却在指数级增长。人们使用定制硬件来加速常见的计算任务，然而日新月异的行业又要求这些定制的硬件可被重新编程来执行新类型的计算任务。FPGA (Field Programmable Gate Array) 是一种硬件可重构的体系结构，常年来被用作专用芯片（ASIC）的小批量替代品。近年来FPGA在微软 [1]、亚马逊 [2]、百度 [3]、腾讯 [4]、阿里云 [5] 等云服务巨头的数据中心大规模部署，以同时提供强大的计算能力和足够的灵活性。2016年，Intel公司以167亿美元的价格收购FPGA巨头Altera公司，以保持其在数据中心领域的领导地位，并探索CPU与FPGA结合的异构服务器计算架构 [6]。


图2.1：领域定制体系结构的兴起。列出了CPU、GPU、FPGA、TPU等不同体系结构估计的效能比（每瓦特能量能执行的操作数量）。尽管CPU的性能已经遇到瓶颈，GPU、FPGA、TPU等定制化硬件体系结构的性能仍在按照摩尔定律的预期不断提高，并且比CPU的能效高1~2个数量级。

以CPU为代表的通用处理器通常采用冯·诺依曼结构及其变体（下称冯氏结构）。冯氏结构中，由于执行单元（如 CPU核）可能执行任意指令，就需要有指令存储器、译码器、各种指令的运算器、分支跳转处理逻辑。由于指令流的控制逻辑复杂，不可能有太多条独立的指令流，因此GPU 使用 SIMD（单指令流多数据流）来让多个执行单元以同样的步调处理不同的数据，CPU 也支持 SIMD指令。而 FPGA 每个逻辑单元的功能在重编程（烧写）时就已经确定，不需要指令。

冯氏结构中使用内存有两种作用。一是保存状态，二是在执行单元间通信。由于内存是共享的，就需要做访问仲裁；为了利用访问局部性，每个执行单元有一个私有的缓存，这就要维持执行部件间缓存的一致性。对于保存状态的需求，FPGA 中的寄存器和片上内存（BRAM）是属于各自的控制逻辑的，无需不必要的仲裁和缓存。对于通信的需求，FPGA 每个逻辑单元与周围逻辑单元的连接在重编程（烧写）时就已经确定，并不需要通过共享内存来通信。

FPGA 实际的表现如何呢？我们分别来看计算密集型任务和通信密集型任务。

计算密集型任务的例子包括矩阵运算、图像处理、机器学习、压缩、非对称加密、必应搜索的排序等。这类任务一般是 CPU把任务卸载（offload）给 FPGA 去执行。对这类任务，Intel Stratix V FPGA 的整数乘法运算性能与 20 核的 CPU 基本相当，浮点乘法运算性能与 8 核的 CPU 基本相当，而比 GPU 低一个数量级。下一代 FPGA，Stratix 10，将配备更多的乘法器和硬件浮点运算部件，从而理论上可达到与现在的顶级 GPU 计算卡旗鼓相当的计算能力。

在数据中心，FPGA 相比 GPU 的核心优势在于延迟。像 Bing 搜索排序这样的任务，要尽可能快地返回搜索结果，就需要尽可能降低每一步的延迟。如果使用 GPU 来加速，要想充分利用 GPU 的计算能力，batch size 就不能太小，延迟将高达毫秒量级。使用 FPGA 来加速的话，只需要微秒级的 PCIe 延迟（我们现在的 FPGA 是作为一块 PCIe 加速卡）。未来 Intel 推出通过 QPI 连接的 Xeon + FPGA 之后，CPU 和 FPGA 之间的延迟更可以降到 100 纳秒以下，跟访问主存没什么区别了。

FPGA 为什么比 GPU 的延迟低这么多？这本质上是体系结构的区别。FPGA 同时拥有流水线并行和数据并行，而 GPU 几乎只有数据并行（流水线深度受限）。例如处理一个数据包有 10 个步骤，FPGA 可以搭建一个 10 级流水线，流水线的不同级在处理不同的数据包，每个数据包流经 10 级之后处理完成。每处理完成一个数据包，就能马上输出。而 GPU 的数据并行方法是做 10 个计算单元，每个计算单元也在处理不同的数据包，然而所有的计算单元必须按照统一的步调，做相同的事情（SIMD，Single Instruction Multiple Data）。这就要求 10 个数据包必须一起输入、一起输出，输入输出的延迟增加了。当任务是逐个而非成批到达的时候，流水线并行比数据并行可实现更低的延迟。因此对流式计算的任务，FPGA 比 GPU 天生有延迟方面的优势。

ASIC 专用芯片在吞吐量、延迟和功耗三方面都无可指摘，但微软并没有采用，主要出于两个原因：
1.	数据中心的计算任务是灵活多变的，而 ASIC研发成本高、周期长。好不容易大规模部署了一批某种神经网络的加速卡，结果另一种神经网络更火了，钱就白费了。FPGA只需要几百毫秒就可以更新逻辑功能。FPGA 的灵活性可以保护投资，事实上，微软现在的 FPGA玩法与最初的设想大不相同。
2.	数据中心是租给不同的租户使用的，如果有的机器上有神经网络加速卡，有的机器上有必应搜索加速卡，有的机器上有网络虚拟化加速卡，任务的调度和服务器的运维会很麻烦。使用FPGA 可以保持数据中心的同构性。

接下来看通信密集型任务。相比计算密集型任务，通信密集型任务对每个输入数据的处理不甚复杂，基本上简单算算就输出了，这时通信往往会成为瓶颈。对称加密、防火墙、网络虚拟化都是通信密集型的例子。

对通信密集型任务，FPGA 相比 CPU、GPU 的优势就更大了。从吞吐量上讲，FPGA 上的收发器可以直接接上 40 Gbps 甚至 100 Gbps 的网线，以线速处理任意大小的数据包；而 CPU需要从网卡把数据包收上来才能处理，很多网卡是不能线速处理 64 字节的小数据包的。尽管可以通过插多块网卡来达到高性能，但CPU 和主板支持的 PCIe 插槽数量往往有限，而且网卡、交换机本身也价格不菲。

从延迟上讲，网卡把数据包收到 CPU，CPU 再发给网卡，即使使用 DPDK 这样高性能的数据包处理框架，延迟也有 4~5微秒。更严重的问题是，通用 CPU的延迟不够稳定。例如当负载较高时，转发延迟可能升到几十微秒甚至更高；现代操作系统中的时钟中断和任务调度也增加了延迟的不确定性。

虽然 GPU 也可以高性能处理数据包，但 GPU 是没有网口的，意味着需要首先把数据包由网卡收上来，再让 GPU 去做处理。这样吞吐量受到 CPU 和/或网卡的限制。GPU 本身的延迟就更不必说了。

那么为什么不把这些网络功能做进网卡，或者使用可编程交换机呢？ASIC的灵活性仍然是硬伤。尽管目前有越来越强大的可编程交换机芯片，比如支持 P4 语言的 Tofino，ASIC仍然不能做复杂的有状态处理，比如某种自定义的加密算法。

综上，在数据中心里 FPGA 的主要优势是稳定又极低的延迟，适用于流式的计算密集型任务和通信密集型任务。

2.2 FPGA在数据中心的部署

2016 年 9 月，《连线》（Wired）杂志发表了一篇《微软把未来押注在 FPGA 上》的报道 [3]，讲述了Catapult 项目的前世今生。紧接着，Catapult 项目的老大 Doug Burger 在 Ignite 2016大会上与微软 CEO Satya Nadella 一起做了 FPGA 加速机器翻译的演示。演示的总计算能力是 103 万 T ops，也就是 1.03 Exa-op，相当于 10 万块顶级 GPU 计算卡。一块 FPGA（加上板上内存和网络接口等）的功耗大约是30 W，仅增加了整个服务器功耗的十分之一。只要规模足够大，对FPGA价格过高的担心将是不必要的。

微软部署 FPGA 并不是一帆风顺的。对于把 FPGA部署在哪里这个问题，大致经历了三个阶段：
1.	专用的 FPGA 集群，里面插满了 FPGA
2.	每台机器一块 FPGA，采用专用网络连接
3.	每台机器一块 FPGA，放在网卡和交换机之间，共享服务器网络

第一个阶段是专用集群，里面插满了 FPGA 加速卡，就像是一个 FPGA 组成的超级计算机。像超级计算机一样的部署方式有几个问题：
1.	不同机器的 FPGA 之间无法通信，FPGA 所能处理问题的规模受限于单台服务器上 FPGA 的数量；
2.	数据中心里的其他机器要把任务集中发到这个机柜，构成了 in-cast，网络延迟很难做到稳定。
3.	FPGA 专用机柜构成了单点故障，只要它一坏，谁都别想加速了；
4.	装 FPGA 的服务器是定制的，冷却、运维都增加了麻烦。

一种不那么激进的方式是，在每个机柜一面部署一台装满 FPGA 的服务器（上图中）。这避免了上述问题 (2)(3)，但(1)(4) 仍然没有解决。

第二个阶段，为了保证数据中心中服务器的同构性（这也是不用 ASIC 的一个重要原因），在每台服务器上插一块FPGA，FPGA 之间通过专用网络连接。这也是微软在 ISCA’14 上所发表论文采用的部署方式。

FPGA 采用Stratix V D5，有172K个ALM，2014个M20K片上内存，1590个 DSP。板上有一个8GB DDR3-1333 内存，一个PCIe Gen3 x8接口，两个10 Gbps网络接口。一个机柜之间的FPGA采用专用网络连接，一组10G网口8个一组连成环，另一组10G网口6个一组连成环，不使用交换机。

这样一个 1632 台服务器、1632 块 FPGA 的集群，把必应的搜索结果排序整体性能提高到了 2倍（换言之，节省了一半的服务器）。本地和远程的 FPGA 均可以降低搜索延迟，远程 FPGA的通信延迟相比搜索延迟可忽略。[4]

FPGA 在必应的部署取得了成功，Catapult 项目继续在公司内扩张。微软内部拥有最多服务器的，就是云计算 Azure 部门了。Azure 部门急需解决的问题是网络和存储虚拟化带来的开销。Azure把虚拟机卖给客户，需要给虚拟机的网络提供防火墙、负载均衡、隧道、NAT等网络功能。由于云存储的物理存储跟计算节点是分离的，需要把数据从存储节点通过网络搬运过来，还要进行压缩和加密。

在 1 Gbps 网络和机械硬盘的时代，网络和存储虚拟化的 CPU 开销不值一提。随着网络和存储速度越来越快，网络上了 40 Gbps，一块 SSD 的吞吐量也能到 1 GB/s，CPU 渐渐变得力不从心了。例如 Hyper-V 虚拟交换机只能处理 25 Gbps 左右的流量，不能达到 40 Gbps 线速，当数据包较小时性能更差；AES-256 加密和 SHA-1 签名，每个 CPU 核只能处理 100 MB/s，只是一块 SSD 吞吐量的十分之一。

为了加速网络功能和存储虚拟化，微软把 FPGA 部署在网卡和交换机之间。如下图所示，每个 FPGA 有一个 4GB DDR3-1333 DRAM，通过两个 PCIe Gen3 x8 接口连接到一个 CPU socket（物理上是 PCIe Gen3 x16 接口，因为 FPGA 没有 x16 的硬核，逻辑上当成两个 x8 的用）。物理网卡（NIC）就是普通的 40 Gbps 网卡，仅用于宿主机与网络之间的通信。


图2.2：微软SmartNIC可编程网卡的架构，其中FPGA位于网络和经典网卡之间

FPGA（SmartNIC）对每个虚拟机虚拟出一块网卡，虚拟机通过 SR-IOV直接访问这块虚拟网卡。原本在虚拟交换机里面的数据平面功能被移到了 FPGA 里面，虚拟机收发网络数据包均不需要 CPU参与，也不需要经过物理网卡（NIC）。这样不仅节约了可用于出售的 CPU 资源，还提高了虚拟机的网络性能（25 Gbps），把同数据中心虚拟机之间的网络延迟降低了 10 倍。

这就是微软部署 FPGA 的第三代架构，也是目前「每台服务器一块 FPGA」大规模部署所采用的架构。FPGA 复用主机网络的初心是加速网络和存储，更深远的影响则是把 FPGA 之间的网络连接扩展到了整个数据中心的规模，做成真正 cloud-scale 的「超级计算机」。第二代架构里面，FPGA 之间的网络连接局限于同一个机架以内，FPGA之间专网互连的方式很难扩大规模，通过 CPU 来转发则开销太高。

第三代架构中，FPGA 之间通过 LTL (Lightweight Transport Layer) 通信。同一机架内延迟在 3微秒以内；8 微秒以内可达 1000 块 FPGA；20 微秒可达同一数据中心的所有 FPGA。第二代架构尽管 8台机器以内的延迟更低，但只能通过网络访问 48 块 FPGA。为了支持大范围的 FPGA 间通信，第三代架构中的 LTL 还支持PFC 流控协议和 DCQCN 拥塞控制协议。

图2.3：FPGA 构成的数据中心加速平面，介于网络交换层（TOR、L1、L2）和传统服务器软件（CPU 上运行的软件）之间。来源：[4]

通过高带宽、低延迟的网络互连的 FPGA构成了介于网络交换层和传统服务器软件之间的数据中心加速平面。除了每台提供云服务的服务器都需要的网络和存储虚拟化加速，FPGA上的剩余资源还可以用来加速必应搜索、深度神经网络（DNN）等计算任务。

对很多类型的应用，随着分布式 FPGA 加速器的规模扩大，其性能提升是超线性的。例如 CNN inference，当只用一块 FPGA 的时候，由于片上内存不足以放下整个模型，需要不断访问 DRAM 中的模型权重，性能瓶颈在DRAM；如果 FPGA 的数量足够多，每块 FPGA 负责模型中的一层或者一层中的若干个特征，使得模型权重完全载入片上内存，就消除了DRAM 的性能瓶颈，完全发挥出 FPGA 计算单元的性能。当然，拆得过细也会导致通信开销的增加。把任务拆分到分布式 FPGA集群的关键在于平衡计算和通信。

在 MICRO’16 会议上，微软提出了 Hardware as a Service(HaaS) 的概念，即把硬件作为一种可调度的云服务，使得 FPGA服务的集中调度、管理和大规模部署成为可能。

2.3 FPGA编程困难的挑战

图2.4：用硬件描述语言编程FPGA

传统上，FPGA使用Verilog、VHDL等硬件描述语言编程，如图2.4所示。众所周知，硬件描述语言难以调试、编写和修改，给软件人员使用FPGA带来了很大挑战。


图2.2：用HLS工具编程FPGA

为了提高FPGA的开发效率，FPGA厂商提供了高层次综合（HLS）工具，可以把C代码编译成硬件模块。但这些工具只是硬件开发工具链的补充，程序员仍然需要手动将从C语言生成的硬件模块插入到HDL项目中，且FPGA与主机CPU之间的通信也需要自行处理。这样的模型仍然不足以让软件开发人员使用FPGA。



图2.3：用OpenCL编程FPGA

近年来，为了让软件开发人员使用FPGA，FPGA厂商提出了基于OpenCL的编程工具链，提供了类似GPU的编程模型。软件开发人员可以把用OpenCL语言编写的核（kernel）卸载到FPGA上。但是，这种方法中多个并行执行的核间需要通过板上共享内存进行通信，而FPGA上的DRAM共享内存吞吐量和延迟都不理想，共享内存还会成为通信瓶颈。其次，FPGA与CPU之间的通信模型是类似GPU的批处理模型，这使得处理延迟较高（约1毫秒），不适用于需要微秒级延迟的网络数据包处理。


New content:

1. CPU - accelerator communication
1) coherent memory
2) I/O DMA
3) Network

2. connectivity among accelerators
1) single server
2) rack
3) data center

3. device
1) FPGA
2) GPU
3) ASIC

New workloads:

compute offload

1) bing: hardware microservice, consolidation

2) compression and encryption: Office 365, Cosmos/Azure data lake, Onedrive

3) AI inference: neural networks, traditional models, AAAI'18

4) 3rd party: general computing acceleration device

infrastructure offload: (pioneer the wave of SmartNICs)

1) networking: network virtualization (compute node); NFV (network node)

2) persistent storage: SOSP'11; compression and encryption (backend): improve throughput; improve compression ratio, save storage space.
hypervisor and sharing (frontend)


\subsection{亚马逊 AWS 云}

在 2017 年 12 月的 Re:Invent 大会上，亚马逊 AWS 云发布了名为 ``Nitro'' 的计算加速架构 \cite{nitro-blog}。
根据 2017 年 Re:Invent 大会和 2018 年 AWS 峰会上亚马逊发布的信息 \cite{nitro-talk,nitro-web}，AWS 使用了定制 ASIC 来实现多种加速和安全功能。
最初，AWS 在 FPGA 和 ASIC 架构之间权衡，并决定采用 ASIC 方案。
为此，2015 年 1 月，亚马逊用 30 多亿美元收购了 ASIC 设计公司 Annapurna 实验室 \cite{annapurna}，该公司以设计基于 ARM 核的片上系统（SoC）见长。

Nitro 项目的发展是分阶段的。与微软 Azure 类似，虚拟机的 I/O 瓶颈最早体现在虚拟网络上。早在 2013 年 11 月，AWS 的 C3 实例就引入了一块独立的网卡以实现高性能网络（enhanced networking），采用 SR-IOV 方式让虚拟机直接访问网卡，绕过虚拟机监控器中的虚拟交换机软件。此技术帮助 Netflix 实现了每秒 200 万个数据包的虚拟机网络吞吐量 \cite{netflix-aws}。

2015 年 1 月，AWS 的 C4 实例开始使用硬件加速弹性块存储（Elastic Block Storage，EBS）。弹性块存储的数据储存在存储节点上，而客户虚拟机运行在计算节点上，因此这是一种远程存储。对客户虚拟机而言，是一块虚拟存储设备，它是由虚拟机监控器 Xen Dom0 中的存储管理软件实现虚拟化的。C4 实例使用高性能网卡而非传统网卡来连接远程的弹性块存储，从而提高了性能。

2017 年 2 月，AWS 的 I3 实例引入了 NVMe 本地存储和专用的存储虚拟化芯片。以往，客户虚拟机访问本地存储，也需要经过虚拟机监控器中的存储管理软件，这是由于一台物理服务器中可能有多台虚拟机，每台虚拟机只能访问属于自己的那一部分存储空间，因此需要隔离。对延迟和吞吐量都很高的 NVMe 存储而言，存储虚拟化软件带来的开销太高了。为此，I3 实例引入的 Nitro 芯片在硬件上实现了存储隔离，因此可以通过 SR-IOV 把 NVMe 存储直通客户虚拟机，实现了每秒 300 万次 I/O 操作的存储性能 \cite{aws-local-storage}。

2017 年 11 月，AWS 的 C5 实例大幅改变了计算节点虚拟化的架构。首先，C4 实例中的远程存储仍然需要软件实现虚拟化，这一部分也可以像 I3 本地存储一样用硬件实现，不过块存储比本地存储的接口更复杂，因此硬件实现的难度更大。其次，在网络、远程和本地存储都已经使用硬件虚拟化后，事实上虚拟机监控器中的管理软件就只剩下数据平面的中断（APIC）功能和控制平面的管理功能了。控制平面的管理功能较为复杂，用纯数字逻辑显然是不现实的。为了把虚拟网络（VPC）、弹性块存储和虚拟化控制平面全部卸载（offload）到加速卡上，Nitro ASIC 采用了基于 ARM 核的片上系统架构，从而保持了数据平面的可编程性和灵活性，还能把控制平面一并卸载到加速卡上。

采用 Nitro 加速卡后，AWS 重新设计了一个轻量级的虚拟机监控器 Nitro 来取代 Xen，而原来运行在 Xen Dom0 上的控制平面转移到了 Nitro ASIC 里，客户虚拟机可以得到接近裸金属（bare-metal）主机的性能。此后，AWS 发布了裸金属实例，客户代码直接在物理机上运行，而所有的存储和网络资源都由 Nitro 卡提供。裸金属实例不仅可以达到极致性能，还可以运行客户所需的任意虚拟化软件（如 VMWare）。

根据公开信息，Nitro 系列芯片主要包括三种芯片：
\begin{enumerate}
	\item 云网络（VPC）和弹性块存储（EBS）加速芯片，一边连接数据中心网络，一边以 PCIe 卡的形式连接 CPU；
	\item 本地 NVMe 存储虚拟化芯片，作为 CPU 和 NVMe 存储设备之间的代理；
	\item 安全芯片，用于验证服务器中各种设备固件的版本，以及在裸金属服务器切换租户时重刷固件清除痕迹。
\end{enumerate}

Nitro 芯片的作用主要可以分为降低成本、提高性能和提高安全性三方面。性能方面，主要是提高吞吐量、降低平均延迟、降低延迟抖动。下面详细讨论。

\textbf{节约 CPU 核。}
网络和存储虚拟化需要占用大量的 CPU 资源来处理每个网络包和存储 I/O 请求。
根据 ClickNP \cite{li2016clicknp} 估计，每个客户虚拟机的 CPU 核，需要预留另外 0.2 个 CPU 核来实现虚拟化。
如果这些功能可以被卸载到专用硬件，所节省的 CPU 核就可以用来安装客户虚拟机。
不管是考虑公有云虚拟机上每个 CPU 核的售价，还是考虑 Xeon CPU 每个核的硬件成本，用专用硬件都能获得明显的成本节约 \cite{smartnic}。

\textbf{提高最大核数。}
节约 CPU 核不仅能够降低成本，还能够提高大型虚拟机实例的最大核数。因为各大公有云厂商都从 Intel 等相同的厂商购买 CPU，因此同一时期能买到的最大 CPU 核数是相对固定的。虚拟化被卸载到硬件后，所有 CPU 核都用于运行客户虚拟机，因此 AWS 的 M5 实例最多可达 96 个 CPU 核。如果不使用硬件卸载，将只有 80 个 CPU 核可用于客户虚拟机，从而降低对追求极致性能客户的吸引力。

\textbf{提高单核频率。}
由于功耗墙的限制，CPU 的核心数目与平均核心频率不可兼得。同一代 CPU 架构下，较高核心频率的 CPU 核数一般较少。对于核数相等的虚拟机实例，如果使用传统的软件虚拟化，物理机就需要 1.2 倍的 CPU 核数，从而平均核心频率就可能降低。例如，在 C5 实例推出前的 72 核 EC2 实例，CPU 基频为 2.7 GHz，但采用同一代 Skylake 架构的 C5 实例虚拟机，CPU 基频就可以达到 3.0 GHz。

\textbf{提高本地存储性能。}
首先，在裸金属服务器上，本地 NVMe 存储可以达到每盘高达 400 K IOPS（I/O 操作每秒）的吞吐量。AWS I3 实例有 8 块 NVMe SSD，达到 3 M IOPS 的吞吐量。而对于常见的存储虚拟化协议栈，每个 CPU 核只能处理 100 K IOPS 左右的吞吐量，这意味着要占用 30 个 CPU 核才能让虚拟机充分利用 NVMe 存储的吞吐量，这个开销太高了。即使只有一块 NVMe 存储，4 个 CPU 核之间的负载均衡仍然是个难题 \cite{li2017kv}。如第 \ref{smartnic-architecture} 节所讨论的，由于硬件分配和处理任务是流水线式而非多个处理单元简单并行，硬件能够比多核软件更好地保证服务质量（QoS）。

其次，在延迟方面，裸金属服务器的 NVMe 存储平均延迟约为 80 微秒。虚拟化软件不仅会增加 20 微秒的平均延迟，而且由于操作系统调度、中断、缓存不命中等因素的影响，在高负载下的尾延迟（tail latency）可高达 1 毫秒（1000 微秒）。采用硬件卸载可以降低平均延迟 20\%，并降低高负载下的尾延迟 90\% 以上。

\textbf{提高远程存储性能和安全性。}
与本地存储类似，硬件加速可以提高远程存储性能。还有一点额外的优势：远程存储被公有云的所有租户共享，对可靠性和安全性要求更高。传统上远程存储协议在虚拟机监控器中运行，虽然逻辑上与客户虚拟机隔离，但由于共享 CPU、内存等资源，仍然不能排除零日（0-day）漏洞和边信道攻击的潜在安全隐患。把远程存储协议从主机 CPU 卸载到 Nitro 卡后，就有了更高的隔离性和更小的攻击表面（attack surface）。

\textbf{提高网络性能和安全性。}
在 Nitro 卡上实现网络虚拟化后，虚拟机可以直接通过 SR-IOV 访问网卡，达到数据中心网络 25 Gbps 的理论上限，尤其是对小数据包应用场景的性能提升明显。根据 ClickNP \cite{li2016clicknp} 估计，对 25 Gbps 线速（line-rate）的 64 字节小数据包，每秒高达 37 百万个，如果用软件虚拟交换机处理，将需要 60 个 CPU 核，这显然是不可接受的。因此大多数云服务商对虚拟网络（VPC）对吞吐量不仅有字节数的限制，还有数据包数的限制。大多数公有云的虚拟网络只支持数百万数据包每秒的吞吐量，需要处理大量小请求的远程过程调用（RPC）、键值存储（KVS）服务器就会遇到性能瓶颈。在延迟方面，软件虚拟化的 AWS 端到端延迟可达 100 微秒以上，而使用 Nitro 虚拟化加速后延迟就降低到 50 微秒以内了。硬件虚拟化对虚拟网络的尾延迟、服务质量和安全性的提升，也与远程存储相似。

\textbf{提高裸金属服务器安全性。}
最后，裸金属服务器上的客户代码可以直接访问服务器内的各种硬件设备，甚至可能烧写带外服务器管理（BMC）等组件的固件 \cite{bare-metal-security}。如果固件内被嵌入了恶意代码，并在下一个租户使用该裸金属服务器时被激活，后果不堪设想。事实上很大一部分客户选择裸金属服务器，正是出于对虚拟机隔离性的担忧。为了在租户开始使用裸金属服务器时提供安全和一致的硬件环境，Nitro 安全芯片会对固件进行重烧写。Nitro 还会在系统启动时进行完整性检查，这类似 UEFI 可信启动技术，但验证的范围不仅包括操作系统引导器，还包括硬件固件。

\subsection{阿里云、腾讯云、华为云}

2018 年，我国云计算服务商也积极地在数据中心部署了可编程网卡。阿里云和腾讯云部署可编程网卡的首要目的是支持裸金属（bare-metal）服务器。相比虚拟机，裸金属服务器可以消除虚拟化带来的开销，实现同样硬件条件下最高的性能；方便部署客户自己的虚拟化软件（如 VMWare），降低客户上云的转型开销；方便使用不支持虚拟化或虚拟化后性能有较大损失的硬件，如 GPU 和 RDMA 网卡；不与其他租户共享服务器硬件，隔离性和安全性更强，也能符合一些机构的合规要求。

在公有云中使用裸金属服务器的主要技术挑战是访问数据中心内的虚拟网络（VPC）和远程存储（EBS）等资源。一种简单的方法是在同一个机架（rack）内放置若干虚拟网络和存储服务器，部署相应的软件；并在柜顶交换机（ToR switch）上配置转发规则，使裸金属服务器的所有网络数据包经过虚拟网络和存储服务器。这种方法需要增加额外的服务器资源，提高了成本。另一种方法是把虚拟网络和存储的数据平面卸载到柜顶交换机上。但是，柜顶交换机的编程灵活性一般较差，不足以支持虚拟存储的应用层协议和虚拟网络的安全规则等。

因此，在服务器上增加一块可编程网卡便成为支持裸金属服务器性能最高的方案。阿里和腾讯采用了 FPGA 数据平面与多核 CPU 控制平面结合的 SoC 方案。2018 年，阿里云发布的 ``神龙'' 裸金属服务器使用自研的 MOC 卡（《阿里云开发智能网卡的动机》，2018；《阿里云弹性裸金属服务器-神龙架构（X-Dragon）揭秘》，2018）实现类似 AWS Nitro 的网络、存储虚拟化。腾讯云在 APNet’18 上发布了基于 FPGA 的可编程网卡方案，主要用于网络虚拟化（FPGA，bare metal cloud，APNet 2018，Towards Converged SmartNIC Architecture for Bare Metal \& Public Clouds）。腾讯仅用 10 个硬件工程师和三个月，就开发出了可编程网卡的原型系统。腾讯正在把可编程网卡的应用范围从裸金属服务器扩展到普通虚拟机，并对两种应用场景使用统一的可编程网卡架构。

华为基于海思半导体的技术积累，发布了两款可编程网卡，同时也用于华为云虚拟网络的加速。SD100 可编程网卡采用了基于多核 CPU 的 SoC 方案，数据平面和控制平面都运行在 ARM CPU 上。另一款（补全名字）可编程网卡采用网络处理器（NP）提供数据平面的可编程性。两种可编程网卡均可达到 100 Gbps 性能。利用可编程网卡，华为发布了 C3ne 网络增强虚拟机实例，实现了每秒千万级的数据包转发，目前在国内云厂商中领先。（《华为发布智能加速引擎部件，全面加速企业应用》，2018；C3ne 官方主页）

