% !TeX root = ../main.tex

\begin{abstract}
数据中心是支持当今世界各种互联网服务的基础设施，主要由计算、存储、网络系统构成。一方面，通用处理器的性能提升逐渐放缓；另一方面，大数据与人工智能对算力的需求与日俱增。不同于容易并行的 Web 服务，大数据与人工智能需要各计算节点间更多的通信，这推动了数据中心网络性能的快速提高，也对共享数据存储的性能提出了更高的要求。然而，数据中心的网络和存储基础设施主要使用通用处理器的软件处理，其性能落后于快速增长的网络、存储、定制化计算硬件性能，日益成为系统的瓶颈。与此同时，在云化的数据中心中，灵活性也是一项重要需求。为此，近年来，可编程网卡在数据中心被广泛部署，利用现场可编程门阵列（FPGA）等定制化硬件加速虚拟网络服务。

本文旨在探索基于可编程网卡的高性能数据中心系统。本文提出，可编程网卡在加速虚拟网络之外，还可以加速网络功能、数据结构、操作系统等。为此，本文提出一个用 FPGA 可编程网卡对云计算数据中心计算、网络、存储节点实现全栈加速的系统。

首先，本文提出用可编程网卡加速云计算中的虚拟网络功能。我们提出了首个在商用服务器中用 FPGA 加速的高灵活性、高性能网络功能处理平台 ClickNP。众所周知，FPGA 编程对软件工程师很不友好。为了简化 FPGA 编程，我们设计了类 C 的 ClickNP 语言和模块化的编程模型，并开发了一系列优化技术，以充分利用 FPGA 的海量并行性。我们实现了 ClickNP 开发工具链，可以与多种商用高层次综合工具集成。我们基于 ClickNP 设计和实现了 200 多个网络元件，并用这些元件组建起多种网络功能。相比基于 CPU 的软件网络功能，ClickNP 的吞吐量提高了 10 倍，延迟降低到 1/10；且具有可忽略的 CPU 开销，可以为云计算中的每个计算节点节约 20\% 的 CPU 核。

其次，本文提出用可编程网卡加速远程数据结构访问。键值存储是最常用的基本数据结构之一，也是很多数据中心内的关键分布式系统组件。我们基于 ClickNP 编程框架，设计实现了一个高性能内存键值存储系统 KV-Direct，在服务器端绕过 CPU，用可编程网卡通过 PCIe 直接访问主机内存。我们把单边 RDMA 的内存操作语义扩展到键值操作语义，解决了单边 RDMA 操作数据结构时通信和同步开销高的问题。我们还利用 FPGA 可重配置的特性，允许用户实现更复杂的数据结构。面对网卡与主机内存之间 PCIe 带宽较低、延迟较高的性能挑战，我们设计了哈希表、内存分配器、乱序执行引擎、负载均衡和缓存、向量操作等一系列性能优化，实现了 10 倍于 CPU 的能耗效率和微秒级的延迟，实现了首个单机性能达到 10 亿次每秒的通用键值存储系统。

最后，本文提出用可编程网卡和用户态运行库相结合的方法为应用程序提供系统原语，从而绕过操作系统内核。套接字是操作系统提供的最常用的通信原语。我们设计实现了一个用户态套接字系统 SocksDirect，与现有应用程序完全兼容，能实现接近硬件极限的吞吐量和延迟，多核性能具有可扩放性，并在高并发负载下保持高性能。我们分别使用共享内存和 RDMA 实现主机内和主机间的通信。为了支持高并发连接数，我们基于 KV-Direct 实现了一个 RDMA 可编程网卡。我们消除了线程间同步、缓冲区管理、大数据拷贝、进程唤醒等一系列开销。SocksDirect 相比 Linux 提升了 7 至 20 倍吞吐量，降低延迟到 1/17 至 1/35，并将 Web 服务器的 HTTP 延迟降低到 1/5.5，解决了长期以来通用网络协议栈性能低、专用网络协议栈兼容性差的矛盾。


  \keywords{数据中心；可编程网卡；现场可编程门阵列；网络功能虚拟化；键值存储；网络协议栈}
\end{abstract}

\begin{enabstract}
	
Data centers are the infrastructure that hosts Internet services all around the world, which are basically composed of compute, storage and network systems.
On one hand, performance improvement of general processors is slowing down.
On the other hand, big data and artificial intelligence impose increasing computational power requirements.
Different from Web services that are easy to parallelize, big data and artificial intelligence require more communication among compute nodes, which pushes the performance of data center network to improve rapidly, as well proposes higher requirements for shared data storage performance.
However, networking and storage infrastructure services in data centers still mainly use software processing on general processors, whose performance lags behind the rapidly increasing performance of hardware in networking, storage and customized computing.
As a result, software processing becomes a bottleneck in data center systems.
In the meantime, in cloud data centers, flexibility is also of great importance.
To this end, recent years witnessed large scale deployment of programmable NICs (Network Interface Cards) in data centers, which use customized hardware such as FPGAs to accelerate network virtualization services.

We aim to explore high performance data center systems with programmable NICs.
We propose that except for accelerating network virtualization, programmable NICs can accelerate network functions, data structures and operating systems as well.
For this purpose, we propose a system that uses FPGA-based programmable NIC for full stack acceleration of compute, networking and storage nodes in cloud data centers.

First, we propose to accelerate virtualized network functions in the cloud with programmable NICs. We propose ClickNP, the first FPGA accelerated network function processing platform on commodity servers with high flexibility and high performance.
It is well known that FPGA programming is unfriendly to software developers.
To simplify FPGA programming, we design a C-like ClickNP language and a modular programming model, as well develop optimization techniques to fully exploit the massive parallelism inside FPGA.
We implement the ClickNP tool-chain that can integrate with multiple commercial high-level synthesis tools.
Based on ClickNP, we design and implement more than 200 network elements and construct various network functions using the elements.
Compared to CPU-based software network functions, ClickNP improves throughput by 10 times and reduces latency to 1/10.
ClickNP also has negligible CPU overhead and can save 20\% CPU cores in cloud compute nodes.

Second, we propose to accelerate remote data structure access with programmable NICs. Key-value store is one of the most common basic data structures, as well as a key distributed system component in many data centers.
We design and implement KV-Direct, a high performance in-memory key-value storage system based on ClickNP programming framework.
KV-Direct bypasses CPU on the server side and uses programmable NICs to directly access host memory via PCIe.
We extend memory semantics of one-sided RDMA to key-value semantics and therefore avoid the communication and synchronization overheads in data structure operations.
We further leverage the reconfigurability of FPGA to enable users to implement more complicated data structures.
To tackle with the performance challenge of limited PCIe bandwidth and high latency between NIC and host memory, we design a series of optimizations including hash table, memory allocator, out-of-order execution engine, load balancing, caching and vector operations.
We achieve 10 times power efficiency than CPU and microsecond scale latency.
We build the first general key-value storage system that achieves 1 billion operations per second performance on a single server.

Lastly, we propose to provide kernel-bypass system primitives for applications using a co-design of programmable NICs and user-space libraries.
Socket is the most common communication primitive provided by the operating system.
We design and implement SocksDirect, a user-space socket system that is fully compatible with existing applications, achieves throughput and latency that are close to hardware limits, has scalable performance for multi-cores, and maintains high performance with many concurrent connections.
We use shared memory and RDMA for intra-host and inter-host communication, respectively.
To support many concurrent connections, we implement an RDMA programmable NIC based on KV-Direct.
We further remove overheads such as thread synchronization, buffer management, large payload copying and process wakeup.
Compared to Linux, SocksDirect improves throughput by 7 to 20 times, reduces latency to 1/17 to 1/35, and reduces HTTP latency of Web servers to 1/5.5.

\enkeywords{Data Center; Programmable NIC; FPGA; Network Function Virtualization; Key-Value Store; Networking Stack}
\end{enabstract}
