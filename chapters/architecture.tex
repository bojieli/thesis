\chapter{系统架构}

本文提出一个基于可编程网卡的高性能数据中心系统架构。
可编程网卡是服务器与外界通信的 ``网关''，也是服务器内硬件设备、虚拟机间通信的 ``枢纽''。
我们把虚拟化、网络和存储功能、操作系统中需要高性能的数据平面卸载到可编程网卡，以降低 ``数据中心税''，让 CPU 集中精力于客户的应用程序。


\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/virt_arch.pdf}
	\caption{回顾：虚拟化的数据中心架构。}
	\label{arch:fig:virt-architecture}
\end{figure}

第 \ref{chapter:intro} 章已经讨论过，如图 \ref{arch:fig:virt-architecture} 所示，虚拟化的数据中心主要可以分为计算、网络、存储节点。
在网络和存储节点，我们采用控制面与数据面分离的设计思想。数据面是操作相对频繁、逻辑相对简单的处理，而控制面是操作相对不频繁、逻辑相对复杂的处理。我们在可编程网卡中实现数据面，在主机 CPU 上实现控制面，实现了数据面完全不经过主机 CPU。这包括第 \ref{chapter:clicknp} 章的虚拟网络功能和第 \ref{chapter:kvdirect} 章的数据结构处理。加速虚拟网络功能和远程数据结构访问也是本文最重要的创新。

在计算节点，亦即客户虚拟机所在的服务器主机上，我们用可编程网卡实现虚拟机监控器（hypervior）的虚拟化功能和操作系统原语。
虚拟化分为 ``一虚多'' 和 ``多虚一'' 两个方面。
``一虚多''，即可编程网卡把计算节点内的硬件资源虚拟化成多个逻辑资源，实现其他计算节点和本地多台虚拟机的多路复用。例如，第 \ref{chapter:clicknp} 章的 ClickNP 将硬件网卡和网络链路虚拟化为每个虚拟机一张虚拟网卡；第 \ref{chapter:clicknp} 章的 KV-Direct 实现了多个客户端并发访问共享键值存储，并能保证一致性。
``多虚一''，即可编程网卡把数据中心内物理上分散的资源虚拟化成一个逻辑资源，实现逻辑资源到物理资源的映射和路由。例如，第 \ref{chapter:clicknp} 章的 ClickNP 将数据中心内网络功能虚拟化成逻辑上统一的网络功能；第 \ref{chapter:clicknp} 章的 KV-Direct 客户端将分布式键值存储虚拟化成逻辑上统一的键值映射；还可以实现存储和内存的解聚（disaggregation）。
为了加速操作系统原语并控制硬件的复杂度，我们把操作系统原语划分为可编程网卡上的可靠通信协议和主机 CPU 上运行的用户态库、用户态管理程序，如第 \ref{chapter:socksdirect} 章 SocksDirect 实现的套接字通信原语。

图 \ref{arch:fig:accel-arch} 显示了基于可编程网卡的系统总体架构。下面我们将按照网络、存储和高层抽象的顺序，简要介绍本文后续各章的总体设计。


\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/accel_arch.pdf}
	\caption{基于可编程网卡的数据中心系统总体架构。}
	\label{arch:fig:accel-arch}
\end{figure}



\section{网络加速}

\subsection{网络虚拟化加速}

我们从第 \ref{chapter:intro} 章的传统数据中心架构（图 \ref{intro:fig:virt-architecture}）开始，逐步把 ``数据中心税'' 消除或者卸载（offload）到可编程网卡上。
如图 \ref{arch:fig:virtual-network} 所示，第一步是用可编程网卡替代原有的普通网卡，并把软件实现的虚拟网络卸载到可编程网卡上。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/virtual_network.pdf}
	\caption{用可编程网卡加速虚拟网络后的架构。}
	\label{arch:fig:virtual-network}
\end{figure}


为了让虚拟机上的操作系统网络协议栈能够使用虚拟网络收发数据包，可编程网卡利用 SR-IOV（Single Root I/O Virtualization）技术 \cite{dong2012high}，虚拟化成多个 PCIe 虚拟设备（VF，Virtual Function），并给每台虚拟机分配一个 PCIe 虚拟设备。
虚拟机内原有的虚拟网卡驱动程序（如基于 virtio 技术 \cite{russell2008virtio} 的）需要替换为本文实现的 FPGA 驱动程序和基于 FPGA 的虚拟网卡驱动程序。
第 \ref{chapter:clicknp} 章的 ClickNP 将硬件网卡和网络链路虚拟化为多个租户的虚拟网络。

如果我们可以绕过虚拟机操作系统的网络协议栈，直接替换虚拟机上应用程序所使用的标准库（即系统调用接口 libc），就不必实现 SR-IOV 硬件虚拟化。
第 \ref{chapter:socksdirect} 章的 SocksDirect 通过截获应用程序关于网络套接字的标准库调用，在用户态实现了容器覆盖网络（container overlay network），即适用于容器的虚拟网络。
为了用户态运行库与可编程网卡间的高效通信，我们在虚拟机内安装 FPGA 驱动程序，将可编程网卡的 PCIe 地址空间的一部分映射到用户态，从而绕过了虚拟机内核和虚拟机监视器（Virtual Machine Monitor 或 Hypervisor）。



\subsection{网络功能加速}

如图 \ref{arch:fig:network-function} 所示，第二步是在网络节点上，把软件实现的虚拟网络功能划分为数据面和用户面，并把数据面卸载到可编程网卡中。
需要注意的是，网络节点和计算节点的划分是逻辑上的。有可能虚拟网络功能被编排到与虚拟机相同的服务器主机上，这时网络节点和计算节点的功能就合二为一了，虚拟网络和虚拟网络功能之间的连接也从数据中心网络简化成了可编程网卡内模块间的连接。

来自源计算节点（或上一个网络节点）的数据包被网络节点的可编程网卡接收之后，在网卡内的数据面进行处理，大多数情况下不需要 CPU 上的控制面介入，就可以把处理后的数据包再发送回数据中心网络，到达目的计算节点（或下一个网络节点）。
第 \ref{chapter:clicknp} 章将介绍如何用高级语言模块化编程实现网络功能，实现 FPGA 数据面与 CPU 控制面的协同处理。


\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/NFV_accel.pdf}
	\caption{用可编程网卡加速网络功能后的架构。}
	\label{arch:fig:network-function}
\end{figure}

\section{存储加速}

\subsection{存储虚拟化加速}

在网络加速之后，第三、四步是存储加速。作为第三步，我们首先计算节点的存储虚拟化功能卸载到可编程网卡中。
虚拟机需要两种访问块存储（block storage）的方式：一是通过操作系统的存储协议栈来访问块设备，二是通过用户态的快速接口绕过操作系统直接访问。
由于操作系统本身和很多软件运行在块存储上，保持第一种传统访问方式的兼容性是必要的。
存储性能敏感的应用则使用本文提供的库来通过第二种方式访问块存储。

如图 \ref{arch:fig:virt-storage} 所示，可编程网卡将存储硬件的物理请求队列（如 SATA 的 32 个请求槽位和 NVMe 的请求队列）虚拟化成多个逻辑请求队列，并给第一种访问方式和每个需要第二种访问方式的应用分配一条逻辑请求队列。
由于第一种传统方式仅为提供兼容性，为了降低开发难度，本文没有使用 SR-IOV 技术，而是复用了现有虚拟机监视器中的软件虚拟化方案。
为了提供第二种高效访问方式，我们在虚拟机内安装 FPGA 驱动程序，将逻辑请求队列映射到用户态，从而应用程序可以通过我们的库接口直接访问可编程网卡，绕过了操作系统和虚拟机监控器。

为了支持多个存储节点组成的分布式存储，虚拟云存储服务需要把逻辑地址映射到存储节点地址。例如，在第 \ref{chapter:clicknp} 章的分布式键值存储中，客户端需要根据一致性哈希（consistent hashing） \cite{nishtala2013scaling}，把键（key）映射到存储节点，再把请求路由到该节点。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/virt_storage.pdf}
	\caption{用可编程网卡加速本地存储和云存储后的架构。}
	\label{arch:fig:virt-storage}
\end{figure}

\subsection{数据结构处理加速}

第四步，我们将存储节点上的数据结构处理卸载（offload）到可编程网卡。
我们以第 \ref{chapter:kvdirect} 章将详细介绍的键值存储为例。
存储节点上的可编程网卡从网络上收到查询（GET）或写入（PUT）某个键（key）的请求后，要从本地的内存或闪存中查询出对应的键值对，处理请求后把结果发送给网络上的请求方。
如图 \ref{arch:fig:data-structure-accel}，这个过程称为数据结构处理的数据面，通常不需要控制面介入。
然而，由于可编程网卡上不适合运行复杂的逻辑，我们把内存分配器分为网卡和主机 CPU 两部分。网卡上缓存若干固定大小的空闲内存块。当空闲内存块不足时，需要主机 CPU 上的控制面通过拆分更大的内存块来补充空闲内存块；当空闲内存块过多时，又需要主机 CPU 来进行垃圾回收，合并成更大的内存块。
通过网卡直接访问内存数据结构的另一个挑战是网卡与内存之间的 PCIe 带宽较低、延迟较高。为此，第 \ref{chapter:kvdirect} 章设计了一系列优化方法来节约带宽、通过并发处理来隐藏延迟。
尽管请求是并发处理的，第 \ref{chapter:kvdirect} 章的设计还能保证多个客户端并发访问的强一致性，即请求在逻辑上按照网络接收的顺序被依次处理。
如果存储节点同时也作为计算节点运行虚拟机，为了解决本地和远程访问同一块存储区域时的一致性问题，不管是本地还是远程访问，都经过可编程网卡处理。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/data_structure_accel.pdf}
	\caption{用可编程网卡加速数据结构处理后的架构。}
	\label{arch:fig:data-structure-accel}
\end{figure}

\section{操作系统加速}

最后一步，我们将操作系统中需要高性能的功能拆分为三部分，分别在可编程网卡、主机 CPU 的用户态运行库和主机 CPU 的用户态守护进程（daemon）中处理。
如图 \ref{arch:fig:os-primitives-accel}，操作系统在图中被替换成了用户态运行库，而可编程网卡中增加了传输协议的功能。
用户态运行库通过替换标准库（如 libc），截获了应用程序的系统调用，从而可以在用户态实现操作系统功能的一部分，而把另一部分功能卸载到可编程网卡。
用户态守护进程主要用于控制面操作，为简化起见，图 \ref{arch:fig:os-primitives-accel} 中没有画出。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/os_primitives_accel.pdf}
	\caption{用可编程网卡加速操作系统通信原语后的架构。}
	\label{arch:fig:os-primitives-accel}
\end{figure}

操作系统中包括通信、存储等子系统，本文以通信系统的加速为例。套接字是应用程序最常用的通信原语，但由于操作系统的多种开销，其性能不令人满意。
第 \ref{chapter:socksdirect} 章设计并实现了一个高性能的用户态套接字系统，与现有应用程序完全兼容，并且对主机内进程间通信和跨主机的通信都能达到接近硬件极限的低延迟和高吞吐量。
系统由可编程网卡上的可靠通信协议和主机 CPU 上运行的用户态库、用户态守护进程三部分构成。
对于跨主机的通信，数据面由可编程网卡和用户态库构成，可编程网卡负责多路复用和可靠传输等低层语义，提供远程直接内存访问（RDMA）原语；用户态库负责把 RDMA 原语封装成 Linux 虚拟文件系统（Virtual File System）的套接字语义，提供无锁消息队列、缓冲区管理、等待事件、零拷贝内存页面重映射等高层语义。
对于主机内的通信，数据面由 CPU 的硬件内存一致性协议（coherence protocol）和用户态库构成。用户态库在进程间建立共享内存队列，并依靠 CPU 的内存一致性协议自动同步。用户态库的功能与跨主机通信类似。
用户态守护进程负责控制面，即初始化、进程创建和退出、连接建立和关闭、与 RDMA 网卡建立队列、在进程之间建立共享内存队列等。

在第 \ref{chapter:socksdirect} 章的设计中，客户应用程序通过 SocksDirect 运行库，直接访问可编程网卡中的 RDMA 功能，不需要经过操作系统内核和虚拟机监控器，可编程网卡也就不需要支持 SR-IOV 硬件虚拟化。



\section{可编程网卡}

在结束基于可编程网卡的数据中心系统架构介绍后，本节介绍可编程网卡内部的软硬件架构。如图 \ref{arch:fig:sw-hw-codesign} 所示，可编程网卡内的逻辑主要由第 \ref{chapter:clicknp} 章的编程框架、第 \ref{chapter:kvdirect} 章的基础服务中间件以及第 \ref{chapter:kvdirect} 章和第 \ref{chapter:socksdirect} 章的应用层构成。主机 CPU 上配套的软件包括第 \ref{chapter:clicknp} 章的 FPGA 通信库和驱动程序、第 \ref{chapter:kvdirect} 章的键值操作库和第 \ref{chapter:socksdirect} 章兼容 Linux 操作系统的套接字通信库。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/sw_hw_codesign.pdf}
	\caption{软硬件协同设计的可编程网卡架构。}
	\label{arch:fig:sw-hw-codesign}
\end{figure}


\subsection{硬件架构}


图 \ref{arch:fig:smartnic-current} 显示了本文使用的 Catapult 可编程网卡 \cite{putnam2014reconfigurable} 硬件架构。
本文第 \ref{chapter:clicknp} 章和第 \ref{chapter:kvdirect} 章使用其中的 FPGA 可重构逻辑来实现网络功能和数据结构处理；第 \ref{chapter:socksdirect} 章使用其中的商用 RDMA 网卡实现套接字原语中的硬件传输协议部分。
来自数据中心网络的数据包从 FPGA 左上角的网络接口被可编程网卡接收。如果它是网络功能或者数据结构处理需求，就直接在 FPGA 可重构逻辑中进行处理，处理过程中 FPGA 需要使用板上 DRAM 和通过 CPU 互连（如 PCIe）访问主机 DRAM。
如果数据包是用于第 \ref{chapter:socksdirect} 章中的套接字通信，它将在 FPGA 中进行虚拟网络的解封装，并通过 FPGA 与商用 RDMA 网卡之间的网络接口发送给商用 RDMA 网卡。
商用 RDMA 网卡会把数据包的内容通过 CPU 互连（如 PCIe） DMA 发送给主机上的用户态套接字库。


\begin{figure}[htbp]
	\centering
	\subfloat[本文实验使用的 Catapult 可编程网卡。]{
		\includegraphics[width=0.4\textwidth]{figures/smartnic-current.pdf}
		\label{arch:fig:smartnic-current}
	}
	\hspace{0.05\textwidth}
	\subfloat[未来的片上系统。]{
		\includegraphics[width=0.5\textwidth]{figures/smartnic-soc.pdf}
		\label{arch:fig:smartnic-soc}
	}
	\caption{可编程网卡结构图。}
\end{figure}


图 \ref{arch:fig:smartnic-current} 所示的可编程网卡架构有三个局限性。
首先，现有的商用 RDMA 网卡当并发连接数较多时，性能会急剧下降 \cite{mprdma}。我们希望利用第 \ref{chapter:kvdirect} 章可扩放键值存储的技术，在 FPGA 可重构逻辑中实现 RDMA 硬件传输协议，实现高并发连接数下的高性能。
其次，FPGA 只适合加速数据面，控制面仍然留在主机 CPU 上。尽管它的计算量不大，但为了性能隔离，计算节点仍然需要预留少量 CPU 核用于控制面处理。第 \ref{chapter:intro} 章已经指出，即使预留一个物理 CPU 核也是相当昂贵的。为此，我们希望在可编程网卡中加入 ARM 多核处理器，用于实现控制面。ARM 多核处理器的成本为数十美元，远低于一个物理 CPU 核的成本。
最后，一些类型的工作负载在 FPGA 内实现的效率不是很高，应当固化在 ASIC 加速器中。第一类是深度学习和机器学习中的向量操作、加密解密操作等计算密集型操作。例如，Intel QuickAssist 加速卡 \cite{intel-qat} 基于 ASIC 的 RSA 非对称加密比第 \ref{chapter:clicknp} 章基于 FPGA 的实现，吞吐量约高 10 倍；基于 ASIC 的 LZ77 压缩算法比我们基于 FPGA 的实现，吞吐量也高一个数量级。所用 ASIC 和 FPGA 芯片的功耗、面积和制程都接近。
第二类是常见数据结构和调度队列。基于内容寻址内存（Content-Addressable Memory，CAM）的查找表是哈希表、乱序执行引擎、缓存、模糊匹配表等多种常见数据结构的必要组件。CAM 在 ASIC 中可以用三态门实现，而在 FPGA 中实现的效率较低。此外，优先队列（可用移位寄存器序列或堆实现）、轮转（round-robin）调度队列、考虑依赖关系的乱序执行调度器等结构在很多应用中广泛使用，将其硬化也是有意义的。
因此，本文希望未来的可编程网卡使用如图 \ref{arch:fig:smartnic-soc} 所示的片上系统架构。
位于片上系统中心的 FPGA 不仅提供了可编程性和计算能力，也可以灵活互连片上的各种计算加速器，组建定制化的内存层次结构，还可以灵活互连主机内外的各种硬件设备，组成数据中心智能互连（intelligent fabric）。
由于硬件平台的限制，本文尚未实现这样的硬件架构，这将作为未来的研究工作。



\subsection{高级语言编程框架}

FPGA 高级语言编程：ClickNP，适合流式处理的模块化 FPGA 高级语言编程

虚拟机热迁移，计算节点对应的网卡状态；网络、存储节点热迁移（升级，扩容等），网卡状态；高可用性，故障恢复……

\subsection{基础服务中间件}



第 \ref{chapter:kvdirect} 章提出的 FPGA 内有状态处理（stateful processing）、数据结构。
第 \ref{chapter:clicknp} 章的有状态网络功能、第 \ref{chapter:kvdirect} 章的内存键值存储、第 \ref{chapter:socksdirect} 章的可扩放 RDMA 的基础。
尽管第 \ref{chapter:clicknp} 章的 ClickNP 在第 \ref{chapter:kvdirect} 章之前发表，发表时的有状态处理和哈希表是使用与应用逻辑耦合紧密的方式实现的，既不容易扩放到大量并发连接，代码的可维护性也不强。
使用第 \ref{chapter:kvdirect} 章的 KV-Direct 作为基础后，这些网络功能的实现将变得模块化和可扩放。

本文使用高级语言编程框架实现基础服务中间件，中间件未来可以硬件化。
