\section{Background}
\label{socksdirect:sec:background}


\subsection{Introduction to Linux Sockets}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Fraction of CPU time in the kernel (socket connection setup, socket data transmission and non-socket system calls) and user-mode applications.}
%	\label{socksdirect:fig:socket-kernel-time}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Performance of back-end systems using inter-server socket, intra-server socket, inter-server RDMA and intra-server shared memory.}
%	\label{socksdirect:fig:backend-performance}
%\end{figure}


%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Latency comparison of mutual exclusion mechanisms (CAS, mutex, futex) and cache migration.}
%	\label{socksdirect:fig:mutual-exclusion}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Throughput (bar) and latency (line) for 16B messages using inter-server TCP socket, inter-server RDMA, intra-server TCP socket, UNIX socket, pipe and shared memory queue.}
%	\label{socksdirect:fig:socket-comparison}
%\end{figure}

Sockets are the standard communication primitives among applications, containers, and hosts. Figure \ref{socksdirect:fig:socket-pseudo-code} illustrates the pseudocode of a typical server application using socket primitives. First, the server creates a socket file descriptor \texttt{lfd} for listening to ports, receiving new connections, and sets it to non-blocking for asynchronous processing. Then it creates an event file descriptor \texttt{efd} for receiving new connection events and events of data transmission on each connection. 

Next, it enters the event loop. For each received event, if it is a new connection, it calls \texttt{accept} to accept it and adds it to event monitoring. If data arrives on an existing connection, it receives all data on that connection (because of the size limit of the receive buffer, a single \texttt{recv} may not receive all data). If the peer is ready to receive (i.e., the receive buffer has free space), it sends out the data to be sent.

The translation of your document is as follows:

\begin{figure}[htbp]
\small
\begin{lstlisting}[style=myCStyle]
int lfd = socket(...); // listen file descriptor (fd)
bind(lfd, listen_addr_and_port, ...);
listen(lfd, BACKLOG);
fcntl(lfd, F_SETFL, fcntl(lfd,F_GETFL,0) | O_NONBLOCK);
int efd = epoll_create(MAXEVENTS); // event fd
epoll_ctl(efd, EPOLL_CTL_ADD, lfd, ...);
while (true) { // main event loop
	int n = epoll_wait(efd, events, MAXEVENTS, 0);
	for (int i=0; i<n; i++) { // iterate events
		if (events[i].data.fd == lfd) { // new connection
			int cfd = accept(sfd, ...); // connection fd
			epoll_ctl(efd, EPOLL_CTL_ADD, cfd, ...);
			fcntl(cfd,F_SETFL,fcntl(cfd,F_GETFL,0)|O_NONBLOCK);
		}
		else if (events[i].events & EPOLLIN){//ready to recv
			do { // fetch all received data
				cnt = recv(events[i].data.fd, recvbuf, buflen);
				recvbuf = next_recv_buf();
			} while (cnt > 0);
			// do processing
		}
		else if (events[i].events & EPOLLOUT){//ready to send
			do { // flush send buf
				cnt = send(events[i].data.fd, sendbuf, sendlen);
				sendbuf += cnt; sendlen -= cnt;
			} while (cnt > 0 && sendlen > 0);
		}
	}
}
\end{lstlisting}

\caption{Pseudocode of a typical socket server application, illustrating the most crucial socket operations. Socket connections are identified by the integer \emph {FD} (file descriptor), a FIFO byte stream channel. Linux employs a readiness-driven I/O multiplexing model, where the operating system informs the application which file descriptors are ready to receive or send, and then the application can prepare the buffer and execute socket operations.}
\label{socksdirect:fig:socket-pseudo-code}
\end{figure}

TCP sockets in contemporary operating systems typically serve three functions: (1) addressing, locating, and connecting to another application; (2) providing a reliable and ordered communication channel, identified by the integer \emph {file descriptor}; (3) polling events from multiple channels, such as poll and epoll. Most Linux applications utilize a readiness-driven I/O multiplexing model, that is, the operating system informs the application which file descriptors are ready to receive or send, and then the application can prepare the buffer and initiate receive or send operations.

\subsection{Overhead in Linux Sockets}
\label{socksdirect:subsec:motivation}

Modern data center networks have microsecond latency and tens of Gbps throughput. However, traditional Linux sockets are implemented in the operating system kernel space with shared data structures, making sockets a well-known bottleneck for communication-intensive applications running on multiple hosts \cite {barroso2017attack}. In addition to inter-host communication, microservices and containers on the same host often communicate with each other, making intra-host socket communication increasingly important in the cloud era. Under stress testing, applications such as Nginx~ \cite {reese2008nginx}, Memcached~ \cite {fitzpatrick2004distributed} and Redis~ \cite {carlson2013redis} consume 50\% to 90\% of CPU time in the kernel, mainly for handling TCP socket operations \cite{jeong2014mtcp}.

Conceptually, the Linux network protocol stack consists of three layers. First, the VFS layer provides the socket API (such as \emph {connect}, \emph {send} and \emph {epoll}) to applications. Socket connections are bidirectional, reliable, and ordered pipelines, identified by the integer \emph {file descriptor}.
Second, the traditional TCP/IP transport layer provides I/O multiplexing, congestion control, packet loss recovery, routing, and Quality of Service (QoS) functions.
Third, the network card layer communicates with network card hardware (or virtual loopback interface for intra-host sockets) to send and receive packets.
It is well known that the VFS layer contributes a large part of the cost in the network protocol stack \cite {clark1989analysis,boyd2010analysis}.
This can be verified by a simple experiment: the latency and throughput of a Linux TCP socket between two processes in a host are only slightly worse than those of a pipe, FIFO, and Unix domain socket. (In Table \ref {socksdirect:tab:operation-performance}, the Linux TCP latency is 11 $\mu$s, throughput is 0.9 M op/s, and the latency of pipe, FIFO, and Unix domain socket is 8~9 $\mu$s, throughput is 0.9~1.2 M op/s.)
Pipes, FIFOs, and Unix domain sockets bypass the transport and network card layers, but their performance is still unsatisfactory.

The seminal work of Clark et al.~\cite{clark1989analysis} categorized socket overhead into per-packet and per-byte costs. In contemporary protocol stacks, due to the substantial cost of connection establishment~\cite{jeong2014mtcp,lin2016scalable}, we propose a new type of cost: per-connection cost. Given that each socket operation incurs a certain cost at the VFS layer, irrespective of the number of packets it manages (some operations, such as \texttt{dup2}, do not manage packets at all), we introduce another new type of cost: per-operation cost.
Subsequently, we will classify socket overhead into four types: per operation, per packet, per byte, and per connection.

\begin{table}[t]
	\centering
	\caption{Overhead of Linux sockets.}
	\label{socksdirect:tab:overhead}
	\small
		\begin{tabularx}{1\textwidth}{l|X|X}
			\hline
			Type & Overhead & Solution in this chapter \\
			\hline
			\hline
			Per operation & Kernel crossing (system call) & User-space library (\S\ref{socksdirect:sec:architecture}) \\
			\hline
			Per operation & Socket file descriptor lock for concurrent threads and processes & Token-based socket sharing (\S\ref{socksdirect:subsec:fork}) \\
			\hline
			\hline
			Per packet & Transport layer protocol (TCP/IP) & Using RDMA or shared memory (\S\ref{socksdirect:subsec:lockless-queue}) \\
			\hline
			Per packet & Buffer management & New ring buffer design (\S\ref{socksdirect:subsec:lockless-queue}) \\
			\hline
			Per packet & I/O multiplexing & Using RDMA or shared memory (\S\ref{socksdirect:subsec:lockless-queue}) \\
			\hline
			Per packet & Interrupt handling & Event notification (\S\ref{socksdirect:subsec:process-mux}) \\
			\hline
			Per packet & Process wakeup & Event notification (\S\ref{socksdirect:subsec:process-mux}) \\
			\hline
			\hline
			Per byte & Data copying & Page remapping (\S\ref{socksdirect:subsec:zerocopy}) \\
			\hline
			\hline
			Per connection  & Kernel file descriptor allocation & File descriptor remapping table (\S\ref{socksdirect:subsec:connection-management}) \\
			\hline
			Per connection & TCB lock management & Dispatch to \libipc{} (\S\ref{socksdirect:subsec:connection-management}) \\
			\hline
			Per connection & Dispatch new connection & Daemon process (\S\ref{socksdirect:subsec:connection-management}) \\
			\hline
		\end{tabularx}
\end{table}

\subsubsection{Overhead per operation}
\label{socksdirect:subsec:per-operation-overhead}

\parab {Kernel crossing.}
Traditionally, the socket API is implemented in the kernel, so a kernel crossing (i.e., system call) is required for each socket operation. Worse, to prevent Meltdown \cite {Lipp2018meltdown} attacks, the Kernel Page Table Isolation (KPTI) patch \cite {kpti} makes kernel crossing 4 times more expensive, as shown in Table \ref {socksdirect:tab:operation-performance} (before the KPTI patch, kernel crossing required 50~ns, and after KPTI, it required 200~ns).
The goal of this chapter is to bypass the kernel without compromising security (\S\ref{socksdirect:sec:architecture}).

\parab {Socket file descriptor lock.}
Many applications are multithreaded for two reasons.
First, unlike FreeBSD, the asynchronous interface for reading and writing disk files in Linux cannot take advantage of operating system cache and buffer, so applications continue to use multithreading and synchronous interfaces \cite {nginx-multi-thread}.
Second, many web application frameworks prefer to handle each user request with a synchronous programming model because it is easier to write and debug \cite {barroso2017attack}.
Multiple threads in a process share socket connections. In addition, after a process forks, the parent process and child process share existing sockets. Sockets can also be passed to another process through Unix domain sockets. To protect concurrent operations, the Linux kernel acquires a lock for each socket operation \cite {boyd2010analysis,han2012megapipe,lin2016scalable}. Table \ref {socksdirect:tab:operation-performance} shows that even without multicore contention, the latency of a shared memory queue protected by atomic operations is up to 4 times that of a lock-free queue, and the throughput is only 22% of that of a lock-free queue.
The goal of this chapter is to minimize synchronization overhead as much as possible by optimizing common cases and removing synchronization operations from common socket operations (\S\ref{socksdirect:subsec:fork}).

\parab{Intra-host communication.}
Most existing approaches for intra-host socket either use kernel network stack or network card loopback.
The kernel network stack has evolved to become quite complicated over the years~\cite{yasukata2016stackmap}, which is an overkill for intra-host communication. % may not need many TCP features, e.g., congestion control and loss recovery.

Arrakis uses the network card to forward packets from one application to another.
As shown in Table~\ref{socksdirect:tab:operation-performance}, the hairpin latency from CPU to network card is still 25x higher than inter-core cache migration delay ($\sim$30 ns). The throughput is also limited by Memory-Mapped I/O (MMIO) doorbell latency and PCIe bandwidth~\cite{neugebauer2018understanding,li2017kv}.

We aim to leverage user-space shared memory for intra-host socket communication.

The main challenge for leveraging RDMA for inter-host socket communication is to bridge the semantics of socket and RDMA~\cite{dragojevic2014farm}.
For example, RDMA preserves messages boundaries while TCP does not.
For I/O multiplexing, RDMA provides a completion notification model while event polling in Linux socket requires a readiness model~\cite{han2012megapipe}.
Further, one-sided and two-sided RDMA verbs have different efficiency and overheads~\cite{kalia2014using,kaminsky2016design}.

We aim to use RDMA efficiently for inter-host socket communication, while falling back to TCP transparently in case of non-RDMA peers.

\parab{Many concurrent connections.}
Internet facing applications often need to serve millions of concurrent connections efficiently~\cite{jeong2014mtcp,lin2016scalable,belay2017ix}.
Moreover, it is also common for two backend applications to create a large number of connections between them, where each connection handles a concurrent task~\cite{ihm2011towards,jang2011sslshader,nishtala2013scaling}. In Linux, a socket connection has dedicated send and receive buffers, each is at least one page (4~KB) in size~\cite{davidskbs}. With millions of concurrent connections, the socket buffers can consume gigabytes of memory, most of which is empty. Random accesses to a large number of buffers also cause CPU cache misses and TLB misses. Similar issue exists in RDMA network cards with limited on-chip memory for caching connection states~\cite{mprdma,kaminsky2016design}.

The translation of your text is as follows:

We aim to minimize memory cache misses per data transmission by multiplexing socket connections.

\begin{table}[t]
	\centering
	\caption{Round-trip latency and single-core throughput operations (test platform settings in \S \ref {socksdirect:subsec:methodology}). Unless otherwise specified, the message size is 8 bytes.}
	\label{socksdirect:tab:operation-performance}
	\small
		\begin{tabular}{l|l|l|}
			\hline
			Operation	& Latency  & Throughput  \\
			& ($\mu$s) & (M operations per second) \\
			\hline
			\hline
			Inter-core cache migration	& 0.03 & 50 \\
			\hline
			Polling 32 empty queues & 0.04 & 24 \\
			\hline
			System call (before KPTI) & 0.05 & 21 \\
			\hline
			Spinlock (no contention) & 0.10 & 10 \\
			\hline
			Allocation and release of buffer & 0.13 & 7.7 \\
			\hline
			Spinlock (with contention) & 0.20 & 5 \\
			\hline
			Lock-free shared memory queue & 0.25 & 27 \\
			\hline
			\textbf{Intra-host \sys} & 0.30 & 22 \\
			\hline
			System call (after KPTI) & 0.20 & 5.0 \\
			\hline
			Copying 1 memory page (4~KiB) & 0.40 & 5.0 \\
			\hline
			Cooperative context switch & 0.52 & 2.0 \\
			\hline
			Mapping a memory page (4~KiB) & 0.78 & 1.3 \\
			\hline
			Intra-host communication via network card & 0.95 & 1.0 \\
			\hline
			Atomic shared memory queue & 1.0 & 6.1 \\
			\hline
			Mapping 32 memory pages (128~KiB) & 1.2 & 0.8 \\
			\hline
			Opening a socket file descriptor & 1.6 & 0.6 \\
			\hline
			Unilateral RDMA write operation & 1.6 & 13 \\
			\hline
			Bilateral RDMA send / receive operation & 1.6 & 8 \\
			\hline
			\textbf{Inter-host \sys} & 1.7 & 8 \\
			\hline
			Process awakening & 2.8$\sim$5.5 & 0.2$\sim$0.4 \\
			\hline
			Linux pipe / FIFO & 8 & 1.2 \\
			\hline
			Unix domain sockets in Linux & 9 & 0.9 \\
			\hline
			Inter-host Linux TCP sockets & 11 & 0.9 \\
			\hline
			Copying 32 memory pages (128~KiB) & 13 & 0.08 \\
			\hline
			Inter-host Linux TCP sockets & 30 & 0.3 \\
			\hline
		\end{tabular}	
\end{table}


\subsubsection{Overhead per packet}
\label{socksdirect:subsec:per-packet-overhead}

\parab {Transport Protocol (TCP / IP).}
Traditionally, TCP/IP has been the de facto standard for data center transport protocols.
The processing of TCP/IP protocol, congestion control, and packet loss recovery consume CPU on every sent and received packet.
Furthermore, packet loss detection, rate-based congestion control, and the TCP state machine employ timers, making it challenging to achieve microsecond granularity and low overhead \cite{jeong2014mtcp}.
Fortunately, in recent years, we have seen large-scale deployment of RDMA in many data centers \cite {guo2016rdma,zhu2015congestion,mittal2015timely}.
RDMA offloads the transport protocol to the RDMA network card, providing a hardware-based transport layer equivalent to TCP/IP.
For inter-host sockets, the aim of this chapter is to leverage the high throughput, low latency, and near-zero CPU overhead of the RDMA hardware transport layer (\S\ref{socksdirect:subsubsec:fork_rdwr}).
For intra-host sockets, the aim of this chapter is to completely bypass the transport layer.

\parab {Buffer Management.}
Traditionally, the CPU sends and receives packets from the network card through a \emph {ring buffer}.
The ring buffer comprises a fixed number of fixed-length metadata entries.
Each entry points to a buffer that stores the packet payload.
To send or receive a packet, it is necessary to allocate and release buffers.
Table \ref {socksdirect:tab:operation-performance} shows the cost of the ring buffer.
Additionally, to ensure that packets of MTU size can be received, each receive buffer should be at least the size of an MTU.
However, many packets are smaller than the MTU~ \cite {thompson1997wide}, so internal fragmentation reduces memory utilization.
Although modern network cards support LSO and LRO~ \cite {lsolro} to batch process multiple packets, the aim of this chapter is to completely eliminate the overhead of buffer management (\S\ref{socksdirect:subsec:lockless-queue}).

\parab {I/O Multiplexing.}
For traditional network cards, packets from different connections are usually mixed in a ring buffer, so the network protocol stack needs to classify the packets into the corresponding socket buffers. Modern network cards support packet steering \cite {mellanox}, which can map specific connections to dedicated ring buffers used by high-performance socket systems \cite {jeong2014mtcp,lin2016scalable,libvma}. This chapter utilizes similar functionality in RDMA network cards to demultiplex received packets into ring buffers dedicated to each connection.

\parab {Interrupt Handling.}
The Linux network protocol stack is divided into system call and interrupt contexts because it handles events from both applications and hardware devices. For instance, when an application calls send, the network protocol stack sends the packet in the process context (if the window allows). When the network card receives the packet, it sends an interrupt to the CPU, and then the network protocol stack processes the received packet in the interrupt context. The ACK clocking mechanism in TCP congestion control \cite {mprdma} requires timely handling of interrupts and timers. The interrupt context is not necessarily on the same core as the application, leading to a decrease in CPU core locality. However, RDMA network card hardware implements packet processing that requires precise timing, so the host CPU no longer needs to handle most of the data plane interrupts.

\parab {Process Awakening.}
When a process calls a remote procedure call (RPC) and waits for a reply, should the CPU switch to other ready-to-run processes? The answer in Linux is yes, and this process switching wake-sleep process takes 3 to 5 $\mu$s, as shown in Table \ref {socksdirect:tab:operation-performance}. Within the round-trip time of RPC within the host, two process awakenings contribute more than half of the latency. For host-to-host RPC via RDMA, the round-trip latency of small messages smaller than the MTU size on the network is even lower than the process awakening latency. For this reason, many distributed systems and user-space protocol stacks use polling to avoid awakening overhead. However, simple polling methods consume a CPU core for each thread and cannot scale to a large number of threads. To hide microsecond-level RPC latency \cite {barroso2017attack}, cooperative context switching through \texttt {sched\_yield} is much faster than process awakening. The goal of this chapter is to efficiently share cores among multiple threads (\S\ref{socksdirect:subsec:process-mux}).

\parab {Container Networking.}
Many container deployments use isolated network namespaces, and these containers communicate through a virtual overlay network. In Linux, a virtual switch \cite {pfaff2015design} forwards packets between the host network card and the virtual network card in the container. This architecture incurs overhead from multiple context switches and memory copies on each packet, and the virtual switch becomes a bottleneck \cite {pfefferle2015hybrid}. Slim~ \cite {slim} reduces three kernel round trips to one. Several recent works \cite {martins2014clickos,roghanchi2017ffwd,huang2017high,nsdi19freeflow} delegate all operations to a virtual switch running as a daemon, thus increasing latency and CPU cost on the data path. The solution in this chapter is a centralized control plane and a distributed data plane (\S\ref{socksdirect:subsec:connection-management}).

The content you provided is already in English and in academic style. As per your instructions, I will keep it as-is. Here is the content:

\subsubsection{Per-byte Overhead}
\label{socksdirect:subsec:per-byte-overhead}

\parab {Payload Copying.}
In most socket systems, the semantics of \texttt {send} and \texttt {recv} result in memory copying between the application and the network protocol stack. For non-blocking \texttt {send}, the system needs to copy data out of the buffer because the application may overwrite the buffer immediately after \texttt {send} returns. Simply eliminating the copy may violate the correctness of the application. Zero-copy \texttt {recv} is even more challenging than \texttt {send}. Linux provides a readiness-based event model, i.e., the application knows about incoming data (e.g., through \texttt {epoll}) and then calls \texttt {recv}, so data received by the network card but not delivered to the application must be stored in the system buffer. Because \texttt {recv} allows the application to provide any buffer as the data target, the system needs to copy data from the system to the application buffer. The goal of this chapter is to implement zero-copy for larger payload transfers in standard socket applications (\S\ref{socksdirect:subsec:zerocopy}).

\subsubsection{Per-connection Overhead}
\label{socksdirect:subsec:per-connection-overhead}

\parab {Kernel File Descriptor Allocation.}
In Linux, each socket connection is a file in VFS, thus requiring the allocation of integer file descriptors and \emph {inode}.
The challenge with user-space sockets is that many APIs (such as open, close, and epoll) support both socket and non-socket file descriptors (such as files and devices), so socket file descriptors must be distinguished from other file descriptors.
Linux-compatible sockets in user space \cite {libvma,rsockets} typically open a file in the kernel to obtain a virtual file descriptor for each socket, so they still require kernel file descriptor allocation.
LOS~ \cite {huang2017high} divides the file descriptor space into user and kernel parts, but violates the Linux property of allocating the smallest available file descriptor.
However, many applications, such as Redis~ \cite {redis} and Memcached~ \cite {memcached}, rely on this property.
The goal of this chapter is to bypass kernel socket file descriptor allocation while maintaining compatibility (\S\ref{socksdirect:subsec:connection-management}).

The content you provided is already in English and it follows an academic style. As per your instructions, I will not make any changes to it.

\parab {Optimization of Kernel Network Protocol Stack:}
The first type of work is the optimization of the kernel TCP/IP protocol stack. FastSocket~\cite{lin2016scalable}, Affinity-Accept~\cite{pesterev2012improving}, FlexSC~\cite{soares2010flexsc}, and zero-copy sockets \cite{thadani1995efficient,chu1996zero,linux-zero-copy} achieve good compatibility and isolation.

MegaPipe~\cite{han2012megapipe} and StackMap~\cite{yasukata2016stackmap} propose new APIs to implement zero-copy and improved I/O multiplexing, at the cost of requiring modifications to the application. However, a significant amount of kernel overhead still exists. The challenge of supporting zero-copy is socket semantics.

\parab {User-space TCP/IP Protocol Stack:}
The second type of work completely bypasses the kernel TCP/IP protocol stack and implements TCP/IP in user-space. In this category, IX~\cite{belay2017ix} and Arrakis~\cite{peter2016arrakis} are new operating system architectures that use virtualization to ensure security and isolation. IX uses LwIP~\cite{dunkels2001design} to implement TCP/IP in user-space, while using the kernel to forward each packet to achieve performance isolation and QoS. In contrast, Arrakis offloads QoS to the network card, thus bypassing the kernel on the data plane. These works use the network card to forward packets between applications on the same host. As shown in Table \ref{socksdirect:tab:operation-performance}, the round-trip (hairpin) latency from the CPU to the network card is much higher than the cache migration latency between cores. The throughput is also limited by the doorbell latency of memory-mapped I/O (MMIO) and PCIe bandwidth \cite{neugebauer2018understanding,li2017kv}.

In addition to these new operating system architectures, many user-space sockets on Linux use high-performance packet I/O frameworks, such as Netmap~\cite{rizzo2012netmap}, Intel DPDK~\cite{dpdk}, and PF_RING \cite{pf-ring}, to directly access network card queues in user-space. SandStorm~\cite{marinos2014network}, mTCP~\cite{jeong2014mtcp}, Seastar~\cite{seastar}, and F-Stack~\cite{fstack} propose new APIs, thus requiring modifications to the application. Most API changes aim to support zero-copy, while the standard API still copies data. FaSST~\cite{kalia2016fasst} and eRPC~\cite{kalia2018datacenter} provide an RPC API instead of sockets. LibVMA~\cite{libvma}, OpenOnload~\cite{openonload}, DBL~\cite{dbl}, LOS~\cite{huang2017high}, and TAS~\cite{taseurosys19} comply with the standard socket API.

The user-space TCP/IP protocol stack offers superior performance compared to Linux, but it still falls short when compared to RDMA and shared memory. A significant reason for this is that existing works do not support the sharing of sockets between threads and processes, which results in compatibility issues in fork and container hot migration, as well as overhead from multi-threaded locks.

Firstly, in LibVMA and RSocket, after a process forks, for sockets created by the parent process before the fork, the child process either takes ownership of all existing sockets or cannot access any sockets (i.e., these sockets still belong to the parent process). There is no way to independently control the ownership of each socket. However, many web services \cite{apache,nginx,php-fpm,python-gunicorn,vsftpd} and key-value stores \cite{memcached} have a master process to accept new connections from the listening socket, then it may fork a child process to handle requests, and the child process needs to access the socket of the new connection. At the same time, the parent process still needs to accept new connections through the listening socket. This makes such web services unable to work properly. A more tricky situation is that the parent process and the child process can simultaneously write to the log server through the existing socket.

Secondly, multi-threading is common in applications. Applications bear the risk of race conditions in socket operations, or must adopt socket file descriptor locks for each operation. The latter method ensures correctness, but even without contention between locks, locks can impair performance.

\parab {Offloading the Transport Layer to the Network Card:}
To reduce the overhead of operating system communication primitives, a series of works offload part of the socket system to the network card hardware. The TCP Offload Engine (TOE) \cite{tcp-chimney-offload} offloads part or all of the TCP/IP protocol stack to the network card, but due to the rapid growth of general processor performance according to Moore's Law, these dedicated hardware have limited performance advantages and only achieve success in dedicated fields, such as iSCSI HBA storage cards \cite{iscsi-hba} and stateless offloads (such as checksum, Receive Side Scaling (RSS), Large Send Offload (LSO), Large Receive Offload (LRO) \cite{lsolro}). In recent years, due to hardware trends and application demands in data centers, the story of \emph{stateful} offloads has begun to revive \cite{chuanxiong-rdma-keynote}. Therefore, RDMA \cite{infiniband2000infiniband} is widely used in production data centers \cite{guo2016rdma}. RDMA provides two types of abstractions: one-sided primitives for reading and writing remote shared memory, and two-sided primitives with socket-like send-receive semantics. Compared with the software-based TCP/IP network protocol stack, RDMA uses hardware offloading to provide ultra-low latency and near-zero CPU overhead. To enable socket applications to use RDMA, RSocket \cite{rsockets}, SDP~\cite{socketsdirect}, and UNH EXS~\cite{russell2008extended} convert socket operations into two-sided RDMA primitives. They have similar designs, with RSocket being the most actively developed and the de facto standard for socket-to-RDMA conversion. FreeFlow~\cite{nsdi19freeflow} uses RDMA network cards to provide container overlay networks, using shared memory for intra-host communication and RDMA for inter-host communication. To implement RDMA virtualization, FreeFlow is essentially a microkernel architecture, with control plane and data plane operations handled by a user-space virtual switch. FreeFlow uses RSocket to convert sockets to RDMA.

However, due to the mismatch between RDMA and socket abstractions, these works have limitations. 
In terms of compatibility, first, they lack support for several important APIs, such as \textit{epoll}, thus they are incompatible with many applications, including Nginx, Memcached, Redis, etc. 
This is because RDMA only provides transport functionality, while \textit{epoll} is a file abstraction integrated with OS event notification. 
Second, RDMA QP does not support \textit{fork} and container hot migration \cite{nsdi19freeflow}, hence RSocket has the same problem. 
Third, since RSocket uses RDMA as the network packet format, it cannot connect to regular TCP/IP peers. 
This is a deployment challenge, as all hosts and applications in a distributed system must switch to RSocket simultaneously. 
The goal of this chapter is to transparently detect whether the remote end supports Rsocket, and if not, fall back to TCP/IP. 
In terms of performance, they cannot eliminate payload copy, socket file descriptor lock, buffer management, process wake-up, and per-connection overhead. 
For example, RSocket allocates buffers and copies payloads at both the sender and receiver. 
Similar to Arrakis, RSocket uses the network card for intra-host communication, leading to performance bottlenecks.

The content you provided is already in English and it's in LaTeX format, not Markdown. As per your instructions, I will keep it as-is. 

Please provide the content that needs to be translated.
