\section{背景}
\label{socksdirect:sec:background}


\subsection{Linux 套接字简介}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Fraction of CPU time in the kernel (socket connection setup, socket data transmission and non-socket system calls) and user-mode applications.}
%	\label{socksdirect:fig:socket-kernel-time}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Performance of back-end systems using inter-server socket, intra-server socket, inter-server RDMA and intra-server shared memory.}
%	\label{socksdirect:fig:backend-performance}
%\end{figure}


%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Latency comparison of mutual exclusion mechanisms (CAS, mutex, futex) and cache migration.}
%	\label{socksdirect:fig:mutual-exclusion}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Throughput (bar) and latency (line) for 16B messages using inter-server TCP socket, inter-server RDMA, intra-server TCP socket, UNIX socket, pipe and shared memory queue.}
%	\label{socksdirect:fig:socket-comparison}
%\end{figure}

套接字（socket）是应用程序，容器和主机之间的标准通信原语。
图 \ref{socksdirect:fig:socket-pseudo-code} 示意了使用套接字原语的典型服务器应用程序的伪代码。首先，服务器创建一个套接字文件描述符 \texttt{lfd} 用于监听端口、接收新连接，并设置为非阻塞以便异步处理。然后创建一个事件文件描述符 \texttt{efd} 用于接收新连接事件和各个连接发送接收数据的事件。
接下来进入事件循环，对每个接收到的事件，如果是新连接，就调用 \texttt{accept} 接收之，并加入事件监视；如果是已有连接上有数据到达，就接收该连接上的所有数据（因为有接收缓冲区大小限制，一次 \texttt{recv} 可能接收不完）；如果对端已经准备好接收（即接收缓冲区有空闲空间），就将待发送的数据发送出去。

\begin{figure}[htbp]
\small
\begin{lstlisting}[style=myCStyle]
int lfd = socket(...); // listen file descriptor (fd)
bind(lfd, listen_addr_and_port, ...);
listen(lfd, BACKLOG);
fcntl(lfd, F_SETFL, fcntl(lfd,F_GETFL,0) | O_NONBLOCK);
int efd = epoll_create(MAXEVENTS); // event fd
epoll_ctl(efd, EPOLL_CTL_ADD, lfd, ...);
while (true) { // main event loop
	int n = epoll_wait(efd, events, MAXEVENTS, 0);
	for (int i=0; i<n; i++) { // iterate events
		if (events[i].data.fd == lfd) { // new connection
			int cfd = accept(sfd, ...); // connection fd
			epoll_ctl(efd, EPOLL_CTL_ADD, cfd, ...);
			fcntl(cfd,F_SETFL,fcntl(cfd,F_GETFL,0)|O_NONBLOCK);
		}
		else if (events[i].events & EPOLLIN){//ready to recv
			do { // fetch all received data
				cnt = recv(events[i].data.fd, recvbuf, buflen);
				recvbuf = next_recv_buf();
			} while (cnt > 0);
			// do processing
		}
		else if (events[i].events & EPOLLOUT){//ready to send
			do { // flush send buf
				cnt = send(events[i].data.fd, sendbuf, sendlen);
				sendbuf += cnt; sendlen -= cnt;
			} while (cnt > 0 && sendlen > 0);
		}
	}
}
\end{lstlisting}

\caption{典型套接字服务器应用程序的伪代码，显示最重要的套接字操作。 套接字连接是由整数\emph {FD}（文件描述符）标识的FIFO字节流通道。 Linux使用就绪驱动的I/O多路复用模型，其中操作系统告诉应用程序哪些文件描述符已准备好接收或发送，然后应用程序可以准备缓冲区并发出套接字操作。}
\label{socksdirect:fig:socket-pseudo-code}
\end{figure}

现代操作系统中的TCP套接字通常具有三个功能:（1）寻址，找到并连接到另一个应用程序；（2）提供可靠且有序的通信通道，由整数 \emph {文件描述符}（File Descriptor）标识；（3）查询来自多个通道的事件，例如 poll 和 epoll。大多数Linux应用程序使用准备驱动（readiness-driven）的 I/O 多路复用模型，即操作系统告诉应用程序哪些文件描述符已准备好接收或发送，然后应用程序可以准备缓冲区并发出接收或发送操作。



\subsection{Linux 套接字中的开销}
\label{socksdirect:subsec:motivation}


现代数据中心网络具有微秒级延迟和数十Gbps吞吐量。 但是，传统的Linux套接字是在具有共享数据结构的操作系统内核空间中实现的，这使得套接字成为在多个主机上运行的通信密集型应用程序众所周知的瓶颈 \cite {barroso2017attack}。 除了主机间通信之外，同一主机上的微服务和容器经常相互通信，使得主机内套接字通信在云时代变得越来越重要。 在压力测试下，Nginx~ \cite {reese2008nginx}，Memcached~ \cite {fitzpatrick2004distributed} 和Redis~ \cite {carlson2013redis} 等应用程序在内核中消耗50\%到9\%％的CPU时间，主要用于处理TCP套接字操作 \cite{jeong2014mtcp}。

从概念上讲，Linux网络协议栈由三层组成。首先，VFS层为应用程序提供套接字API（例如\emph {connect}，\emph {send} 和 \emph {epoll}）。套接字连接是双向、可靠且有序的管道，由整数 \emph {文件描述符}（文件描述符）标识。
其次，传统的TCP/IP传输层提供I/O复用、拥塞控制、丢包恢复、路由和服务质量保证（QoS）功能。
第三，网卡层与网卡硬件（或用于主机内套接字的虚拟环回接口）通信以发送和接收数据包。
众所周知，VFS层贡献了网络协议栈中的很大一部分成本 \cite {clark1989analysis,boyd2010analysis}。
这可以通过一个简单的实验来验证：主机中两个进程之间的Linux TCP套接字的延迟和吞吐量只比管道（pipe）、FIFO和Unix域套接字（Unix domain socket）差一点。（表 \ref {socksdirect:tab:operation-performance} 中，Linux TCP 延迟为 11 $\mu$s、吞吐量为 0.9 M op/s，管道、FIFO和Unix域套接字的延迟为 8～9 $\mu$s、吞吐量为 0.9～1.2 M op/s。）
管道、FIFO和Unix域套接字绕过了传输层和网卡层，但它们的性能仍然不尽如人意。

Clark 等的经典工作~\cite{clark1989analysis} 将套接字开销划分为每数据包和每字节开销。在现代协议栈中，由于连接创建也有显著开销~\cite{jeong2014mtcp,lin2016scalable}，我们引入一类新的开销：每连接开销；由于每次套接字操作在 VFS 层有一定的开销，与其处理的数据包数量无关（有些操作，如 \texttt{dup2}，根本不处理数据包），我们引入另一类新的开销：每次操作开销。
下文将套接字开销分为四种类型：每次操作，每个数据包，每个字节和每个连接。

% categorized socket overhead into per-packet and per-byte costs.
%Because connection establishment incurs significant cost~\cite{jeong2014mtcp,lin2016scalable} and some operations do not touch payload (e.g. \texttt{dup2}, \texttt{fnctl} and \texttt{epoll}), we include two additional types of costs: per connection cost and per operation cost, as Table~\ref{socksdirect:tab:overhead} shows.

\begin{table}[t]
	\centering
	\caption{Linux 套接字的开销。}
	\label{socksdirect:tab:overhead}
	\small
		\begin{tabularx}{1\textwidth}{l|X|X}
			\hline
			类型 & 开销 & 本章的解决方案 \\
			\hline
			\hline
			每操作 & 内核穿越（系统调用） & 用户态库 (\S\ref{socksdirect:sec:architecture}) \\
			\hline
			每操作 & 并发线程和进程的套接字文件描述符锁 & 基于令牌的套接字共享 (\S\ref{socksdirect:subsec:fork}) \\
			\hline
			\hline
			每数据包 & 传输层协议（TCP/IP） & 使用 RDMA 或共享内存 (\S\ref{socksdirect:subsec:lockless-queue}) \\
			\hline
			每数据包 & 缓冲区管理 & 新的环形缓冲区设计 (\S\ref{socksdirect:subsec:lockless-queue}) \\
			\hline
			每数据包 & I/O 多路复用 & 使用 RDMA 或共享内存 (\S\ref{socksdirect:subsec:lockless-queue}) \\
			\hline
			每数据包 & 中断处理 & 事件通知 (\S\ref{socksdirect:subsec:process-mux}) \\
			\hline
			每数据包 & 进程唤醒 & 事件通知 (\S\ref{socksdirect:subsec:process-mux}) \\
			\hline
			\hline
			每字节 & 数据复制 & 页面重映射 (\S\ref{socksdirect:subsec:zerocopy}) \\
			\hline
			\hline
			每连接  & 内核文件描述符分配 & 文件描述符重映射表 (\S\ref{socksdirect:subsec:connection-management}) \\
			\hline
			每连接 & TCB 锁管理 & 分派到 \libipc{} (\S\ref{socksdirect:subsec:connection-management}) \\
			\hline
			每连接 & 分派新连接 & 守护进程 (\S\ref{socksdirect:subsec:connection-management}) \\
			\hline
		\end{tabularx}
\end{table}

\subsubsection{每次操作的开销}
\label{socksdirect:subsec:per-operation-overhead}

\parab {内核穿越（kernel crossing）。}
传统上，套接字API在内核中实现，因此需要针对每个套接字操作进行内核穿越（即系统调用）。更糟糕的是，为防止 Meltdown \cite {Lipp2018meltdown} 攻击，内核页表隔离（KPTI）补丁 \cite {kpti} 使内核穿越变得4倍昂贵，如表 \ref {socksdirect:tab:operation-performance} 所示（使用 KPTI 补丁前，内核穿越需要 50~ns，而使用 KPTI 后需要 200~ns）。
本章的目标是绕过内核而不影响安全性（\S\ref{socksdirect:sec:architecture}）。

\parab {套接字文件描述符锁。}
许多应用程序都是多线程的，原因有两个。
首先，与FreeBSD不同，用于在Linux中读写磁盘文件的异步接口无法利用操作系统缓存和缓冲区，因此应用程序继续使用多线程和同步接口 \cite {nginx-multi-thread}。
其次，许多Web应用程序框架更喜欢用同步编程模型处理每个用户请求，因为同步编程模型更容易编写和调试 \cite {barroso2017attack}。
进程中的多个线程共享套接字连接。此外，在进程fork之后，父进程和子进程共享现有套接字。套接字也可以通过Unix域套接字传递给另一个进程。为了保护并发操作，Linux内核为每个套接字操作获取每个套接字锁 \cite {boyd2010analysis,han2012megapipe,lin2016scalable}。表 \ref {socksdirect:tab:operation-performance} 表明，即使没有多核争用，受原子操作保护的共享内存队列相比无锁队列的延迟可达 4 倍，吞吐量也只有无锁队列的 22\%。
本章的目标是通过优化常见情况并删除常用套接字操作中的同步操作来尽可能降低同步开销（\S\ref{socksdirect:subsec:fork}）。

%\parab{Intra-host communication.}
%Most existing approaches for intra-host socket either use kernel network stack or 网卡 loopback.
%The kernel network stack has evolved to become quite complicated over the years~\cite{yasukata2016stackmap}, which is an overkill for intra-host communication. % may not need many TCP features, e.g., congestion control and loss recovery.

%Arrakis uses the 网卡 to forward packets from one application to another.
%As shown in Table~\ref{socksdirect:tab:operation-performance}, the hairpin latency from CPU to 网卡 is still 25x higher than inter-core cache migration delay ($\sim$30 ns). The throughput is also limited by Memory-Mapped I/O (MMIO) doorbell latency and PCIe bandwidth~\cite{neugebauer2018understanding,li2017kv}.

%We aim to leverage user-space shared memory for intra-host socket communication.


%%For inter-host communication inside data centers, there are two challenges for using RDMA.
%%The first is to transparently determine whether or not the remote host supports \sys{}.
%%We cannot use an out-of-band communication channel such as RDMA CM because it cannot pass through middleboxes in cloud network.
%%The second is to bridge the semantics of socket and RDMA~\cite{dragojevic2014farm}.
%The main challenge for leverage RDMA for inter-host socket communication is to bridge the semantics of socket and RDMA~\cite{dragojevic2014farm}.
%For example, RDMA preserves messages boundaries while TCP does not.
%For I/O multiplexing, RDMA provides a completion notification model while event polling in Linux socket requires a readiness model~\cite{han2012megapipe}.
%Further, one-sided and two-sided RDMA verbs have different efficiency and overheads~\cite{kalia2014using,kaminsky2016design}.

%We aim to use RDMA efficiently for inter-host socket communication, while falling back to TCP transparently in case of non-RDMA peers.

%\parab{Many concurrent connections.}
%Internet facing applications often need to serve millions of concurrent connections efficiently~\cite{jeong2014mtcp,lin2016scalable,belay2017ix}.
%%In addition, because a socket connection provides a FIFO abstraction and the OS offers event multiplexing, 
%Moreover, it is also common for two backend applications to create large number of connections between them, where each connection handles a concurrent task~\cite{ihm2011towards,jang2011sslshader,nishtala2013scaling}. In Linux, a socket connection has dedicated send and receive buffers, each is at least one page (4~KB) in size~\cite{davidskbs}. With millions of concurrent connections, the socket buffers can consume gigabytes of memory, most of which is empty. Random accesses to a large number of buffers also cause CPU cache misses and TLB misses. Similar issue exists in RDMA 网卡s with limited on-chip memory for caching connection states~\cite{mprdma,kaminsky2016design}.
%%, which is slower than sequential access to a single buffer~\cite{li2017kv}. 
%%This problem is exaggerated in RDMA. RDMA 网卡 caches connection states in limited on-board memory. Just over a few hundred active RDMA connections could cause cache misses and degrade performance~\cite{mprdma,kaminsky2016design}. 
%%Moreover, event-driven applications use \texttt{epoll} to detect which connections are ready for send or receive. If the event queue is separated from data queues, each \texttt{recv} operation involves two cache migrations for event and data~\cite{yasukata2016stackmap}. 
%
%We aim to minimize memory cache misses per data transmission by multiplexing socket connections.


\begin{table}[t]
	\centering
	\caption{往返延迟和单核吞吐量操作（测试平台设置在\S \ref {socksdirect:subsec:methodology} 中）。未特别说明的情况下，消息大小为8个字节。}
	\label{socksdirect:tab:operation-performance}
	\small
		\begin{tabular}{l|l|l|}
			\hline
			操作	& 延迟  & 吞吐量  \\
			& ($\mu$s) & (M 次操作每秒) \\
			\hline
			\hline
			核间缓存迁移	& 0.03 & 50 \\
			\hline
			轮询 32 个空队列 & 0.04 & 24 \\
			\hline
			系统调用（KPTI 前） & 0.05 & 21 \\
			\hline
			自旋锁（无竞争） & 0.10 & 10 \\
			\hline
			分配和释放缓冲区 & 0.13 & 7.7 \\
			\hline
			自旋锁（有竞争） & 0.20 & 5 \\
			\hline
			无锁共享内存队列 & 0.25 & 27 \\
			\hline
			\textbf{主机内 \sys} & 0.30 & 22 \\
			\hline
			系统调用（KPTI 后） & 0.20 & 5.0 \\
			\hline
			复制 1 个内存页（4~KiB） & 0.40 & 5.0 \\
			\hline
			协作式上下文切换 & 0.52 & 2.0 \\
			\hline
			映射一个内存页（4~KiB） & 0.78 & 1.3 \\
			\hline
			主机内通过网卡通信 & 0.95 & 1.0 \\
			\hline
			原子共享内存队列 & 1.0 & 6.1 \\
			\hline
			映射 32 个内存页（128~KiB） & 1.2 & 0.8 \\
			\hline
			打开套接字文件描述符 & 1.6 & 0.6 \\
			\hline
			单边 RDMA 写操作 & 1.6 & 13 \\
			\hline
			双边 RDMA 发送 / 接收操作 & 1.6 & 8 \\
			\hline
			\textbf{主机间 \sys} & 1.7 & 8 \\
			\hline
			进程唤醒 & 2.8$\sim$5.5 & 0.2$\sim$0.4 \\
			\hline
			Linux 管道 / FIFO & 8 & 1.2 \\
			\hline
			Linux 中的 Unix 域套接字 & 9 & 0.9 \\
			\hline
			主机间 Linux TCP 套接字 & 11 & 0.9 \\
			\hline
			复制 32 个内存页（128~KiB） & 13 & 0.08 \\
			\hline
			主机间 Linux TCP 套接字 & 30 & 0.3 \\
			\hline
		\end{tabular}	
\end{table}


\subsubsection{每个数据包的开销}
\label{socksdirect:subsec:per-packet-overhead}

\parab {传输协议（TCP / IP）。}
传统上，TCP/IP是数据中心传输协议的事实标准。
TCP/IP协议处理、拥塞控制和丢包恢复在每个发送和接收的数据包上消耗CPU。
此外，丢包检测，基于速率的拥塞控制和TCP状态机使用定时器，很难实现微秒级粒度和低开销 \cite{jeong2014mtcp}。
幸运的是，近年来在许多数据中心见证了RDMA的大规模部署 \cite {guo2016rdma,zhu2015congestion,mittal2015timely}。
RDMA将传输协议卸载到RDMA 网卡，提供了与TCP/IP相当的基于硬件的传输层。
对于主机间套接字，本章的目标是利用 RDMA 硬件传输层的高吞吐量、低延迟和接近零的 CPU 开销（\S\ref{socksdirect:subsubsec:fork_rdwr}）。
对于主机内套接字，本章的目标是完全绕过传输层。

\parab {缓冲管理。}
传统上，CPU通过\emph {环形缓冲区}（ring buffer）从网卡发送和接收数据包。
环形缓冲区由固定数量的固定长度的元数据条目组成。
每个条目都指向一个存储数据包有效负载的缓冲区。
要发送或接收数据包，需要分配和释放缓冲区。
表 \ref {socksdirect:tab:operation-performance} 显示了环形缓冲区的成本。
此外，为了确保可以接收MTU大小的分组，每个接收缓冲区应该具有至少一个MTU的大小。
但是，许多数据包小于MTU~ \cite {thompson1997wide}，因此内部碎片会降低内存利用率。
虽然现代网卡支持LSO和LRO~ \cite {lsolro} 以批量处理多个数据包，但本章的目标是完全消除缓冲区管理的开销（\S\ref{socksdirect:subsec:lockless-queue}）。

\parab {I/O多路复用。}
对于传统的网卡，接收到的不同连接的数据包通常在环形缓冲区中混合，因此网络协议栈需要将数据包分类到相应的套接字缓冲区中。
现代网卡支持接收数据包转向 \cite {mellanox}，它可以将特定连接映射到专用环形缓冲区，该缓冲区由高性能套接字系统使用 \cite {jeong2014mtcp,lin2016scalable,libvma}。
本章利用RDMA网卡中的类似功能，将接收到的数据包解复用（demultiplex）到每个连接专属的环形缓冲区。

\parab {中断处理。}
Linux网络协议栈分为系统调用和中断上下文，因为它处理来自应用程序和硬件设备的事件。
例如，当应用程序调用 send 时，网络协议栈进程上下文中将数据包发送出去（如果窗口允许）。当网卡接收到该数据包时，网卡向 CPU 发送一个中断，然后网络协议栈在中断上下文中处理接收到的数据包。
TCP拥塞控制中的ACK时钟（ACK clocking）机制 \cite {mprdma} 要求及时处理中断和定时器。
中断上下文不一定与应用程序在同一核心上，导致 CPU 核的局部性下降。
但是，RDMA 网卡硬件实现了需要精确计时的数据包处理，因此主机 CPU 不再需要处理大部分数据平面的中断。



\parab {进程唤醒。}
当进程调用远程过程调用（RPC）并等待回复时，是否应让CPU切换到准备运行的其他进程？
Linux的答案是肯定的，这个进程切换的唤醒睡眠过程需要3到5 $\mu$s，如表 \ref {socksdirect:tab:operation-performance} 所示。
在主机内 RPC 的往返时间内，两次进程唤醒贡献了超过一半的延迟。
对于通过 RDMA 的主机间 RPC，小于 MTU 大小的小消息在网络上的往返延迟甚至低于进程唤醒的延迟。
为此，许多分布式系统和用户态协议栈使用轮询来避免唤醒开销。
然而，简单的轮询方法会为每个线程消耗一个 CPU 核，不能扩放到大量线程。
为了隐藏微秒级RPC延迟 \cite {barroso2017attack}，通过 \texttt {sched\_yield} 的协作上下文切换比进程唤醒要快得多。
本章的目标是高效地在多个线程之间共享核心（\S\ref{socksdirect:subsec:process-mux}）。

\parab {容器网络。}
许多容器部署使用隔离的网络命名空间，这些容器通过虚拟覆盖网络（virtual overlay network）进行通信。
在Linux中，虚拟交换机 \cite {pfaff2015design} 在主机网卡和容器中的虚拟网卡之间转发数据包。
这种架构会在每个数据包上产生多个上下文切换和内存拷贝的开销，虚拟交换机成为瓶颈 \cite {pfefferle2015hybrid}。
Slim~ \cite {slim} 将三次内核往返减少到一次。
最近的几个工作 \cite {martins2014clickos,roghanchi2017ffwd,huang2017high,nsdi19freeflow} 将所有操作委托给作为守护进程运行的虚拟交换机，因此它增加了数据路径上的延迟和CPU成本。
本章的解决方案是集中式的控制平面和分布式的数据平面（\S\ref{socksdirect:subsec:connection-management}）。


\subsubsection{每字节的开销}
\label{socksdirect:subsec:per-byte-overhead}

\parab {有效载荷（payload）复制。}
在大多数套接字系统中，\texttt {send}和\texttt {recv}的语义会导致应用程序和网络协议栈之间的内存复制。 对于非阻塞\texttt {send}，系统需要将数据复制出缓冲区，因为应用程序可能会在\texttt {send}返回后立即覆盖缓冲区。 简单地删除拷贝可能会违反应用程序的正确性。
零拷贝\texttt {recv}甚至比\texttt {send}更难。
Linux提供了基于准备就绪的事件模型，即应用程序知道传入的数据（例如通过\texttt {epoll}）然后调用\texttt {recv}，因此网卡接收但未传递给应用程序的数据必须存储在系统缓冲区中。
因为\texttt {recv}允许应用程序提供任何缓冲区作为数据目标，所以系统需要将数据从系统复制到应用程序缓冲区。
本章的目标是在标准套接字应用程序中为较大有效载荷传输实现零拷贝（\S\ref{socksdirect:subsec:zerocopy}）。



\subsubsection{每条连接的开销}
\label{socksdirect:subsec:per-connection-overhead}

\parab {内核文件描述符分配。}
在Linux中，每个套接字连接都是VFS中的文件，因此需要分配整数文件描述符和\emph {inode}。
用户空间套接字的挑战是有许多API（例如open，close和epoll）同时支持套接字和非套接字文件描述符（例如文件和设备），因此必须将套接字文件描述符与其他文件描述符区分开来。
用户空间中的Linux兼容套接字 \cite {libvma,rsockets} 通常在内核中打开一个文件以获取每个套接字的虚拟文件描述符，因此它们仍然需要内核文件描述符分配。
LOS~ \cite {huang2017high} 将文件描述符空间划分为用户和内核部分，但违反了Linux分配最小可用文件描述符的属性。
但是，许多应用程序，如Redis~ \cite {redis} 和Memcached~ \cite {memcached} 都依赖于此属性。
本章的目标是在保持兼容性的同时绕过内核套接字文件描述符分配（\S\ref{socksdirect:subsec:connection-management}）。

\parab {TCP控制块管理中的锁。}
在建立连接期间，Linux获取几个全局锁来分配TCB（TCP Control Block，TCP控制块）。
最近的工作，如MegaPipe~ \cite {han2012megapipe} 和FastSocket~ \cite {lin2016scalable} 通过对全局表进行分区来减少锁争用，但正如表~ \ref {socksdirect:tab:operation-performance} 所示，非争用自旋锁是仍然很贵。
本章将工作分发到每个进程中的用户空间库\libipc {} （\S\ref{socksdirect:subsec:connection-management}）。

\parab {新连接调度。}
多个进程和线程可以侦听同一端口以接受传入连接。
在Linux中，处理\texttt {accept}调用的核心在传入连接的队列上进行竞争。
本章利用委托（delegation）比锁 \cite {roghanchi2017ffwd} 更快的事实，使用管程守护程序来分派新连接（\S\ref{socksdirect:subsec:connection-management}）。


%Most modern applications use socket in an event-driven style. similar to Figure~\ref{socksdirect:fig:socket-pseudo-code}.

%Socket is a well-known bottleneck for communication intensive applications. We stress test Nginx load balancer, memcached and Redis with one request per TCP connection or a stream of requests on pre-established TCP connections, and find that 50\%$\sim$90\% CPU time is consumed in the kernel, mostly dealing with socket system calls. The one request per connection scenario shows much lower throughput, because of overhead in connection creation.

%If we were able to mitigate the overhead associated with sockets, application performance would be 2x$\sim$10x.
%As another example, we replaced socket with RDMA and enabled zero copy gRPC in distributed Tensorflow~\cite{abadi2016tensorflow}, and training a VGG network shows 5x speedup.

%Socket is a well-known bottleneck in fast data center networks \RED{(cite)}. In a data center, we categorize systems that process queries from Internet users to be \textit{front-end} (\textit{e.g.} DNS server, load balancer and Web service), and the other systems (\textit{e.g.} database, stream processing and machine learning) to be \textit{back-end}.

%For front-end systems, as shown in Figure~\ref{socksdirect:fig:socket-kernel-time}, 70\% -- 90\% CPU time is used in socket system calls. They maintain a large number of socket connections to Internet users. Establishing a socket connection takes $\approx$40$\mu$s CPU time, so a CPU core can only serve 25K new connections per second~\cite{lin2016scalable}.

%For back-end systems, the performance using socket is significantly worse than using RDMA or shared memory. This is because back-end systems interact frequently with other nodes and services, consequently the latency is vital for their performance. As shown in Figure~\ref{socksdirect:fig:socket-comparison}, socket latency between different processes in a same server is $\approx$10~$\mu$s, even much higher than inter-server RDMA latency ($\approx$2~$\mu$s).

%\textbf{Event-driven programming.}

%\textbf{Socket process: connection setup and data transmission.}

\subsection{高性能套接字系统}
\label{socksdirect:subsec:related-work}

学术界和工业界都提出了许多高性能套接字系统，如表 \ref {socksdirect:tab:related-work} 所示。


\begin{sidewaystable}[htbp]
	\centering
	\caption{高性能套接字系统的比较。}
	\label{socksdirect:tab:related-work}
	\scalebox{0.7}{
		\begin{tabularx}{1.4\textwidth}{l|X|X|X|X|X|X|X|X|X|X|}
			\hline
			& FastSocket & MegaPipe / StackMap & IX & Arrakis & SandStorm / mTCP & LibVMA & OpenOnload & Rsocket / SDP & FreeFlow & SocksDirect \\
			\hline
			\hline
			类别 & \multicolumn{2}{c|}{内核优化} & \multicolumn{5}{c|}{用户态 TCP/IP 协议栈} & \multicolumn{3}{c|}{卸载到 RDMA 网卡} \\
			%\hline
			%Changes needed for deployment & New kernel & New kernel & New kernel & New kernel & Lib+driver & Lib+driver & Lib+driver & Lib+driver & Lib+driver +daemon & Lib+driver +daemon \\
			\hline
			\hline
			\multicolumn{11}{c|}{兼容性} \\
			\hline
			\hline
			对现有应用程序透明 & \yes &   & \yes & \yes &   & \yes & \yes & \yes & \yes & \yes \\
			\hline
			\texttt{epoll} (Nginx, Memcached 等) & \yes & \yes & \yes & \yes & \yes & \yes & \yes & & & \yes \\
			\hline
			与普通 TCP/IP 对端兼容 &	\yes & \yes & \yes & \yes & \yes & \yes & \yes & & & \yes \\
			\hline
			主机内通信 & \yes & \yes & & \yes & & & & & \yes & \yes \\
			\hline
			多个应用程序监听同一端口 & \yes & \yes & & & & & & \yes & \yes & \yes \\
			\hline
			完整的 fork 支持 & \yes & \yes & & & & & \yes & & & \yes \\
			\hline
			容器热迁移 & \yes & \yes & & & & & & & & \yes \\
			\hline
			\hline
			\multicolumn{11}{c|}{隔离性} \\
			\hline
			\hline
			访问控制策略 & 内核 & & 内核 & 内核 & & & & 内核 & 守护进程 & 守护进程 \\
			\hline
			容器 / 虚拟机间的隔离 & \yes & & \yes & \yes & & & & \yes & \yes & \yes \\
			\hline
			QoS（性能隔离） & 内核 & 内核 & 内核 & 网卡 & 网卡 & 网卡 & 网卡 & 网卡 & 守护进程 & 网卡 \\
			\hline
			\hline
			\multicolumn{11}{c|}{消除性能开销} \\
			\hline
			\hline
			数据面上的系统调用 & & & 批处理 & \yes & 批处理 & \yes & \yes & \yes & \yes & $<$16KB 消息 \\
			\hline
			套接字文件描述符锁 & & & & & & & & & & \yes \\
			\hline
			传输协议 & & & & & & & & \yes & \yes & \yes \\
			\hline
			缓冲区管理 & & & & & & & & & & \yes \\
			\hline
			I/O 多路复用与中断处理 & & 改进 & \yes & \yes & 改进 & \yes & \yes & \yes & \yes & \yes \\
			\hline
			进程唤醒 & & & & & & & & & & \yes \\
			\hline
			数据复制 & & \yes & & & \yes & & & & & $\ge$16KB 消息 \\
			\hline
			内核文件描述符分配 & & \yes &  &  & \yes & & & & & \yes \\
			\hline
			TCB 管理与连接分派 & \yes & \yes & \yes & \yes & \yes & & & & & \yes \\
			\hline
		\end{tabularx}
	}
\end{sidewaystable}



\parab {内核网络协议栈优化：}
第一类工作是优化内核TCP / IP协议栈。 FastSocket~ \cite {lin2016scalable}，Affinity-Accept~ \cite {pesterev2012improving}，FlexSC~ \cite {soares2010flexsc}和零拷贝套接字 \cite {thadani1995efficient,chu1996zero,linux-zero-copy}实现良好的兼容性和隔离性。

MegaPipe~ \cite {han2012megapipe}和StackMap~ \cite {yasukata2016stackmap}提出了新的API来实现零拷贝和改进I / O多路复用，代价是需要修改应用程序。
但是，大量的内核开销仍然存在。
支持零拷贝的挑战是套接字语义。

\parab {用户态TCP / IP协议栈：}
第二类工作完全绕过内核TCP / IP协议栈并在用户空间（user-space）中实现TCP / IP。
在这个类别中，IX~ \cite {belay2017ix}和Arrakis~\cite {peter2016arrakis}是新的操作系统架构，它使用虚拟化来确保安全性和隔离性。 IX利用LwIP~ \cite {dunkels2001design}在用户空间中实现TCP / IP，同时使用内核转发每个数据包以实现性能隔离和QoS。相比之下，Arrakis将QoS卸载到网卡，因此绕过数据平面的内核。
这些工作使用网卡在同一主机中的应用程序之间转发数据包。
如表 \ref {socksdirect:tab:operation-performance} 所示，从CPU到网卡的往返（hairpin）延迟远远高于核心间缓存迁移延迟。
吞吐量也受到内存映射I / O（MMIO）的门铃（doorbell）延迟和PCIe带宽的限制 \cite {neugebauer2018understanding,li2017kv}。

除了这些新的操作系统体系结构，许多用户空间套接字在Linux上使用高性能数据包I / O框架，例如Netmap~ \cite {rizzo2012netmap}，Intel DPDK~ \cite {dpdk}和PF\_RING \cite {pf-ring}，以便直接访问用户空间中的网卡队列。
SandStorm~ \cite {marinos2014network}，mTCP~ \cite {jeong2014mtcp}，Seastar~ \cite {seastar}和F-Stack~ \cite {fstack}提出了新的API，因此需要修改应用程序。
大多数API更改旨在支持零拷贝，标准API仍然会复制数据。
FaSST~ \cite {kalia2016fasst}和eRPC~ \cite {kalia2018datacenter}提供RPC API而不是套接字。
LibVMA~ \cite {libvma}，OpenOnload~ \cite {openonload}，DBL~ \cite {dbl}，LOS~ \cite {huang2017high} 和 TAS~\cite{taseurosys19} 符合标准套接字API。

用户空间TCP / IP协议栈提供了比Linux更好的性能，但它仍然不接近RDMA和共享内存。
一个重要原因是现有的工作都不支持在线程和进程之间共享套接字，导致fork和容器热迁移中的兼容性问题，以及多线程锁开销。

首先，在 LibVMA 和 RSocket 中，当进程fork后，对于父进程在 fork 前创建的套接字，子进程或者获取所有已有套接字的所有权，或者不能访问任何套接字（即这些套接字仍归父进程所有）。没有办法独立地控制每个套接字的所有权。
但是，许多Web服务 \cite {apache,nginx,php-fpm,python-gunicorn,vsftpd}和键值存储 \cite {memcached} 都有一个主进程来从监听套接字中接受新连接，然后它可能 fork 一个子进程来处理请求，子进程需要访问新连接的套接字。
与此同时，父进程仍需要通过监听套接字接受新连接。
这使得此类Web服务无法正常工作。
更棘手的情况是父进程和子进程可以通过现有套接字同时写入日志服务器。

第二，多线程在应用程序中很常见。
应用程序在套接字操作中承担竞争条件的风险，或者每次操作必须采用套接字文件描述符锁。
后一种方法保证了正确性，但即使没有锁之间的争用（contention），锁也会损害性能。

\parab {将传输层卸载到网卡：}
为了降低操作系统通信原语的开销，一系列工作将套接字系统的一部分卸载到网卡硬件上。
TCP 卸载引擎（TCP Offload Engine，TOE） \cite {tcp-chimney-offload} 将部分或全部的 TCP / IP 协议栈卸载到网卡，但由于通用处理器的性能按摩尔定律迅速增长，这些专用硬件的性能优势有限，仅在专用领域中获得成功，例如 iSCSI HBA 存储卡 \cite {iscsi-hba} 和无状态卸载（例如校验和、接收侧扩放（RSS）、大发送数据卸载（LSO）、大接收数据卸载（LRO） \cite {lsolro}）。
近年来，由于数据中心的硬件趋势和应用需求，\emph {有状态}卸载的故事开始复兴 \cite {chuanxiong-rdma-keynote}。
因此，RDMA \cite {infiniband2000infiniband} 在生产数据中心中广泛使用 \cite {guo2016rdma}。
RDMA 提供了两类抽象：读写远程共享内存的单边原语，以及类似套接字发送接收语义的双边原语。
与基于软件的 TCP / IP 网络协议栈相比，RDMA 使用硬件卸载来提供超低延迟和接近零的 CPU 开销。
为了使套接字应用程序能够使用 RDMA，RSocket \cite {rsockets}，SDP~ \cite {socketsdirect} 和 UNH EXS~ \cite {russell2008extended} 将套接字操作转换为双边 RDMA 原语。
它们具有相似的设计，其中 RSocket 的开发最活跃，是套接字转换 RDMA 的事实标准。
FreeFlow~ \cite {nsdi19freeflow} 利用 RDMA 网卡提供容器（container）覆盖网络（overlay network），它利用共享内存进行主机内部通信，利用 RDMA 进行主机间通信。
为了实现 RDMA 虚拟化，FreeFlow 本质上是一种微内核架构，控制面和数据面操作都由用户态的虚拟交换机处理。
FreeFlow 使用 RSocket 将套接字转换为 RDMA。

但是，由于RDMA和套接字的抽象不匹配，这些工作有局限性。
在兼容性方面，首先，它们缺乏对几个重要API的支持，例如\textit {epoll}，因此它与许多应用程序不兼容，包括Nginx，Memcached，Redis等。
这是因为RDMA仅提供传输功能，而\textit {epoll}是与OS事件通知集成的文件抽象。
其次，RDMA QP不支持\textit {fork}和容器热迁移 \cite{nsdi19freeflow}，因此RSocket也有同样的问题。
第三，由于RSocket使用RDMA作为网络数据包格式，因此无法连接到常规TCP / IP对等体。
这是一个部署挑战，因为分布式系统中的所有主机和应用程序必须同时切换到RSocket。
本章的目标是透明地检测远程端是否支持Rsocket，如果没有则回退到TCP / IP。
在性能方面，它们无法删除有效负载拷贝，套接字文件描述符锁，缓冲区管理，进程唤醒和每个连接开销。
例如，RSocket在发送方和接收方分配缓冲区并复制有效负载。
与Arrakis类似，RSocket使用网卡进行主机内通信，从而导致性能瓶颈。

%In addition, they use two-sided RDMA \texttt{send} and \texttt{recv}, but one-sided RDMA \texttt{write} has higher throughput due to lower CPU cost on receive side (Table~\ref{socksdirect:tab:operation-performance}).
%However, such simple translations suffer from throughput degradation with a large number of concurrent connections. This is because RDMA 网卡 keeps per-connection states using a $\approx$2~MB~\cite{kalia2018datacenter} on-网卡 memory as cache. With hundreds of concurrent connections, we will suffer from frequent cache misses, resulting in serious throughput degradation~\cite{mprdma,kaminsky2016design}. 


%Spinlock in kernel is implemented with Compare-And-Swap (CAS) instruction, which costs $100\sim200$~ns per acquire and release. In comparison, shared memory message passing only needs one cache migration~\cite{roghanchi2017ffwd} (30~ns).
%In comparison, shared memory message passing
%Scalability analysis of Linux system calls~\cite{boyd2010analysis,clements2015scalable} suggests that many socket operations are not commutable and thus impossible to scale for all cases. 
%For example, socket provides \texttt{SO\_REUSEPORT} option. When this option is enabled, multiple processes on one host can \texttt{bind} to the same port and incoming connections are dispatched to the listeners. During connection setup, coordination is required to allocate file descriptors and port numbers, load balance connections and allocate buffers. In addition, a socket may be shared by multiple processes and threads in a same process share socket connections. When a process forks, the parent and child processes also share the sockets created before fork.


%For inter-server data transmission, Linux kernel copies data four times: on the send side, from application to kernel socket buffer, then to 网卡 ring buffer; the receive side is similar. For intra-server, data is copied three times: from application to kernel send buffer, then to kernel receive buffer via loopback interface, and finally to another application. Each CPU core can copy $\approx$5~GiB data per second~\cite{panda2016netbricks}, that's why a kernel TCP connection is hard to saturate an 100~Gbps 网卡.
%Historically, \texttt{send} was designed as a blocking operation, and the send buffer may be overwritten by the application after \texttt{send} function call.
%In a non-blocking \texttt{send}, the process could overwrite the send buffer after \texttt{send} function call returns. To avoid race condition on the data buffer, network stack needs to copy data to an internal buffer. %before \texttt{send} returns, and then send the copied data in background.
%For the \texttt{recv} operation, application provides a buffer and read the data after \texttt{recv}.
%If we implement \texttt{send} as a lazy operation, \textit{i.e.}, the data is buffered on the sender, the receiver needs to wait for a round-trip time on \texttt{recv}.
%For this reason, most socket implementations send data to the receiver eagerly.
%Because the receiver's network stack cannot predict the user-provided \texttt{recv} address, it needs to buffer received data internally, then copy to the desired destination when \texttt{recv} is called.
%Our goal in \sys is to achieve zero copy for large data transfers without making changes to applications. We accomplish this by taking advantage of the page mapping mechanism in virtual memory. 







%\textbf{Virtual file system.}
%Each socket connection is a \textit{file descriptor} (文件描述符) in Linux. First, each new connection involves expensive \texttt{inode} and \texttt{dentry} creation in the \texttt{proc} file system, which is not scalable and rarely used except for diagnostic and monitoring utilities. Second, socket operations need to pass through the virtual file system (VFS), which introduces CPU overhead and additional latency.





%\subsection{High Performance Socket Systems}
%\label{socksdirect:sec:related}


\iffalse
\begin{table*}[t]
	\centering
	\scalebox{0.88}{
		\begin{tabular}{l|c|ccc|cc|ccc|}
			\hline
			Category	& \multicolumn{1}{c|}{Linux Opt.} & \multicolumn{3}{c|}{New Kernel Stack} & \multicolumn{2}{c|}{User-space Packet} & \multicolumn{3}{c|}{User-space Socket} \\
			\hline
			System	& FastSocket & IX & MegaPipe & StackMap & Arrakis & FreeFlow & mTCP & libvma & SocksDirect \\
			\hline
			\hline
			Socket-like API & \yes & & \yes & \yes & \yes & & \yes & \yes & \yes \\
			\hline
			Linux Compatible & \yes & & & & & & & \yes & \yes \\
			\hline
			Process Isolation & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes \\
			\hline
			\hline
			Kernel Bypass & & & & & \yes & \yes & \yes & \yes & \yes \\
			\hline
			Cooperative Multitasking & & & & & & & & & \yes \\
			\hline
			Zero Copy & & \yes & & \yes & \yes & \yes & & & \yes \\
			\hline
			\hline
			网卡-bypass IPC & \yes & & \yes & \yes & & \yes & \yes & & \yes \\
			\hline
			RDMA as Transport & & & & & & \yes & & & \yes \\
			\hline
			\hline
			Scalable Socket Creation & \yes & \yes & \yes & \yes & \yes & \yes & \yes & & \yes \\
			\hline
			Lock-free Multi-thread & & & & & & & & & \yes \\
			\hline
			Scale to Many Sockets & \yes & \yes & \yes & \yes & & & \yes & & \yes \\
			\hline
		\end{tabular}
	}
	\caption{Comparison of high performance socket systems.}
	\label{socksdirect:tab:old-related-work}
	
\end{table*}
\fi
