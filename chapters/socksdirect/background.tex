\section{背景}
\label{socksdirect:sec:background}

\subsection{Linux 套接字中的开销}
\label{socksdirect:subsec:motivation}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Fraction of CPU time in the kernel (socket connection setup, socket data transmission and non-socket system calls) and user-mode applications.}
%	\label{socksdirect:fig:socket-kernel-time}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Performance of back-end systems using inter-server socket, intra-server socket, inter-server RDMA and intra-server shared memory.}
%	\label{socksdirect:fig:backend-performance}
%\end{figure}


%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Latency comparison of mutual exclusion mechanisms (CAS, mutex, futex) and cache migration.}
%	\label{socksdirect:fig:mutual-exclusion}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/fixme}
%	\caption{Throughput (bar) and latency (line) for 16B messages using inter-server TCP socket, inter-server RDMA, intra-server TCP socket, UNIX socket, pipe and shared memory queue.}
%	\label{socksdirect:fig:socket-comparison}
%\end{figure}

\iffalse
\begin{table*}[htbp]
	\centering
	\begin{tabular}{ll|ll|ll|ll}
		\hline
		\multicolumn{2}{c|}{Initialization} &
		\multicolumn{2}{c|}{Connection Mgmt} &
		\multicolumn{2}{c|}{Data Transmission} &
		\multicolumn{2}{c}{Process Mgmt} \\
		\hline
		API & Cat. &
		API & Cat. &
		API & Cat. &
		API & Cat. \\
		\hline
		\hline
		\textbf{socket} & Local &
		\textbf{connect} & NoPart &
		\textbf{send(to,(m)msg)} & P2P &
		\textit{(v)fork} & NoPart \\
		\hline
		bind & NoPart &
		\textbf{accept(4)} & P2P &
		\textbf{recv(from,(m)msg)} & P2P &
		\textit{pthread\_create} & NoPart \\
		\hline
		listen & NoPart &
		\textbf{\textit{fcntl, ioctl}} & Local &
		\textbf{\textit{write(v)}} & P2P &
		\textit{clone} & NoPart \\
		\hline
		socketpair & Local &
		\textbf{(get,set)sockopt} & Local &
		\textbf{\textit{read(v)}} & P2P &
		\textit{execve} & NoPart \\
		\hline
		getsockname  & Local &
		\textbf{\textit{close}, shutdown} & P2P &
		\textbf{\textit{memcpy}} & Local &
		\textit{exit} & P2P \\
		\hline
		\textbf{\textit{malloc}} & Local &
		getpeername & Local &
		\textit{(p)select} & P2P &
		\textit{sleep} & P2P \\
		\hline
		\textbf{\textit{realloc}} & Local &
		\textit{dup(2)} & P2P &
		\textit{(p)poll} & P2P &
		\textit{daemon} & P2P \\
		\hline
		\textit{epoll\_create} & Local &
		\textbf{\textit{epoll\_ctl}} & Local &
		\textbf{\textit{epoll\_(p)wait}} & P2P &
		\textit{sigaction} & Local \\
		\hline
	\end{tabular}
	\caption{Linux APIs that are related to socket and intercepted by \libipc{}. Categories include local, peer-to-peer (P2P) and non-partitionable (NoPart). APIs in \textit{italic} indicate usages besides socket. APIs in \textbf{bold} are called relatively more frequently.}
	
	\label{socksdirect:tab:socket-api}
\end{table*}
\fi

\iffalse
\begin{figure}
\begin{lstlisting}[style=CStyle]
int lfd = socket(...); // listen file descriptor (fd)
bind(lfd, listen_addr_and_port, ...);
listen(lfd, BACKLOG);
fcntl(lfd, F_SETFL, fcntl(lfd,F_GETFL,0) | O_NONBLOCK);
int efd = epoll_create(MAXEVENTS); // event fd
epoll_ctl(efd, EPOLL_CTL_ADD, lfd, ...);
while (true) { // main event loop
	int n = epoll_wait(efd, events, MAXEVENTS, 0);
	for (int i=0; i<n; i++) { // iterate events
		if (events[i].data.fd == lfd) { // new connection
			int cfd = accept(sfd, ...); // connection fd
			epoll_ctl(efd, EPOLL_CTL_ADD, cfd, ...);
			fcntl(cfd,F_SETFL,fcntl(cfd,F_GETFL,0)|O_NONBLOCK);
		}
		else if (events[i].events & EPOLLIN){//ready to recv
			do { // fetch all received data
				cnt = recv(events[i].data.fd, recvbuf, buflen);
				recvbuf = next_recv_buf();
			} while (cnt > 0);
			// do processing
		}
		else if (events[i].events & EPOLLOUT){//ready to send
			do { // flush send buf
				cnt = send(events[i].data.fd, sendbuf, sendlen);
				sendbuf += cnt; sendlen -= cnt;
			} while (cnt > 0 && sendlen > 0);
		}
	}
}
\end{lstlisting}

\caption{Pseudo-code of a typical socket server application, showing most important socket operations. A socket connection is a FIFO byte-stream channel identified by an integer \emph{file descriptor} (FD). Linux enables a readiness-driven I/O multiplexing model, in which the OS tells application which FDs are ready to receive or send, then the application may prepare buffers and issue socket operations.}
\label{socksdirect:fig:socket-pseudo-code}

\end{figure}
\fi


Socket是应用程序，容器和主机之间的标准通信原语。
现代数据中心网络具有微秒级基本延迟和数十Gbps吞吐量。 但是，传统的Linux套接字是在具有共享数据结构的操作系统内核空间中实现的，这使得套接字成为在多个主机上运行的通信密集型应用程序的众所周知的瓶颈 \cite {barroso2017attack}。 除了主机间通信之外，同一主机上的云应用程序和容器经常相互通信，使得主机内插槽通信在云时代变得越来越重要。 在压力测试下，Nginx~ \cite {reese2008nginx}，Memcached~ \cite {fitzpatrick2004distributed}和Redis~ \cite {carlson2013redis}等应用程序在内核中消耗50％到90％的CPU时间，主要处理TCP套接字操作 \cite{jeong2014mtcp}。

要了解socket的成本，我们首先需要了解socket的功能。
现代操作系统中的TCP套接字通常具有三个功能:（1）寻址，找到并连接到另一个应用程序；（2）提供可靠且有序的通信通道，由整数\emph {文件描述符}（FD）标识；（3）来自多个频道的多重事件，例如poll和epoll。 大多数Linux应用程序使用准备驱动的I / O多路复用模型。 操作系统告诉应用程序哪些FD已准备好接收或发送，然后应用程序可以准备缓冲区并发出接收或发送操作。

从概念上讲，Linux网络堆栈由三层组成。首先，VFS层为应用程序提供套接字API（例如\emph {connect}，\emph {send}，\emph {epoll}）。套接字连接是双向，可靠且有序的管道，由整数\emph {文件描述符}（FD）标识。
其次，传统的TCP / IP传输层提供I / O复用，拥塞控制，丢失恢复，路由和QoS功能。
第三，NIC层与NIC硬件（或用于主机内套接字的虚拟环回接口）通信以发送和接收分组。
众所周知，VFS层贡献了很大一部分成本 \cite {clark1989analysis,boyd2010analysis}。
这可以通过一个简单的实验来验证:主机中两个进程之间的Linux TCP套接字的延迟和吞吐量只比管道，FIFO和Unix域套接字差一点（表 \ref {socksdirect:tab:operation-performance} ）。
管道，FIFO和UNIX域套接字绕过传输层和NIC层，但它们的性能仍然不尽如人意。

在下文中，我们将套接字开销分为四种类型:每个操作，每个数据包，每个字节和每个连接。

%Clark et. al~\cite{clark1989analysis} categorized socket overhead into per-packet and per-byte costs.
%Because connection establishment incurs significant cost~\cite{jeong2014mtcp,lin2016scalable} and some operations do not touch payload (e.g. \texttt{dup2}, \texttt{fnctl} and \texttt{epoll}), we include two additional types of costs: per connection cost and per operation cost, as Table~\ref{socksdirect:tab:overhead} shows.

\begin{table}[t]
	\centering
		\begin{tabularx}{1\textwidth}{l|X|X}
			\hline
			Type & Overhead & Our Solution \\
			\hline
			\hline
			Per op & Kernel crossing (syscall) & User space library (\S\ref{socksdirect:sec:architecture}) \\
			\hline
			Per op & Socket FD locks for concurrent threads \& processes & Lock-free socket sharing (\S\ref{socksdirect:subsec:fork}) \\
			\hline
			\hline
			Per packet & Transport protocol (TCP/IP) & Use RDMA / SHM (\S\ref{socksdirect:subsec:lockless-queue}) \\
			\hline
			Per packet & Buffer management & New ring buffer (\S\ref{socksdirect:subsec:lockless-queue}) \\
			\hline
			Per packet & I/O multiplexing & Use RDMA / SHM (\S\ref{socksdirect:subsec:lockless-queue}) \\
			\hline
			Per packet & Interrupt handling & Event notification (\S\ref{socksdirect:subsec:process-mux}) \\
			\hline
			Per packet & Process wakeup & Event notification (\S\ref{socksdirect:subsec:process-mux}) \\
			\hline
			\hline
			Per byte & Payload copy & Page remapping (\S\ref{socksdirect:subsec:zerocopy}) \\
			\hline
			\hline
			Per conn. & Kernel FD allocation & FD remapping table (\S\ref{socksdirect:subsec:connection-management}) \\
			\hline
			Per conn. & Locks in TCB management & Distribute to \libipc{} (\S\ref{socksdirect:subsec:connection-management}) \\
			\hline
			Per conn. & New connection dispatch & Monitor daemon (\S\ref{socksdirect:subsec:connection-management}) \\
			\hline
		\end{tabularx}
	\caption{Linux 套接字的开销。}
	\label{socksdirect:tab:overhead}
	
\end{table}

\subsubsection{每次操作的开销}
\label{socksdirect:subsec:per-operation-overhead}
\quad

\parab {内核穿越。}
传统上，套接字API在内核中实现，因此需要针对每个套接字操作进行内核交叉（系统调用）。更糟糕的是，内核页表隔离（KPTI）补丁 \cite {kpti}以防止Meltdown \cite {Lipp2018meltdown}攻击使内核交叉4倍昂贵，如表 \ref {socksdirect:tab:operation-performance}显示。
我们的目标是绕过内核而不会影响安全性。

\parab {套接字文件描述符锁。}
许多应用程序都是多线程的，原因有两个。
首先，与FreeBSD不同，用于在Linux中读写磁盘文件的异步接口无法利用操作系统缓存和缓冲区，因此应用程序继续使用具有多个线程的同步接口 \cite {nginx-multi-thread}。
其次，许多Web应用程序框架更喜欢同步编程模型来处理每个用户请求，因为它更容易编写和调试 \cite {barroso2017attack}。
进程中的多个线程共享套接字连接。此外，在进程分叉之后，父进程和子进程共享现有套接字。套接字也可以通过Unix域套接字传递给另一个进程。为了保护并发操作，Linux内核为每个套接字操作获取每个套接字锁定 \cite {boyd2010analysis,han2012megapipe,lin2016scalable}。表 \ref {socksdirect:tab:operation-performance}表明，受原子操作保护的共享内存队列具有4x延迟和无锁队列的22％吞吐量，即使没有争用。
我们的目标是通过优化常见情况并删除常用套接字操作中的同步来最小化同步开销。

%\parab{Intra-host communication.}
%Most existing approaches for intra-host socket either use kernel network stack or NIC loopback.
%The kernel network stack has evolved to become quite complicated over the years~\cite{yasukata2016stackmap}, which is an overkill for intra-host communication. % may not need many TCP features, e.g., congestion control and loss recovery.

%Arrakis uses the NIC to forward packets from one application to another.
%As shown in Table~\ref{socksdirect:tab:operation-performance}, the hairpin latency from CPU to NIC is still 25x higher than inter-core cache migration delay ($\sim$30 ns). The throughput is also limited by Memory-Mapped I/O (MMIO) doorbell latency and PCIe bandwidth~\cite{neugebauer2018understanding,li2017kv}.

%We aim to leverage user-space shared memory for intra-host socket communication.


%%For inter-host communication inside data centers, there are two challenges for using RDMA.
%%The first is to transparently determine whether or not the remote host supports \sys{}.
%%We cannot use an out-of-band communication channel such as RDMA CM because it cannot pass through middleboxes in cloud network.
%%The second is to bridge the semantics of socket and RDMA~\cite{dragojevic2014farm}.
%The main challenge for leverage RDMA for inter-host socket communication is to bridge the semantics of socket and RDMA~\cite{dragojevic2014farm}.
%For example, RDMA preserves messages boundaries while TCP does not.
%For I/O multiplexing, RDMA provides a completion notification model while event polling in Linux socket requires a readiness model~\cite{han2012megapipe}.
%Further, one-sided and two-sided RDMA verbs have different efficiency and overheads~\cite{kalia2014using,kaminsky2016design}.

%We aim to use RDMA efficiently for inter-host socket communication, while falling back to TCP transparently in case of non-RDMA peers.

%\parab{Many concurrent connections.}
%Internet facing applications often need to serve millions of concurrent connections efficiently~\cite{jeong2014mtcp,lin2016scalable,belay2017ix}.
%%In addition, because a socket connection provides a FIFO abstraction and the OS offers event multiplexing, 
%Moreover, it is also common for two backend applications to create large number of connections between them, where each connection handles a concurrent task~\cite{ihm2011towards,jang2011sslshader,nishtala2013scaling}. In Linux, a socket connection has dedicated send and receive buffers, each is at least one page (4~KB) in size~\cite{davidskbs}. With millions of concurrent connections, the socket buffers can consume gigabytes of memory, most of which is empty. Random accesses to a large number of buffers also cause CPU cache misses and TLB misses. Similar issue exists in RDMA NICs with limited on-chip memory for caching connection states~\cite{mprdma,kaminsky2016design}.
%%, which is slower than sequential access to a single buffer~\cite{li2017kv}. 
%%This problem is exaggerated in RDMA. RDMA NIC caches connection states in limited on-board memory. Just over a few hundred active RDMA connections could cause cache misses and degrade performance~\cite{mprdma,kaminsky2016design}. 
%%Moreover, event-driven applications use \texttt{epoll} to detect which connections are ready for send or receive. If the event queue is separated from data queues, each \texttt{recv} operation involves two cache migrations for event and data~\cite{yasukata2016stackmap}. 
%
%We aim to minimize memory cache misses per data transmission by multiplexing socket connections.


\begin{table}[t]
	\centering
	\scalebox{0.8}{
		\begin{tabular}{l|l|l|}
			\hline
			Operation	& Latency  & Throughput  \\
			& ($\mu$s) & (M~op/s) \\
			\hline
			\hline
			Inter-core cache migration	& 0.03 & 50 \\
			\hline
			Poll 32 empty queues & 0.04 & 24 \\
			\hline
			System call (before KPTI) & 0.05 & 21 \\
			\hline
			Spinlock (no contention) & 0.10 & 10 \\
			\hline
			Allocate and deallocate a buffer & 0.13 & 7.7 \\
			\hline
			Spinlock (contended) & 0.20 & 5 \\
			\hline
			Lockless shared memory queue & 0.25 & 27 \\
			\hline
			\textbf{Intra-host \sys} & 0.30 & 22 \\
			\hline
			System call (after KPTI) & 0.20 & 5.0 \\
			\hline
			Copy one page (4~KiB) & 0.40 & 5.0 \\
			\hline
			Cooperative context switch & 0.52 & 2.0 \\
			\hline
			Map one page (4~KiB) & 0.78 & 1.3 \\
			\hline
			NIC hairpin within a host & 0.95 & 1.0 \\
			\hline
			Atomic shared memory queue & 1.0 & 6.1 \\
			\hline
			Map 32 pages (128~KiB) & 1.2 & 0.8 \\
			\hline
			Open a socket FD & 1.6 & 0.6 \\
			\hline
			One-sided RDMA write & 1.6 & 13 \\
			\hline
			Two-sided RDMA send / recv & 1.6 & 8 \\
			\hline
			\textbf{Inter-host \sys} & 1.7 & 8 \\
			\hline
			Process wakeup & 2.8$\sim$5.5 & 0.2$\sim$0.4 \\
			\hline
			Linux pipe / FIFO & 8 & 1.2 \\
			\hline
			Unix domain socket in Linux & 9 & 0.9 \\
			\hline
			Intra-host Linux TCP socket & 11 & 0.9 \\
			\hline
			Copy 32 pages (128~KiB) & 13 & 0.08 \\
			\hline
			Inter-host Linux TCP socket & 30 & 0.3 \\
			\hline
		\end{tabular}
	}
	\caption{往返延迟和单核吞吐量操作（测试平台设置在\S \ref {socksdirect:subsec:methodology}中）。 如果未指定，则消息大小为8个字节。}
	\label{socksdirect:tab:operation-performance}
	
\end{table}


\subsubsection{每个数据包的开销}
\label{socksdirect:subsec:per-packet-overhead}
\quad

\parab {传输协议（TCP / IP）。}
传统上，TCP / IP是数据中心中的事实上的传输协议。
TCP / IP协议处理，拥塞控制和丢失恢复在每个发送和接收的数据包上消耗CPU。
此外，丢失检测，基于速率的拥塞控制和TCP状态机使用定时器，这很难实现微秒级粒度和低开销 \cite{jeong2014mtcp}。
幸运的是，近年来在许多数据中心见证了RDMA的大规模部署 \cite {guo2016rdma,zhu2015congestion,mittal2015timely}。
RDMA提供与TCP / IP相当的基于硬件的传输，因此需要将传输协议卸载到RDMA NIC。
对于主机内套接字，我们的目标是完全绕过传输协议。 Linux \emph {pipe}的％。

\parab {缓冲管理。}
传统上，CPU通过\emph {ring buffers}从NIC发送和接收数据包。
环形缓冲区由固定数量的固定长度的元数据条目组成。
每个条目都指向一个存储数据包有效负载的缓冲区。
要发送或接收数据包，需要分配和释放缓冲区。
成本显示在表 \ref {socksdirect:tab:operation-performance}中。
此外，为了确保可以接收MTU大小的分组，每个接收缓冲区应该具有至少一个MTU的大小。
但是，许多数据包小于MTU~ \cite {thompson1997wide}，因此内部碎片会降低内存利用率和位置。
虽然现代NIC支持LSO和LRO~ \cite {lsolro}在缓冲区中批量处理多个数据包，但我们的目标是完全消除开销。

\parab {I / O多路复用。}
对于传统的NIC，接收到的不同连接的数据包通常在环形缓冲区中混合，因此网络堆栈需要将数据包分类到相应的套接字缓冲区中。
现代NIC支持接收数据包转向 \cite {mellanox}，它可以将特定连接映射到专用环形缓冲区，该缓冲区由高性能套接字系统使用 \cite {jeong2014mtcp,lin2016scalable,libvma}。
我们利用RDMA网卡中的类似功能，将接收到的数据包解复用到QP。

\parab {中断处理。}
Linux网络堆栈分为系统调用和中断上下文，因为它处理来自应用程序和硬件设备的事件。
TCP拥塞控制中的ACK时钟 \cite {mprdma}要求及时响应收到的数据包。
中断上下文不一定与应用程序在同一核心上，导致核心位置不佳和核心间同步。
但是，对于RDMA NIC，需要精确计时的数据包处理由NIC硬件处理。
因此，\libipc {}运行每个套接字操作完成并使用轮询等待阻塞操作。

\begin{sidewaystable*}[t]
	\centering
	\scalebox{0.68}{
		\begin{tabularx}{1.45\textwidth}{l|X|X|X|X|X|X|X|X|X|X|}
			\hline
			& FastSocket & MegaPipe / StackMap & IX & Arrakis & SandStorm / mTCP & LibVMA & OpenOnload & Rsocket / SDP & FreeFlow & SocksDirect \\
			\hline
			\hline
			Category & \multicolumn{2}{c|}{Kernel optimization} & \multicolumn{5}{c|}{User-space TCP/IP stack} & \multicolumn{3}{c|}{Offloading to RDMA NICs} \\
			%\hline
			%Changes needed for deployment & New kernel & New kernel & New kernel & New kernel & Lib+driver & Lib+driver & Lib+driver & Lib+driver & Lib+driver +daemon & Lib+driver +daemon \\
			\hline
			\hline
			\multicolumn{11}{c|}{Compatibility} \\
			\hline
			\hline
			Transparent to existing applications & \yes &   & \yes & \yes &   & \yes & \yes & \yes & \yes & \yes \\
			\hline
			\texttt{epoll} (Nginx, Memcached etc.) & \yes & \yes & \yes & \yes & \yes & \yes & \yes & & & \yes \\
			\hline
			Compatible with regular TCP peers &	\yes & \yes & \yes & \yes & \yes & \yes & \yes & & & \yes \\
			\hline
			Intra-host communication & \yes & \yes & & \yes & & & & & \yes & \yes \\
			\hline
			Multiple applications listen a port & \yes & \yes & & & & & & \yes & \yes & \yes \\
			\hline
			Full fork support & \yes & \yes & & & & & \yes & & & \yes \\
			\hline
			Container live migration & \yes & \yes & & & & & & & & \yes \\
			\hline
			\hline
			\multicolumn{11}{c|}{Isolation} \\
			\hline
			\hline
			Access control policy & Kernel & & Kernel & Kernel & & & & Kernel & Daemon & Daemon \\
			\hline
			Isolation among containers (VMs) & \yes & & \yes & \yes & & & & \yes & \yes & \yes \\
			\hline
			QoS (performance isolation) & Kernel & Kernel & Kernel & NIC & NIC & NIC & NIC & NIC & Daemon & NIC \\
			\hline
			\hline
			\multicolumn{11}{c|}{Removed Performance Overheads} \\
			\hline
			\hline
			Kernel crossing on data plane & & & Batched & \yes & Batched & \yes & \yes & \yes & \yes & $<$16KB msg \\
			\hline
			Socket FD locks & & & & & & & & & & \yes \\
			\hline
			Transport protocol & & & & & & & & \yes & \yes & \yes \\
			\hline
			Buffer management & & & & & & & & & & \yes \\
			\hline
			I/O multiplexing \& Interrupt handling & & Improved & \yes & \yes & Improved & \yes & \yes & \yes & \yes & \yes \\
			\hline
			Process wakeup & & & & & & & & & & \yes \\
			\hline
			Payload copy & & \yes & & & \yes & & & & & $\ge$16KB msg \\
			\hline
			Kernel FD allocation & & \yes &  &  & \yes & & & & & \yes \\
			\hline
			TCB mgmt \& Connection dispatch & \yes & \yes & \yes & \yes & \yes & & & & & \yes \\
			\hline
		\end{tabularx}
	}
	\caption{高性能套接字系统的比较。}
	\label{socksdirect:tab:related-work}
	
\end{sidewaystable*}




\parab {进程唤醒。}
当进程调用RPC并等待回复时，是否应将CPU放弃到准备运行的其他进程？
Linux的答案是肯定的，唤醒睡眠过程需要3到5 $\mu$s，如表 \ref {socksdirect:tab:operation-performance}所示。
在主机内RPC的往返时间内，两个进程唤醒贡献了超过一半的延迟。
％信号量，互斥量或互斥量的唤醒成本也在3到5美元/亩的范围内。
注意到通过RDMA的RPC延迟甚至低于唤醒成本，许多系统使用轮询，每个核仅允许一个进程。
为了隐藏微秒级RPC延迟 \cite {barroso2017attack}，我们观察到通过\texttt {sched \_yield}的协作上下文切换比进程唤醒要快得多。
我们的目标是有效地在流程之间共享核心。

\parab {容器网络。}
许多容器部署使用隔离的网络命名空间用于容器，这些容器通过虚拟覆盖网络进行通信。
在Linux中，虚拟交换机 \cite {pfaff2015design}在主机NIC和容器中的虚拟NIC之间转发数据包。
这种架构会在每个数据包上产生多个上下文切换和内存副本的开销，虚拟交换机成为瓶颈 \cite {pfefferle2015hybrid}。
Slim~ \cite {slim}将三次内核往返减少到一次。
几个工作 \cite {martins2014clickos,roghanchi2017ffwd,huang2017high,nsdi19freeflow}将所有操作委托给作为守护进程运行的虚拟交换机，因此它增加了数据路径上的延迟和CPU成本。
我们的解决方案是用于隔离和分布式数据平面以实现性能的集中控制平面。


\subsubsection{每字节的开销}
\label{socksdirect:subsec:per-byte-overhead}
\quad

\parab {内容（payload）复制。}
在大多数套接字系统中，\texttt {send}和\texttt {recv}的语义会导致应用程序和网络堆栈之间的内存复制。 对于非阻塞\texttt {send}，系统需要将数据复制出缓冲区，因为应用程序可能会在\texttt {send}返回后立即覆盖缓冲区。 简单地删除副本可能会违反应用程序的正确性。
零拷贝\texttt {recv}甚至比\texttt {send}更难。
Linux提供了基于准备就绪的事件模型，即应用程序知道传入的数据（例如通过\texttt {epoll}）然后调用\texttt {recv}，因此NIC接收但未传递给应用程序的数据必须 存储在系统缓冲区中。
因为\texttt {recv}允许应用程序提供任何缓冲区作为数据目标，所以系统需要将数据从系统复制到应用程序缓冲区。
我们的目标是在标准套接字应用程序中为大型传输提供零拷贝。



\subsubsection{每条连接的开销}
\label{socksdirect:subsec:per-connection-overhead}
\quad

\parab {内核FD分配。}
在Linux中，每个套接字连接都是VFS中的文件，因此需要分配整数FD和\emph {inode}。
用户空间套接字的挑战是有许多API（例如open，close和epoll）支持套接字和非套接字FD（例如文件和设备），因此我们必须将套接字FD与其他FD区分开来。
用户空间中的Linux兼容套接字 \cite {libvma,rsockets}通常在内核中打开一个文件以获取每个套接字的虚拟FD，因此它们仍然需要内核FD分配。
LOS~ \cite {huang2017high}将FD空间划分为用户和内核部分，但违反了Linux分配最小可用FD的属性。
但是，许多应用程序，如Redis~ \cite {redis}和Memcached~ \cite {memcached}都依赖于此属性。
我们的目标是在保持兼容性的同时绕过内核插槽FD分配。

\parab {TCB管理中的锁定。}
在建立连接期间，Linux获取几个全局锁来分配TCB（TCP控制块）。
最近的工作，如MegaPipe~ \cite {han2012megapipe}和FastSocket~ \cite {lin2016scalable}通过对全局表进行分区来减少锁争用，但正如Table~ \ref {socksdirect:tab:operation-performance}所示，非争用自旋锁是仍然很贵。
我们将工作分发到每个进程中的用户空间库\libipc {}。

\parab {新连接调度。}
多个进程和线程可以侦听同一端口以接受传入连接。
在Linux中，处理\texttt {accept}调用的核心在传入连接的队列上进行竞争。
我们利用委托比锁定 \cite {roghanchi2017ffwd}更快并使用监视器守护程序来分派新连接的事实。


%Most modern applications use socket in an event-driven style. similar to Figure~\ref{socksdirect:fig:socket-pseudo-code}.

%Socket is a well-known bottleneck for communication intensive applications. We stress test Nginx load balancer, memcached and Redis with one request per TCP connection or a stream of requests on pre-established TCP connections, and find that 50\%$\sim$90\% CPU time is consumed in the kernel, mostly dealing with socket system calls. The one request per connection scenario shows much lower throughput, because of overhead in connection creation.

%If we were able to mitigate the overhead associated with sockets, application performance would be 2x$\sim$10x.
%As another example, we replaced socket with RDMA and enabled zero copy gRPC in distributed Tensorflow~\cite{abadi2016tensorflow}, and training a VGG network shows 5x speedup.

%Socket is a well-known bottleneck in fast data center networks \RED{(cite)}. In a data center, we categorize systems that process queries from Internet users to be \textit{front-end} (\textit{e.g.} DNS server, load balancer and Web service), and the other systems (\textit{e.g.} database, stream processing and machine learning) to be \textit{back-end}.

%For front-end systems, as shown in Figure~\ref{socksdirect:fig:socket-kernel-time}, 70\% -- 90\% CPU time is used in socket system calls. They maintain a large number of socket connections to Internet users. Establishing a socket connection takes $\approx$40$\mu$s CPU time, so a CPU core can only serve 25K new connections per second~\cite{lin2016scalable}.

%For back-end systems, the performance using socket is significantly worse than using RDMA or shared memory. This is because back-end systems interact frequently with other nodes and services, consequently the latency is vital for their performance. As shown in Figure~\ref{socksdirect:fig:socket-comparison}, socket latency between different processes in a same server is $\approx$10~$\mu$s, even much higher than inter-server RDMA latency ($\approx$2~$\mu$s).

%\textbf{Event-driven programming.}

%\textbf{Socket process: connection setup and data transmission.}

\subsection{高性能套接字系统}
\label{socksdirect:subsec:related-work}

学术界和业界都提出了许多高性能套接字系统，如表 \ref {socksdirect:tab:related-work}所示。

\parab {内核网络堆栈优化:}第一类工作是优化内核TCP / IP堆栈。 FastSocket~ \cite {lin2016scalable}，Affinity-Accept~ \cite {pesterev2012improving}，FlexSC~ \cite {soares2010flexsc}和零拷贝套接字 \cite {thadani1995efficient,chu1996zero,linux-zero-copy}实现良好的兼容性和隔离性。

MegaPipe~ \cite {han2012megapipe}和StackMap~ \cite {yasukata2016stackmap}提出了新的API来实现零拷贝和改进I / O多路复用，代价是需要修改应用程序。
但是，大量的内核开销仍然存在。
支持零拷贝的挑战是套接字语义。

\parab {用户空间TCP / IP堆栈:}第二类工作完全绕过内核TCP / IP堆栈并在用户空间中实现TCP / IP。
在这个类别中，IX~ \cite {belay2017ix}和Arrakis~\cite {peter2016arrakis}是新的操作系统架构，它使用虚拟化来确保安全性和隔离性。 IX利用LwIP~ \cite {dunkels2001design}在用户空间中实现TCP / IP，同时使用内核转发每个数据包以实现性能隔离和QoS。相比之下，Arrakis将QoS卸载到NIC，因此绕过数据平面的内核。
他们使用NIC在同一主机中的应用程序之间转发数据包。
如表 \ref {socksdirect:tab:operation-performance}所示，从CPU到NIC的发夹延迟远远高于核心间缓存迁移延迟。
吞吐量也受到内存映射I / O（MMIO）门铃延迟和PCIe带宽的限制 \cite {neugebauer2018understanding,li2017kv}。

除了这些新的OS体系结构外，许多用户空间套接字在Linux上使用高性能数据包I / O框架，例如Netmap~ \cite {rizzo2012netmap}，Intel DPDK~ \cite {dpdk}和PF\_RING \cite {pf-ring}，以便直接访问用户空间中的NIC队列。
SandStorm~ \cite {marinos2014network}，mTCP~ \cite {jeong2014mtcp}，Seastar~ \cite {seastar}和F-Stack~ \cite {fstack}提出了新的API，因此需要修改应用程序。
大多数API更改旨在支持零拷贝，标准API仍然会复制数据。
FaSST~ \cite {kalia2016fasst}和eRPC~ \cite {kalia2018datacenter}提供RPC API而不是套接字。
LibVMA~ \cite {libvma}，OpenOnload~ \cite {openonload}，DBL~ \cite {dbl}和LOS~ \cite {huang2017high}符合标准套接字API。

用户空间TCP / IP堆栈提供了比Linux更好的性能，但它仍然不接近RDMA和SHM。
一个重要原因是现有的工作都不支持在线程和进程之间共享套接字，导致fork和容器实时迁移中的兼容性问题，以及多线程锁定开销。

首先，当进程分叉时，只有父进程和子进程中的一个可以使用现有套接字。
但是，许多Web服务 \cite {apache,nginx,php-fpm,python-gunicorn,vsftpd}和键值存储 \cite {memcached}都有一个主进程来接受连接并（可选）读取请求头，然后它可以分叉一个子进程来处理请求，其中子进程需要访问套接字。
同时，父进程仍需要通过现有套接字接受新连接。
这使得此类Web服务无法正常工作。
更棘手的情况是父进程和子进程可以通过现有套接字同时写入日志服务器。

其次，容器实时迁移与fork有关。
它类似于分叉容器（到新主机），子容器应该仍然能够访问套接字。

第三，多线程在应用程序中很常见。
应用程序在套接字操作中承担竞争条件的风险，或者每次操作必须采用套接字FD锁定。
后一种方法保证了正确性，但即使没有争用，锁定也会损害性能。

\parab {卸载到NIC的传输:}
第三类工作将套接字系统的一部分卸载到NIC硬件上。
TCP卸载引擎（TOE） \cite {tcp-chimney-offload}将部分或完整的TCP / IP堆栈卸载到NIC，但它们仅在专用区域中起飞（例如iSCSI HBA~ \cite {iscsi-hba}）和无状态卸载（例如校验和，流动转向和LSO / LRO~ \cite {lsolro}）。
由于数据中心的硬件趋势和应用需求，\emph {stateful}卸载的故事近年来有所不同 \cite {chuanxiong-rdma-keynote}。
因此，RDMA~ \cite {infiniband2000infiniband}在生产数据中心中广泛使用 \cite {guo2016rdma}。
与基于软件的TCP / IP网络堆栈相比，RDMA使用硬件卸载来提供超低延迟和接近零的CPU开销。
要使套接字应用程序能够使用RDMA，RSocket~ \cite {rsockets}，SDP~ \cite {socketsdirect}和UNH EXS~ \cite {russell2008extended}将套接字操作转换为RDMA谓词。
它们具有相似的设计，而RSocket是最活跃的设计。
FreeFlow~ \cite {nsdi19freeflow}虚拟化容器覆盖网络的RDMA NIC，它利用SHM进行主机内部和RDMA进行主机间通信。
FreeFlow使用RSocket将套接字转换为RDMA。

但是，由于RDMA和套接字的抽象不匹配，这些工作有局限性。
在兼容性方面，首先，他们缺乏对几个重要API的支持，例如\textit {epoll}，因此它与许多应用程序不兼容，包括Nginx，Memcached，Redis等。
这是因为RDMA仅提供传输功能，而\textit {epoll}是与OS事件通知集成的文件抽象。
其次，RDMA QP不支持\textit {fork}和容器实时迁移 \cite{nsdi19freeflow}，因此RSocket也有同样的问题。
第三，由于RSocket使用RDMA作为有线协议，因此无法连接到常规TCP / IP对等体。
这是一个部署挑战，因为分布式系统中的所有主机和应用程序必须同时切换到RSocket。
我们的目标是透明地检测远程端是否支持Rsocket，如果没有则回退到TCP / IP。
在性能方面，他们无法删除有效负载副本，套接字FD锁，缓冲区管理，进程唤醒和每个连接开销。
例如，RSocket在发送方和接收方分配缓冲区并复制有效负载。
与Arrakis类似，RSocket使用NIC进行主机内通信，从而导致性能瓶颈。

%In addition, they use two-sided RDMA \texttt{send} and \texttt{recv}, but one-sided RDMA \texttt{write} has higher throughput due to lower CPU cost on receive side (Table~\ref{socksdirect:tab:operation-performance}).
%However, such simple translations suffer from throughput degradation with a large number of concurrent connections. This is because RDMA NIC keeps per-connection states using a $\approx$2~MB~\cite{kalia2018datacenter} on-NIC memory as cache. With hundreds of concurrent connections, we will suffer from frequent cache misses, resulting in serious throughput degradation~\cite{mprdma,kaminsky2016design}. 


%Spinlock in kernel is implemented with Compare-And-Swap (CAS) instruction, which costs $100\sim200$~ns per acquire and release. In comparison, shared memory message passing only needs one cache migration~\cite{roghanchi2017ffwd} (30~ns).
%In comparison, shared memory message passing
%Scalability analysis of Linux system calls~\cite{boyd2010analysis,clements2015scalable} suggests that many socket operations are not commutable and thus impossible to scale for all cases. 
%For example, socket provides \texttt{SO\_REUSEPORT} option. When this option is enabled, multiple processes on one host can \texttt{bind} to the same port and incoming connections are dispatched to the listeners. During connection setup, coordination is required to allocate file descriptors and port numbers, load balance connections and allocate buffers. In addition, a socket may be shared by multiple processes and threads in a same process share socket connections. When a process forks, the parent and child processes also share the sockets created before fork.


%For inter-server data transmission, Linux kernel copies data four times: on the send side, from application to kernel socket buffer, then to NIC ring buffer; the receive side is similar. For intra-server, data is copied three times: from application to kernel send buffer, then to kernel receive buffer via loopback interface, and finally to another application. Each CPU core can copy $\approx$5~GiB data per second~\cite{panda2016netbricks}, that's why a kernel TCP connection is hard to saturate an 100~Gbps NIC.
%Historically, \texttt{send} was designed as a blocking operation, and the send buffer may be overwritten by the application after \texttt{send} function call.
%In a non-blocking \texttt{send}, the process could overwrite the send buffer after \texttt{send} function call returns. To avoid race condition on the data buffer, network stack needs to copy data to an internal buffer. %before \texttt{send} returns, and then send the copied data in background.
%For the \texttt{recv} operation, application provides a buffer and read the data after \texttt{recv}.
%If we implement \texttt{send} as a lazy operation, \textit{i.e.}, the data is buffered on the sender, the receiver needs to wait for a round-trip time on \texttt{recv}.
%For this reason, most socket implementations send data to the receiver eagerly.
%Because the receiver's network stack cannot predict the user-provided \texttt{recv} address, it needs to buffer received data internally, then copy to the desired destination when \texttt{recv} is called.
%Our goal in \sys is to achieve zero copy for large data transfers without making changes to applications. We accomplish this by taking advantage of the page mapping mechanism in virtual memory. 







%\textbf{Virtual file system.}
%Each socket connection is a \textit{file descriptor} (FD) in Linux. First, each new connection involves expensive \texttt{inode} and \texttt{dentry} creation in the \texttt{proc} file system, which is not scalable and rarely used except for diagnostic and monitoring utilities. Second, socket operations need to pass through the virtual file system (VFS), which introduces CPU overhead and additional latency.





%\subsection{High Performance Socket Systems}
%\label{socksdirect:sec:related}


\iffalse
\begin{table*}[t]
	\centering
	\scalebox{0.88}{
		\begin{tabular}{l|c|ccc|cc|ccc|}
			\hline
			Category	& \multicolumn{1}{c|}{Linux Opt.} & \multicolumn{3}{c|}{New Kernel Stack} & \multicolumn{2}{c|}{User-space Packet} & \multicolumn{3}{c|}{User-space Socket} \\
			\hline
			System	& FastSocket & IX & MegaPipe & StackMap & Arrakis & FreeFlow & mTCP & libvma & SocksDirect \\
			\hline
			\hline
			Socket-like API & \yes & & \yes & \yes & \yes & & \yes & \yes & \yes \\
			\hline
			Linux Compatible & \yes & & & & & & & \yes & \yes \\
			\hline
			Process Isolation & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes & \yes \\
			\hline
			\hline
			Kernel Bypass & & & & & \yes & \yes & \yes & \yes & \yes \\
			\hline
			Cooperative Multitasking & & & & & & & & & \yes \\
			\hline
			Zero Copy & & \yes & & \yes & \yes & \yes & & & \yes \\
			\hline
			\hline
			NIC-bypass IPC & \yes & & \yes & \yes & & \yes & \yes & & \yes \\
			\hline
			RDMA as Transport & & & & & & \yes & & & \yes \\
			\hline
			\hline
			Scalable Socket Creation & \yes & \yes & \yes & \yes & \yes & \yes & \yes & & \yes \\
			\hline
			Lock-free Multi-thread & & & & & & & & & \yes \\
			\hline
			Scale to Many Sockets & \yes & \yes & \yes & \yes & & & \yes & & \yes \\
			\hline
		\end{tabular}
	}
	\caption{Comparison of high performance socket systems.}
	\label{socksdirect:tab:old-related-work}
	
\end{table*}
\fi
