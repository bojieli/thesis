
\iffalse
\section{Discussion}
\label{socksdirect:sec:discussion}

\textbf{Limitations of RDMA.} First, RDMA is not suitable for WAN. Second, RDMA has scalability issue when one server connects to many servers. Software transport in CPU access connection states in host memory, while hardware RDMA transport caches connection states in 网卡 and swaps out to host memory when cache overflows. First, CPU cache miss costs less than 0.1$\mu$s, while 网卡 cache miss costs 0.5$\mu$s~\cite{kaminsky2016design}. Second, CPU memory bandwidth is an order of magnitude larger than 网卡 PCIe bandwidth. In light of this, a host should switch to software transport when it actively communicates with a large number of hosts. Fortunately, Modern 网卡s has an increasing size of memory and supports more active connections without performance degradation~\cite{kaminsky2016design}.

\textbf{Multiple sender ordering.}
Ordering of multiple senders (linux is sequentially consistent).

\textbf{Remaining compatibility problems.}
Applications that do not use GNU libc.
Linux AIO and Windows I/O completion ports.
/proc fs.
\fi
