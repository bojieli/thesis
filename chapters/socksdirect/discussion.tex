\section{Discussion}
\label{socksdirect:sec:discussion}

\textbf{Limitations of RDMA.} First, RDMA is not suitable for WAN. Second, RDMA has a scalability issue when one server connects to many servers. Software transport in CPU accesses connection states in host memory, while hardware RDMA transport caches connection states in the network card and swaps out to host memory when cache overflows. First, a CPU cache miss costs less than 0.1$\mu$s, while a network card cache miss costs 0.5$\mu$s~\cite{kaminsky2016design}. Second, the CPU memory bandwidth is an order of magnitude larger than the network card PCIe bandwidth. In light of this, a host should switch to software transport when it actively communicates with a large number of hosts. Fortunately, modern network cards have an increasing size of memory and support more active connections without performance degradation~\cite{kaminsky2016design}.

\textbf{Multiple sender ordering.}
The ordering of multiple senders (Linux is sequentially consistent).

\textbf{Remaining compatibility problems.}
Applications that do not use GNU libc.
Linux AIO and Windows I/O completion ports.
/proc fs.
