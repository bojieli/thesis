\section{系统性能评估}
\label{socksdirect:sec:evaluation}


%\begin{figure}[htpb]
%	\centering
%	\includegraphics[width=\columnwidth]{eval/microbenchmark/fork-tput.pdf}
%	\caption{Throughput of SocksDirect with fork happening}
%	\label{socksdirect:fig:eval-fork-tput}
%\end{figure}

\sys{} 在三个组件中实现：一个用户空间库 \libipc {} 和一个带有17K行C ++代码的监控守护进程，以及一个支持零拷贝的内核模块。
本节从以下方面评估\sys{}：

\parab {有效地为主机内套接字使用共享内存。}
对于8字节消息，\sys 实现0.3 $\mu$s RTT和每秒23~M消息的吞吐量。 对于大型消息，\sys 使用零拷贝来实现Linux的1/13延迟和26x吞吐量。

\parab {有效地使用RDMA进行主机间套接字。}
\sys 达到1.7 $\mu$s RTT，接近原始RDMA性能。
零拷贝时，一个连接会使100~Gbps链路饱和。

%\parab{Robust with number of connections.}
%The performance above can be maintained with up to 100 million connections.

%\parab{Corner-case operations does not affect long-term performance.}
%After corner-case operations such as \texttt{fork}, the performance recovers quickly.

\parab {可扩展核心数。}
随着核心数量的增加，吞吐量几乎可以线性扩展。


\parab {使用未经修改的端到端应用程序显着加速。}
例如，\sys  {}将Nginx HTTP请求延迟减少5.5倍到20倍。


\subsection{评估方法}
\label{socksdirect:subsec:methodology}

本节使用两个Xeon E5-2698 v3 CPU，256~GiB内存和一个Mellanox ConnectX-4网卡的服务器评估 \sys{}。服务器与Arista 7060CX-32S 交换机通过 100 Gbps 以太网接口互连 \cite {arista-7060cx}。不同于第 \ref{chapter:clicknp}、\ref{chapter:kvdirect} 章，本节仅使用可编程网卡中的商用网卡部分，且商用网卡升级到了 100 Gbps，没有使用 FPGA。服务器使用Ubuntu 16.04和Linux 4.15，将RoCEv2用于RDMA协议，每64条消息轮询一次完成队列。
每个线程都固定在CPU内核上。在收集数据之前，进行了足够的预热测试。
对于延迟，本节使用一个乒乓应用程序报告平均往返时间，误差条代表1％和99％百分位数。
对于吞吐量，一方保持发送数据而另一方不断接收数据。
本节将比较Linux，原始RDMA写原语（write verb），Rsocket~ \cite {rsockets}和LibVMA~ \cite {libvma}，这是针对Mellanox 网卡优化的用户空间TCP / IP协议栈。
我们也比较了没有批处理和零拷贝的 \sys{}，用 ``SD (unopt)'' 表示。
本节没有评估mTCP~ \cite {jeong2014mtcp}，因为底层DPDK库对Mellanox ConnectX-4网卡的支持有限。由于批处理，mTCP具有比RDMA高得多的延迟，报告的吞吐量为每秒1.7~M包~~ \cite {kalia2018datacenter}。

\subsection{性能微基准测试}
\label{socksdirect:subsec:microbenchmark}

\subsubsection{延迟和吞吐量}



图 \ref {socksdirect:fig:eval-msgsize-intra}显示了一对发送方和接收方线程之间的主机内套接字性能。
对于8字节消息，\sys 实现0.3 $ \mu $ s往返延迟（Linux的1/35）和每秒23~M消息吞吐量（Linux的20倍）。
相比之下，一个简单的共享内存队列具有0.25 $ \mu $ s往返延迟和27~M吞吐量，表明\sys 增加了很少的开销。
RSocket具有6x延迟和1/4吞吐量的\sys  {}，因为它使用网卡转发主机内数据包，这会导致PCIe延迟。
LibVMA只是将内核TCP套接字用于主机内部。
\sys  {}的单向延迟为0.15 $ \mu $ s，甚至低于内核穿越（0.2 $ \mu $ s）。基于内核的套接字需要在发送方和接收方都进行内核交叉。


由于内存复制，对于8~KiB消息，\sys 的吞吐量仅比Linux高60％，延迟低4倍。对于大小至少为16~KiB的消息，\sys 使用页面重映射来实现零拷贝。
对于1~MiB消息，\sys 比Linux具有1/13延迟和26x吞吐量。
由于事件通知延迟，RSocket的延迟不稳定，在某些情况下甚至可能比Linux大。



\begin{figure*}[htbp]
	\centering
	\subfloat[单机内吞吐量。]{                    
		%\begin{minipage}{0.4\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{eval/microbenchmark/msgsize-ipc-tput.pdf}
		\label{socksdirect:fig:eval-msgsize-ipc-tput}
		%\end{minipage}
	}
	\subfloat[单机内延迟。]{
		%\begin{minipage}{0.4\textwidth}
		\centering \includegraphics[width=0.5\textwidth]{eval/microbenchmark/msgsize-ipc-lat.pdf}
		\label{socksdirect:fig:eval-msgsize-ipc-lat}
		%\end{minipage}
	}
	
	\caption{不同消息大小下的单机内通信单核消息性能。}
	\label{socksdirect:fig:eval-msgsize-intra}
\end{figure*}


图 \ref {socksdirect:fig:eval-msgsize-inter} 显示了一对线程之间的主机间套接字性能。
对于8字节消息，\sys{} 实现每秒18M消息吞吐量（Linux的15倍）和1.7微秒的延迟（Linux的1/17）。
吞吐量和延迟接近原始RDMA写操作（如虚线所示），它没有套接字语义。
批处理不影响我们评测的延迟，由于仅当发送队列满时，RDMA 写操作才会被延迟处理，而我们评测延迟仅使用一条消息。
由于批处理，\sys  {}对于8字节消息的吞吐量甚至高于RDMA。
非批处理的 \sys{} 消息吞吐量介于 RSocket 和 RDMA 之间。
LibVMA 也使用批处理达到了较好的性能，但延迟是\sys  {}的7倍。
对于小于8~KiB的消息大小，主机间RDMA的吞吐量略低于主机内共享内存，因为环形缓冲区结构是共享的。
对于512B到8KiB消息，以及更大的没有启用零拷贝的消息，\sys  {}受数据包复制的限制，但由于缓冲管理开销减少，仍然比RSocket和LibVMA更快。
对于零拷贝消息（$\ge$ 16 KiB），\sys  {}使网络带宽饱和，其具有所有比较工作的3.5倍吞吐量和RSocket的72％延迟。


\begin{figure*}[htbp]
	\centering
	\subfloat[跨主机吞吐量。]{
		%\begin{minipage}{0.4\textwidth}
		\centering \includegraphics[width=0.5\textwidth]{eval/microbenchmark/msgsize-network-tput.pdf}
		\label{socksdirect:fig:eval-msgsize-network-tput}
		%\end{minipage}
	}
	\subfloat[跨主机延迟。]{
		%\begin{minipage}{0.4\textwidth}
		\centering \includegraphics[width=0.5\textwidth]{eval/microbenchmark/msgsize-network-lat.pdf}
		\label{socksdirect:fig:eval-msgsize-network-lat}
		%\end{minipage}
	}
	
	\caption{不同消息大小下的跨主机通信单核消息性能。}
	\label{socksdirect:fig:eval-msgsize-inter}
\end{figure*}

\subsubsection{延迟分解}

表 \ref{tab:microbenchmark} 说明了为什么 \sys{} 的性能超越了其他系统。
每次套接字操作，Linux 都需要内核穿越，除 \sys{} 以外的系统在线程安全模式下都需要加锁。
每个数据包，\sys{} 节约了缓冲区管理开销，并将传输层和数据包处理卸载到网卡。
为了传输一个数据包，\sys{} 利用单边 RDMA 写操作，仅需要在发送端和接收端分别进行一次 DMA 操作。
RSocket 使用双边 RDMA，LibVMA 使用一个类似的数据包接口，因此接收端需要增加一次 DMA 操作。
LibVMA 和 RSocket 使用网卡来转发单机内的数据包，而 \sys{} 使用共享内存。
Linux 的高延迟主要是由于中断处理和进程唤醒。
对于较大的消息，\sys{} 消除了数据拷贝，页面重映射的开销明显更低。
RSocket 比 LibVMA 和 Linux 的性能更好，由于它把发送端的数据拷贝、RDMA 发送操作和接收端的数据拷贝操作流水线化了。
\sys{} 的连接建立延迟主要来源于通过 Linux 裸套接字的初始握手以及通过 \texttt{libibverbs} 创建 RDMA QP。

\begin{table}[htbp]
	\centering
	\small
		\begin{tabular}{l|l|r|r|r|r}
			\hline
			类型 & 开销 & \sys & LibVMA & RSocket & Linux \\
			\hline
			\hline
			每操作 & 总共（线程不安全） & 53 & 56 & 71 & 413 \\
			\hline
			每操作 & 总共（线程安全） & 53 & 177 & 209 & 413 \\
			\hline
			每操作 & C 库封装 & 15 & 10 & 10 & 12 \\
			\hline
			每操作 & 内核穿越（系统调用） & N/A & N/A & N/A & 205 \\
			\hline
			每操作 & 套接字文件描述符锁 & N/A & 121 & 138 & 160 \\
			\hline
			\hline
			每数据包 & 总共（主机间） & 850 & 2200 & 1700 & 15000 \\
			\hline
			每数据包 & 总共（主机内） & 150 & 1300 & 1000 & 5800 \\
			\hline
			每数据包 & 缓冲区管理 & 50 & 320 & 370 & 430 \\
			\hline
			每数据包 & 传输层协议 & N/A & 260 & N/A & 360 \\
			\hline
			每数据包 & 数据包处理 & N/A & 200 & N/A & 500 \\
			\hline
			每数据包 & 网卡门铃和 DMA & 600 & 900 & 900 & 2100 \\
			\hline
			每数据包 & 网卡处理 \& wire & \multicolumn{4}{c}{200} \\
			\hline
			每数据包 & 处理网卡中断 & N/A & N/A & N/A & 4000 \\
			\hline
			每数据包 & 进程唤醒 & N/A & N/A & N/A & 5000 \\
			\hline
			\hline
			每千字节 & 总共（主机间）& 173 & 540 & 239 & 365 \\
			\hline
			每千字节 & 总共（主机内） & 13 & 381 & 212 & 160 \\
			\hline
			每千字节 & 线上传输 & \multicolumn{4}{c}{160} \\
			\hline
			\hline
			每连接 & 总共（主机间）& 47000 & 18000 & 77000 & 47000 \\
			\hline
			每连接 & 总共（主机内） & 700 & 3800 & 33000 & 14700 \\
			\hline
			每连接 & 初始 TCP 握手 & 16000 & 16000 & 47000 & N/A \\
			\hline
			每连接 & 管程处理 & 180 & N/A & N/A & N/A \\
			\hline
			每连接 & RDMA QP 创建 & 30000 & N/A & 30000 & N/A \\
			\hline
		\end{tabular}
	\caption{\sys{} 和其他系统的延迟分解。每操作延迟使用 \texttt{fcntl()} 测量，每数据包和每千字节延迟是从 \texttt{send()} 到 \texttt{recv()} 的时间，每连接延迟是连接创建的延迟。表中数字单位为纳秒，仅代表粗略估计。}
	\label{tab:microbenchmark}
\end{table}


\subsubsection{多核可扩放性}






\sys 实现了主机内和主机间套接字的几乎线性可扩展性。
对于主机内套接字，\sys 在16对发送器和接收器核心之间提供每秒306~M消息的吞吐量，这是Linux的40倍和RSocket的30倍。
LibVMA回退到Linux用于主机内套接字。
使用RDMA作为主机间套接字，\sys 使用批处理以16个内核实现每秒276~M个消息的吞吐量，这是本章使用的RDMA网卡的消息吞吐量的2.5倍，也是 RSocket 吞吐量的 8 倍。
不启用批处理时，\sys{} 只能达到 62~M 的吞吐量，是裸 RDMA 的 60\%。
由于缓冲区管理的可扩展性有限，RSocket的主机内部吞吐量为24~M，主机间为33~M。
由于共享网卡队列上的锁争用，与单线程相比，LibVMA的吞吐量减少到两个线程的1/4，而三个和更多线程的1/10。
Linux吞吐量从1到7个核心线性扩展，并在环回或具有更多核心的网卡队列上出现瓶颈。
尽管本文没有测试，mTCP 预计将在多核情况下有更好的可扩放性。

\begin{figure*}[htbp]
	\subfloat[单机内吞吐量。]{                    
		%\begin{minipage}{0.4\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{eval/microbenchmark/corenum-IPC-tput.pdf}
		\label{socksdirect:fig:eval-cornum-ipc}
		%\end{minipage}
	}
	\subfloat[跨主机吞吐量。]{
		%\begin{minipage}{0.4\textwidth}
		\centering \includegraphics[width=0.5\textwidth]{eval/microbenchmark/corenum-network-tput.pdf}
		\label{socksdirect:fig:eval-cornum-network}
		%\end{minipage}
	}
	
	\caption{不同 CPU 核数下的 8 字节消息吞吐量。}
	\label{socksdirect:fig:eval-corenum-tput}
\end{figure*}


%The multi-thread scalability of \sys  attributes to the partitioning of states and removal of synchronization.
%We can also see that shared memory communication has 5x throughput than RDMA.% Using RDMA 网卡 for intra-host socket would meet this bottleneck and thus not scalable.

%Figure~\ref{socksdirect:fig:eval-conn-setup-tput} shows the throughput of connection creation with different number of cores. Each core can create 1.4~M new connections per second, which is 20x of Linux and 2x of mTCP~\cite{jeong2014mtcp}. The upper bound is 5.3~M connections per second, where the monitor becomes a bottleneck.

最后评估共享核心的多个线程的性能。 每个线程都需要等待轮到它来处理消息。
如图 \ref {socksdirect:fig:eval-context-switch}所示，尽管消息处理延迟几乎与活动进程的数量呈线性增长，但它仍然是Linux的1/20到1/30。


\begin{figure}[htbp]
	%\centering
	%\includegraphics[width=\textwidth]{eval/microbenchmark/conn-setup-tput.pdf}
	%
	%\caption{Connection creation throughput with number of cores.}
	%\label{socksdirect:fig:eval-conn-setup-tput}
	
	%\begin{minipage}{0.4\textwidth}
	\centering \includegraphics[width=0.5\textwidth]{eval/microbenchmark/sharecore-lat.pdf}
	
	\caption{多进程共享 CPU 核的消息处理延迟。}
	\label{socksdirect:fig:eval-context-switch}
	%\end{minipage}
\end{figure}


%Finally, we benchmark the throughput and latency after \texttt{fork} and other corner-case operations. Initially, there is only one pair of sender and receiver. At time $T_0$, receiver forks, and the parent process keeps receiving. At time $T_1$, the child process begins to receives takes over the socket. At time $T_2$, sender forks, and only the parent sends. At time $T_3$, the child sender also starts sending. We find that both throughput and latency resume to initial maximal performance within 1~ms after each event.
