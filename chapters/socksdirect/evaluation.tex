\section{Evaluation}
\label{socksdirect:sec:evaluation}

\begin{figure*}[t!]
		\centering
		\subfloat[Intra-host throughput.]{                    
			%\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.5\textwidth]{eval/microbenchmark/msgsize-ipc-tput.pdf}
			\label{socksdirect:fig:eval-msgsize-ipc-tput}
			%\end{minipage}
		}
		\subfloat[Intra-host latency.]{
			%\begin{minipage}{0.4\textwidth}
			\centering \includegraphics[width=0.5\textwidth]{eval/microbenchmark/msgsize-ipc-lat.pdf}
			\label{socksdirect:fig:eval-msgsize-ipc-lat}
			%\end{minipage}
		}
		
		\caption{Single-core intra-host performance with message sizes.}
		\label{socksdirect:fig:eval-msgsize-intra}
\end{figure*}


\begin{figure*}[t!]
		\centering
		\subfloat[Inter-host throughput.]{
			%\begin{minipage}{0.4\textwidth}
			\centering \includegraphics[width=0.5\textwidth]{eval/microbenchmark/msgsize-network-tput.pdf}
			\label{socksdirect:fig:eval-msgsize-network-tput}
			%\end{minipage}
		}
		\subfloat[Inter-host latency.]{
			%\begin{minipage}{0.4\textwidth}
			\centering \includegraphics[width=0.5\textwidth]{eval/microbenchmark/msgsize-network-lat.pdf}
			\label{socksdirect:fig:eval-msgsize-network-lat}
			%\end{minipage}
		}
		
		\caption{Single-core inter-host performance with message sizes.}
		\label{socksdirect:fig:eval-msgsize-inter}
\end{figure*}

\begin{figure*}[t!]
		\subfloat[Intra-host throughput.]{                    
			%\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.5\textwidth]{eval/microbenchmark/corenum-IPC-tput.pdf}
			\label{socksdirect:fig:eval-cornum-ipc}
			%\end{minipage}
		}
		\subfloat[Inter-host throughput.]{
			%\begin{minipage}{0.4\textwidth}
			\centering \includegraphics[width=0.5\textwidth]{eval/microbenchmark/corenum-network-tput.pdf}
			\label{socksdirect:fig:eval-cornum-network}
			%\end{minipage}
		}
		
		\caption{8-byte data transmission throughput with number of cores.}
		\label{socksdirect:fig:eval-corenum-tput}
\end{figure*}

\begin{figure*}[t!]
		\centering
		\subfloat[intra-host]{                    
			%\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.5\textwidth]{eval/microbenchmark/connnum-ipc-tput.pdf}
			\label{socksdirect:fig:eval-connnum-ipc-tput}
			%\end{minipage}
		}
		\subfloat[inter-host]{
			%\begin{minipage}{0.4\textwidth}
			\centering \includegraphics[width=0.5\textwidth]{eval/microbenchmark/connnum-network-tput.pdf}
			\label{socksdirect:fig:eval-connnum-network-tput}
			%\end{minipage}
		}
		
		\caption{Single-core throughput with number of connections.}
		\label{socksdirect:fig:eval-connnum-tput}
\end{figure*}

%\begin{figure}[htpb]
%	\centering
%	\includegraphics[width=\columnwidth]{eval/microbenchmark/fork-tput.pdf}
%	\caption{Throughput of SocksDirect with fork happening}
%	\label{socksdirect:fig:eval-fork-tput}
%\end{figure}

We implement \sys in three components: a user-space library \libipc{} and a monitor daemon with 17K lines of C++ code, and a modified RDMA NIC driver to support zero copy. We evaluate \sys in the following aspects:

\parab{Use shared memory efficiently for intra-host socket.}
For 8-byte messages, \sys achieves 0.3$\mu$s RTT and up to 23~M messages per second throughput. For large messages, \sys uses zero copy to achieve 1/13 latency and 26x throughput of Linux.

\parab{Use RDMA efficiently for inter-host socket.}
\sys achieves 1.7$\mu$s RTT, close to raw RDMA performance.
With zero copy, one connection saturates an 100~Gbps link.

%\parab{Robust with number of connections.}
%The performance above can be maintained with up to 100 million connections.

\parab{Scalable with number of cores.}
The throughput is almost linearly scalable with number of cores. %In addition, a single CPU core can create 1.4~M connections per second.

%\parab{Corner-case operations does not affect long-term performance.}
%After corner-case operations such as \texttt{fork}, the performance recovers quickly.

\parab{Significant speedup with unmodified end-to-end applications.}
As an example, \sys{} reduces Nginx HTTP request latency by 5.5x to 20x.


\subsection{Methodology}
\label{socksdirect:subsec:methodology}

We evaluate \sys on servers with two Xeon E5-2698 v3 CPUs, 256~GiB memory and a Mellanox ConnectX-4 NIC. The servers are interconnected with an Arista 7060CX-32S 100G switch~\cite{arista-7060cx}. We use Ubuntu 16.04 with Linux 4.15, RoCEv2 for RDMA and poll completion queue every 64 messages.
Each thread is pinned on a CPU core. We run tests for enough warm-up rounds before collecting data.
For latency, we build a ping-pong application and report the mean round-trip time, with error bars representing 1\% and 99\% percentile.
%For throughput, one side keeps sending data while the other side keeps receiving data.
We compare with Linux, raw RDMA write verb, Rsocket~\cite{rsockets}, and LibVMA~\cite{libvma}, a user-space TCP/IP stack optimized for Mellanox NICs.
We did not evaluate mTCP~\cite{jeong2014mtcp} because the underlying DPDK library has limited support on our NIC. mTCP has much higher latency than RDMA due to batching, and the reported throughput was 1.7~M packets per second~\cite{kalia2018datacenter}. %The throughput reported in mTCP paper was 1.7~M packets per second~\cite{kalia2018datacenter}, while RDMA achieves more than 10~M messages per second.
This work does not raise any ethical issues.

\subsection{Microbenchmarks}
\label{socksdirect:subsec:microbenchmark}

\subsubsection{Latency and Throughput}
\quad



Figure~\ref{socksdirect:fig:eval-msgsize-intra} shows intra-host socket performance between a pair of sender and receiver threads.
For 8-byte messages, \sys achieves 0.3$\mu$s round-trip latency (1/35 of Linux) and 23~M messages per second throughput (20x of Linux).
In comparison, a simple SHM queue has 0.25$\mu$s round trip latency and 27~M throughput, indicating that \sys adds little overhead.
RSocket has 6x latency and 1/4 throughput of \sys{}, because it uses the NIC to forward intra-host packets, which incurs PCIe latency.
LibVMA simply uses kernel TCP socket for intra-host.
The one-way delay of \sys{} is 0.15$\mu$s, even lower than a kernel crossing (0.2$\mu$s). Kernel-based sockets require a kernel crossing on both sender and receiver.

Due to memory copy, for 8~KiB messages, the throughput of \sys is only 60\% higher than Linux, and the latency is 4x lower. For messages with at least 16~KiB size, \sys uses page remapping to achieve zero copy.
For 1~MiB messages, \sys achieves 1/13 latency and 26x throughput than Linux.
The latency of RSocket is unstable and may be even larger than Linux in some cases, because of event notification delay.


Figure~\ref{socksdirect:fig:eval-msgsize-inter} shows inter-host socket performance between a pair of threads.
For 8-byte messages, \sys achieves 18M messages per second throughput (15x of Linux) and 1.7$\mu$s latency (1/17 of Linux).
The throughput and latency is close to raw RDMA write operations (shown as dashed line), which does not have socket semantics.
\sys{} has even higher throughput than RDMA for 8-byte messages due to batching.
LibVMA also uses batching to achieve better throughput than \sys{} for 16 to 128 byte messages, but the latency is 7x of \sys{}.
For message sizes less than 8~KiB, the throughput of inter-host RDMA is  slightly lower than intra-host SHM, because the ring buffer structure is shared.
For 512B to 8KiB messages, \sys{} is bounded by packet copy, but still faster than RSocket and LibVMA due to reduced buffer management overheads.
For zero copy messages ($\ge$16 KiB), \sys{} saturates the network bandwidth, which has 3.5x throughput of all compared works and 72\% latency of RSocket.



\begin{figure}[t!]
		%\centering
		%\includegraphics[width=\textwidth]{eval/microbenchmark/conn-setup-tput.pdf}
		%
		%\caption{Connection creation throughput with number of cores.}
		%\label{socksdirect:fig:eval-conn-setup-tput}
		
		%\begin{minipage}{0.4\textwidth}
		\centering \includegraphics[width=0.5\textwidth]{eval/microbenchmark/sharecore-lat.pdf}
		
		\caption{Message processing latency where processes share a core.}
		\label{socksdirect:fig:eval-context-switch}
		%\end{minipage}
\end{figure}
\begin{figure}[t!]
				\centering \includegraphics[width=0.5\textwidth]{eval/web/msgsize-clocal-lat.pdf}
		
		\caption{Nginx HTTP request end-to-end latency.}
		\label{socksdirect:fig:eval-nginx}
\end{figure}
\begin{figure}[t!]
		\centering
		\includegraphics[width=0.5\textwidth]{eval/microbenchmark/nfv-tun-tput.pdf}
		
		\caption{Throughput of network function pipeline.}
		\label{socksdirect:fig:eval-tun-tput}
		%\includegraphics[width=\textwidth]{eval/microbenchmark/nginx-short-tput.pdf}
		%
		%\label{socksdirect:fig:eval-nginx-short}
		%\caption{Nginx throughput.}
		
		%\centering \includegraphics[width=\textwidth]{eval/microbenchmark/nginx-multiround-tput.pdf}
		%
		%\caption{End-to-end HTTP request latency of a web service.}
		%\label{socksdirect:fig:eval-nginx-multiround}
		
		%\centering
		%\includegraphics[width=\textwidth]{eval/microbenchmark/corenum-http-tput.pdf}
		%
		%\caption{Multi-core scalability of HTTP backend service throughput.}
		%\label{socksdirect:fig:eval-http-tput}
\end{figure}


\subsubsection{Multi-core Scalability}
\quad

%Figure~\ref{socksdirect:fig:eval-connnum-tput} shows the throughput with different number of concurrent connections.
%We establish connections between two processes before testing, then send and receive data from the connections in a round-robin order.
%\sys can support more than 100 million concurrent connections with 16~GiB of memory, and the throughput does not degrade under such high concurrency.
%\sys achieves connection stability by multiplexing connections via a single queue.
%In comparison, the performance of RDMA, LibVMA and Linux drops quickly as the number of connections increase. There is a sharp performance drop with more than 512 RDMA connections, because the RDMA transport states saturate the NIC cache. Although LibVMA and Linux do not use RDMA as transport, they maintain per-FD buffers, which lead to CPU cache and TLB miss with thousands of connections. Furthermore, LibVMA installs flow steering rules to NIC for each connection, which also leads to NIC cache miss.




Figure~\ref{socksdirect:fig:eval-corenum-tput} shows the throughput of 8-byte messages with different number of cores.
Sender and receiver process each creates several threads, then each thread pins on a core and communicates with a peer thread.
\sys achieves almost linear scalability for both intra-host and inter-host sockets.
For intra-host socket, \sys provides 306~M message per second throughput between 16 pairs of sender and receiver cores, which is 40x of Linux and 30x of RSocket.
LibVMA falls back to Linux for intra-host socket.
Using RDMA for inter-host socket, \sys uses batching to achieve 276~M messages per second throughput with 16 cores, which is 2.5x of the message throughput of our RDMA NIC.
RSocket peeks at 24~M for intra-host and 33~M for inter-host due to limited scalability of buffer management.
%Although \sys{} creates $n^2$ queues for $n$ sender and receiver threads, in this workload only $n$ of them are active.
%Because the NIC only caches active RDMA connections and \libipc{} only polls active queues, the idle queues do not degrade performance.
Due to lock contention on shared NIC queues, compared to single thread, the throughput of LibVMA reduces to 1/4 with two threads, and 1/10 with three and more threads.
The Linux throughput scales linearly from 1 to 7 cores and bottlenecks on the loopback or NIC queues with more cores.
%The multi-thread scalability of \sys attributes to the partitioning of states and removal of synchronization.
%We can also see that shared memory communication has 5x throughput than RDMA.% Using RDMA NIC for intra-host socket would meet this bottleneck and thus not scalable.

%Figure~\ref{socksdirect:fig:eval-conn-setup-tput} shows the throughput of connection creation with different number of cores. Each core can create 1.4~M new connections per second, which is 20x of Linux and 2x of mTCP~\cite{jeong2014mtcp}. The upper bound is 5.3~M connections per second, where the monitor becomes a bottleneck.


Finally, we evaluate the performance of multiple threads sharing a core. Each thread needs to wait for its turn to process messages.
As Figure~\ref{socksdirect:fig:eval-context-switch} shows, although the message processing latency increases almost linearly with number of active processes, it is still 1/20 to 1/30 of Linux.



%Finally, we benchmark the throughput and latency after \texttt{fork} and other corner-case operations. Initially, there is only one pair of sender and receiver. At time $T_0$, receiver forks, and the parent process keeps receiving. At time $T_1$, the child process begins to receives takes over the socket. At time $T_2$, sender forks, and only the parent sends. At time $T_3$, the child sender also starts sending. We find that both throughput and latency resume to initial maximal performance within 1~ms after each event.
