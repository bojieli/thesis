%!TEX root=main.tex
\section{系统实现}
\label{clicknp:sec:impl}

\subsection{\name 工具链和硬件实现}

\iffalse
\textbf{图：编译生成的 OpenCL 代码示例，（1）只包含顺序和选择结构，不包含分支结构，（2）非阻塞（每个流水级有最坏时钟周期数的保证）。可以避免 handler 阻塞 signal。}
\fi

本文实现了一个\name 编译器，作为\name 工具链的前端（\S \ref {clicknp:subsec:toolchain}）。
对于宿主程序，使用Visual C++作为后端。进一步集成了Altera OpenCL SDK（v15.1）\cite {aoc}和Xilinx Vivado HLS（v2015.4）\cite {vivado}作为FPGA程序的后端。
\name 编译器包含约 1 万行 C++、flex 和 bison 代码，它们解析配置文件和元件声明，执行第 \ref {clicknp:sec:optimization} 节所述的优化，并生成特定于每个商业高层次综合工具的代码。
使用 Altera OpenCL 时，每个\name 元件都被编译成 \textit {kernel}，元件之间的连接被转换为Altera 扩展通道（channel）。
使用Xilinx Vivado HLS 时，将每个元件编译成IP内核，并使用AXI流（stream）来实现元件之间的连接。
元件也可以编译为CPU二进制文件，管理器线程将为每个主机元件创建一个工作线程。
主机和FPGA元件之间的每个连接都映射到PCIe I / O通道的\textit {插槽}（\S \ref {clicknp:subsec:pcie}）。


本章的硬件平台基于Altera Stratix V FPGA和Catapult shell \cite {putnam2014reconfigurable}。
Catapult shell还包含一个OpenCL特定的运行时（BSP）。\name 用户逻辑通过 BSP 与shell通信。
\name 用户逻辑运行在独立的时钟域内，BSP 将 shell 中处于不同时钟域的 PCIe DMA、DRAM 等接口通过异步 FIFO 转换为用户逻辑所在的时钟域。
BSP 还提供了 OpenCL 内核启动、停止等管理功能。
撰写本文时，作者还没有获得Xilinx硬件平台。
因此，主系统评估基于使用\name + OpenCL的Altera平台，使用Vivado HLS生成的报告（例如频率和面积成本）来了解\name + Vivado的性能。

FPGA 编程的一个限制是编译时间相当长。一个网络数据包简单转发的功能就需要 3 小时来编译。编译时间主要由高层次综合、IP 核生成、硬件描述语言逻辑综合、FPGA 布局布线、时序分析等几个阶段构成。
本章采用几项技术来缩短 FPGA 编译时间。
第一，OpenCL 编程框架会把高层次综合生成的 Verilog 模块生成 IP 核，插入到 shell 部分中，这需要复制大量 IP 核和 shell 部分的代码。注意到用户逻辑的外围接口是固定的，本文预先生成 IP 核，只需将高层次综合 Verilog 模块文件替换进项目中；对于 shell 部分代码，使用文件引用而非拷贝。
第二，为了缩短逻辑综合时间，将固定的 shell 部分通过逻辑综合生成网表（netlist）文件，利用设计分区（design partition）保留综合结果，可以将 shell 部分的逻辑综合时间减少约 35 分钟。
第三，为了加快 FPGA 布局算法的收敛速度，在初始编译完成后，增加 shell 部分各模块的布局约束，将其布局基本上固定下来。shell 部分的模块大多与芯片固定位置的硬核（hard IP）交互，固定布局在硬核附近是合理的。但是为了更好的性能，本文没有使用设计分区将 shell 部分的布局布线完全固定下来。这项优化可节约大约 20 分钟。
第四，区分调试（debug）和发布（release）编译模式。调试模式旨在验证逻辑上的正确性，而不追求性能。在调试模式下，把用户逻辑的时钟频率固定为 50~MHz，大大降低了布局布线的难度；使用设计分区固化 shell 部分的布局布线，这部分高频逻辑的布局布线需要较长的时间。调试模式相比发布模式可节约 25 分钟编译时间，在用户逻辑更复杂时节约的时间将更多。
第五，删除不必要的时序分析模型。OpenCL 框架默认在四种情形下分析时序约束，但只要满足其中最苛刻的一种，其余三种也能满足，因此我们仅保留最苛刻的时序约束模型。
第六，OpenCL 框架为了尽可能提高性能，首先使用较高的用户逻辑时钟频率（如 250~MHz）编译，然后根据时序分析结果计算出最长延迟和能正确工作的最高时钟频率，再用该时钟频率进行二次布局布线和时序分析。这对计算密集型工作负载可以达到尽可能高的吞吐量。然而，本文关注网络数据包处理，只需达到网络线速的处理能力，因此可以固定时钟频率为 180 MHz，大多数用户逻辑均可达到此频率，因此无需重新布局布线。

\begin{table}[htbp]
	\centering
	\caption{FPGA 编译加速技术。}
	\label{clicknp:tab:fpga-compilation}
	\small
	\begin{tabular}{l|p{.4\textwidth}|p{.15\textwidth}|p{.15\textwidth}}
		\toprule
		编译阶段 & 优化方法 & 优化前编译时间（分） & 优化后编译时间（分） \\
		\midrule
		ClickNP 编译 & -- & 0.1 & 0.1 \\
		高层次综合 & -- & 1 & 1 \\
		生成 IP 核 & 预先生成 IP 核；使用文件引用而非拷贝 & 10 & 0 \\
		逻辑综合 & 保留 shell 的综合结果 & 50 & 15 \\
		布局布线 & 增加 shell 的布局约束；调试模式下降低用户逻辑的时钟频率，保留 shell 的布局布线结果 & 60 & 40 (15)* \\
		时序分析 & 删除不必要的时序分析模型 & 15 & 5 (0)* \\
		二次布局布线 & 固定时钟频率，无需重新布局布线 & 30 & 0 \\
		二次时序分析 & 删除 & 15 & 0 \\
		\midrule
		总计 & -- & 180 & 60 (30)* \\
		\bottomrule
		\multicolumn{4}{l}{* 括号内是调试模式下的编译时间。}
	\end{tabular}
\end{table}

表 \ref{clicknp:tab:fpga-compilation} 总结了上述编译加速技术。优化前，开发者每个工作日只能进行 3 轮调试，而优化后在调试模式下可调试 10 轮左右，需要性能的系统测试也可进行约 6 轮，大大提高了工作效率。


\subsection{\name 元件库}
\label{clicknp:subsec:lib}

本文实现了一个包含近 200 个元件的\name 元件库。
其中一部分（20％）直接来自Click Modular Router，但使用\name 编程框架重新编写。
这些元件涵盖了网络功能的大量基本操作，包括数据包解析，校验和计算，封装/解封，哈希表，最长前缀匹配（LPM），速率限制，加密和数据包调度。
由于\name 的模块化体系结构，每个元件的代码大小都是适中的。
元件的平均代码行数（Line of Code，LoC）是80，最复杂的元件 \textit {PacketBuffer} 有196行C代码。


\begin{sidewaystable}[htbp]
	\centering
	\caption{\name 中的一些网络元件。}
	\label{clicknp:tab:elements}
	\small
	\begin{tabular}{l|l|l|l|l|l|l|r|r}
		\toprule
		&	&	& \multicolumn{4}{c|}{性能} & \multicolumn{2}{c}{资源占用 (\%)} 			\\
		\cline{4-7} \cline{8-9}  
		
		元件 	& 配置 & 优化 & \tabincell{c}{最高频率\\ (MHz)} & 峰值吞吐量 & 加速比 (FPGA/CPU) & 延迟（时钟周期） & LE & BRAM \\
		\midrule
		L4\_Parser (A1-5)  & N/A & REG & 221.93 & 113.6 Gbps & 31.2x / 41.8x & 11 & 0.8\% & 0.2\% \\
		IPChecksum (A1-4) & N/A & UL & 226.8 & 116.1 Gbps & 33.1x / 55.1x & 18 & 2.3\% & 1.3\% \\
		NVGRE\_Encap (A1,4) & N/A & REG, UL & 221.8 & 113.6 Gbps & 35.5x / 42.9x & 9 & 1.5\% & 0.6\% \\
		\midrule
		AES\_CTR (A3) & 16 字节块大小 & UL & 217.0 & 27.8 Gbps & 79.9x / 255x & 70 & 4.0\% & 23.1\% \\
		SHA1 (A3) & 64B 字节块大小 & MS, UL & 220.8 & 113.0 Gbps & 157.5x / 83.1x & 105 & 7.9\% & 6.6\% \\
		\midrule
		\midrule
		CuckooHash (A2) & 128K 个条目 & MS, UL, DW & 209.7 & 209.7 Mpps & 43.6x / 57.5x & 38 & 2.0\% & 65.5\% \\
		HashTCAM (A2) & 16 x 1K 个条目 & MS, UL, DW & 207.4 & 207.4 Mpps & 155.9x / 696x & 48 & 18.7\% & 22.0\% \\
		LPM\_Tree (A2) & 16K 个条目 & MS, UL, DW & 221.8 & 221.8 Mpps & 34.5x / 45.2x & 181 & 4.3\% & 13.2\% \\
		FlowCache (A4) & 4 路组相连，16K 条目 & MS, DW & 105.6 & 105.6 Mpps & 55.8x / 21.5x & 27 & 5.6\% & 46.9\% \\
		\midrule
		
		SRPrioQueue (A5) & 32 个数据包的缓冲区 & REG, UL & 214.5 & 214.5 Mpps & 150.3x / 28.6x & 41 & 2.6\% & 0.6\% \\
		RateLimiter (A1,5) & 16K flows & MS, DW & 141.5 & 141.5 Mpps & 7.0x / 65.3x & 14 & 16.9\% & 14.1\% \\
		\bottomrule
		
		\multicolumn{9}{l} {\textbf{优化方法。} REG=Using Registers 使用寄存器; MS=Memory Scattering 内存散射; UL=Unroll Loop 循环展开; DW=Delay Write 延迟写入。} \\
		\multicolumn{9}{l} {\parbox{\textwidth}{\textbf{性能提升} 一列对比了应用第 \S\ref{clicknp:sec:optimization}  节的优化后和优化前的 FPGA 性能。也列出了与 CPU 实现间的性能对比。}}
		
	\end{tabular} 
\end{sidewaystable}

表 \ref {clicknp:tab:elements} 显示了在 \name 中实现的一组选定的关键元件。
除了元件名称，还标记了使用该元件的演示网络功能（在 \S \ref {clicknp:sec:application} 中讨论）。
之前在 \S \ref {clicknp:subsec:paral_in_elem} 中讨论的优化技术最小化了内存依赖性，并平衡了流水线阶段。
第3列总结了每个元件使用的优化技术。
对于表 \ref {clicknp:tab:elements} 顶部的元件，元件中的处理逻辑需要访问数据包的每个字节。其中吞吐量以Gbps显示。
但是，表格底部的元件只处理数据包头（元数据）。因此，使用每秒数据包来测量吞吐量更有意义。
值得注意的是，表 \ref {clicknp:tab:elements} 中测量的吞吐量是相应元件可以实现的最大吞吐量。
当它们在真正的网络功能中工作时，其他组件，例如以太网端口，可能是瓶颈。
作为参考，表 \ref {clicknp:tab:elements} 将优化版本与本文最初在FPGA上的实现进行比较，而不应用\S \ref {clicknp:sec:optimization} 中讨论的技术以及CPU实现。
很明显，经过优化后，所有这些元件都可以非常有效地处理数据包，与初始的FPGA实现相比，速度提高了7倍~117倍，与一个CPU内核上的软件实现相比，速度提高了21％。
这种性能提升来自于利用FPGA中的巨大并行性的能力。
考虑到FPGA的功耗（大约30W）和CPU（每个核心大约5W），\name 元件的能耗效率比CPU高4到120倍。

表 \ref {clicknp:tab:elements} 还显示了每个元件的处理延迟。
可以看到，这个延迟很低：平均值是 $0.19 \mu s$，最大值仅为$0.8 \mu s$（LPM \_Tree）。
最后两列总结了每个元件的资源利用率。 利用率归一化为FPGA芯片的容量。
大多数元件只使用少量的逻辑元件。
这是合理的，因为大多数数据包操作都很简单。
HashTCAM和RateLimiter具有适度的逻辑资源使用，因为这些元件具有更大的仲裁逻辑。
但是，BRAM的使用很大程度上依赖于元件的配置。 例如，BRAM使用率随流表中支持的条目数呈线性增长。
总而言之，本文使用的FPGA芯片有足够的容量来支持包含少量元件的有意义的网络功能。



\subsection{PCIE I/O 通道}
\label{clicknp:subsec:pcie}

如前所述，\name 的一个关键属性是支持联合CPU / FPGA处理。
本章通过设计高吞吐量，低延迟的PCIe I / O通道来实现这一目标。
为此，本文扩展了OpenCL运行时，添加一个新的I / O通道，并连接到shell中的PCIe交换机。
PCIe交换机将仲裁新添加的I / O通道与shell中的其他组件（例如DDR内存控制器）之间的访问。


\iffalse
\textbf{图：ClickNP element，channel，signal 菊花链，command hub 的示例}
\fi

在FPGA中，一个名为\textit {CmdHub}的特殊元件由\name 编译器自动生成，使用FIFO缓冲区将数据从不同的插槽重定向到相应的FPGA元件。
\textit {CmdHub}还用于将控制信号从管理器线程分发到FPGA元件。
为了识别目标元件，在信号消息中嵌入元件ID，并且\textit {CmdHub}可以读取ID并再次通过FIFO缓冲器将信号消息转发到相应的元件。

\name 支持灵活的 I/O 操作。如图 \ref{clicknp:fig:pcie-io} 所示，\name 提供了基于槽位（slot）和工作队列（work queue）两种与主机 CPU 通信的抽象，还提供了裸 DMA 操作的接口。


\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{image/pcie-io}
	\caption{PCIe I/O 通道的架构。}
	\label{clicknp:fig:pcie-io}
\end{figure}


在基于槽位的抽象中，PCIe 物理链路划分为64个逻辑子通道，即\textit {slots}（插槽）。
每个插槽都有一对用于DMA操作的发送和接收缓冲区。
在64个插槽中，33个由 OpenCL BSP 用于管理 ClickNP 内核和访问板载 DDR（即 OpenCL 控制通道），一个插槽用于将信号传递给 \name 元件。
剩余的30个插槽用于FPGA和CPU元件之间的数据通信。
CPU 上的槽位抽象可以在中断或轮询模式下工作。

槽位抽象中每发送一个数据都需要至少 4 次 DMA 操作 \footnote{从主机 CPU 发送数据到 FPGA 的过程是：主机 CPU 写 FPGA 中的下行控制寄存器（又称门铃，doorbell）；FPGA 从主机内存中 DMA 读取数据。当 FPGA 处理完槽位中的数据后，写主机内存中的下行完成寄存器，并向主机 CPU 发送中断。从 FPGA 向主机 CPU 发送数据的过程是：FPGA 读内部的上行控制寄存器，判断为空；FPGA 向主机内存 DMA 写入数据，并向主机 CPU 发送中断。主机接收 FPGA 发来的数据过程是：读 FPGA 中的上行控制寄存器，判断非空；读取主机内存中的数据；写 FPGA 中的上行控制寄存器，表示处理完毕。}，且需要等待对面的设备处理完成才能在同一槽位发送下一个数据。
为分摊 DMA 开销、提高消息发送的并发度，工作队列是槽位抽象的扩展，每个槽位不再仅有一对缓冲区，而是有一对用于发送和接收的环形缓冲区队列。
每条环形缓冲区队列有 128 个槽位，按照先进先出的方式访问。
发送端发现环形缓冲区队列中还有尚未被取走的数据时，就无需通知对端，节约了 CPU 通过 PCIe MMIO 发送门铃（doorbell）和 FPGA 发送中断的开销。

除了槽位和工作队列，FPGA 和 CPU 之间还需要更灵活的通信方式。首先，第 \ref{chapter:kvdirect} 章的键值存储中，FPGA 需要直接读写主机内存中的键值，而无需主机 CPU 参与，这就需要 FPGA 能够直接发出裸的 PCIe DMA 读写请求。
其次，在基于可编程网卡的内存解聚中，FPGA 将远程内存通过 PCIe MMIO 直接映射到主机内存空间，主机 CPU 直接访问此内存空间，生成 PCIe DMA 读写请求发送到 FPGA。FPGA 中的用户逻辑需要处理这些 DMA 被动读写操作。
最后，一些应用（如传统 OpenCL 应用）可能希望主机 CPU 与 FPGA 之间采用 FPGA 板上的 DRAM 作为共享内存，因此 FPGA 板上的 DRAM 通过 PCIe MMIO 映射到主机内存空间，由 shell 中的 PCIe 被动读写逻辑发送到 DRAM 控制器。由于主机 PCIe MMIO 读写大块数据的效率较低，也支持主机 CPU 通过控制寄存器，让 FPGA shell 主动发起 DMA 在板上 DRAM 和主机内存间搬运数据。



图 \ref {clicknp:fig:pcie} 显示了具有不同插槽数和批量大小的PCIe I/O通道的基准测试结果。
作为基线，还测量了OpenCL全局内存（global memory）操作的性能 -- 到目前为止，这是OpenCL \cite {opencl} 中为CPU/FPGA通信提供的唯一方法。
可以看到，单个插槽的最大吞吐量约为8.4~Gbps。
通过4个插槽，PCIe I/O通道的总吞吐量可达25.6~Gbps \footnote{这是本章写作时使用的 PCIe Gen2 x8 硬核的实际最高性能。事实上该 FPGA 支持 PCIe Gen3 x8 硬核，第 \ref{chapter:kvdirect} 章通过替换硬核和优化 shell，实现了 2 倍的 PCIe I/O 通道吞吐量。}。
但是，OpenCL的吞吐量低得惊人，低于1~Gbps。这是因为全局内存API旨在传输 GB 级的大量数据。这可能适用于具有大数据集的应用程序，但不适用于需要强流处理（stream prpocessing）功能的网络功能。
同样，图 \ref {clicknp:fig:pcie}（b）显示了通信延迟。由于OpenCL未针对流处理进行优化，因此OpenCL延迟高达1~ms，通常是网络功能无法接受的。
相比之下，PCIe I/O 通道在轮询模式下具有非常低的 1 $\mu$s 的延迟（一个 CPU 核重复轮询状态寄存器），而在中断模式下的延迟为 9 $\mu$ s（几乎没有CPU开销） 。

\begin{figure}[htbp]
	\centering
	\subfloat[]{
		\includegraphics[width=0.5\textwidth]{eval/pcie_1}
	}
	\subfloat[]{
		\includegraphics[width=0.5\textwidth]{eval/pcie_2}
	}

	\caption{PCIe I/O 通道的性能。Y 轴是对数坐标系。}

	\label{clicknp:fig:pcie}
\end{figure}


\subsection{Verilog 元件}

对于一些元件（如乘法器阵列和调度器），高层次综合工具难以生成高效的硬件逻辑。为了节约面积和降低延迟，这些元件需要用硬件描述语言来编写。在 \name 中，每个元件被编译成一个 OpenCL 核（kernel），它被高层次综合工具编译成一个 Verilog 模块，使用 Altera Avalon ST 或 Xilinx AXI 接口连接管道，使用 Altera Avalon MM 或 Xilinx AXI 接口连接全局内存。为了把开发者用硬件描述语言编写的 Verilog 元件集成到系统中，开发者需要编写一个接口相同的空元件作为占位符（或功能相同的高级语言元件用于 CPU 调试与测试），并用 \textbf{verilog} 关键字在 \name 配置文件中声明。编译工具链会将高层次综合工具为占位符元件生成的 Verilog 模块替换为开发者的实现。


\iffalse
\subsection{调试与测试}

\name 提供三种方法用于调试与测试。

\textbf{CPU 功能仿真。}
\name 元件使用类 C 的高级语言编写，因此一个元件可以编译成 CPU 上运行的一个线程，而管道就是线程之间的队列。开发者可以使用熟悉的调试工具来调试功能和单元测试。

\textbf{时钟精确的硬件逻辑仿真。}
元件之间的通信管道有死锁的可能，在 CPU 功能仿真时由于时序与硬件逻辑不一致，不一定能发现问题。为此，\name 可以集成硬件开发者熟悉的硬件逻辑仿真框架，如 ModelSim。为了便于软件开发者使用，\name 封装了 printf 等日志语句。

查看 \textbf{.state} 变量的值……

\textbf{实际 FPGA 运行。}
硬件逻辑仿真速度很慢，不能反映实际性能，且难以测试与 PCIe DMA、网络之间的交互。

日志高速发送：通过 PCIe I/O 通道传输到主机上。

SignalTap

断点如何实现……

\subsection{元件热迁移和高可用}

\textbf{虚拟机热迁移，计算节点对应的网卡状态；网络、存储节点热迁移（升级，扩容等），网卡状态；高可用性，故障恢复……}

\textbf{热升级：不丢包，的时候原来的 FPGA 不处理了，buffer 起来。把旧 FPGA 的状态迁移到新 FPGA，让交换机开始往新 FPGA 的 buffer 里灌，然后从旧 FPGA 的 buffer 里把数据倒出来在新的 FPGA 里处理，处理完后开始处理新 FPGA 自己的 buffer，迁移结束。}

\textbf{高可用：状态机复制的方法，两个 FPGA 收到相同的数据包序列，只要元件内部没有随机化或与时间相关的处理逻辑，就能保证两个 FPGA 收到的数据包序列相同。检测到备份节点故障，只需启动一个新的备份节点，然后执行热升级操作。检测到主节点故障，切换输出到备份节点（可能出现少量数据包丢失或重复，TCP 可以安全地处理这些情况）。}
\fi

\egg{

\egg{
Control signals can only be generated by the manager thread. 
When the manager thread sends a signal to a target element in FPGA, it will embed the ID of the element in the signal message, and 
passes the message to \textit{CmdHub} through slot 32.
\textit{CmdHub} will parse the message and forward the signal request to corresponding elements, again through FIFO buffers. 
}



\egg{
As aforementioned, OpenCL advocates batch processing model where communication between host program and a kernel in FPGA must go through shared DRAM, and the host program cannot control the kernel while it is running.
We could use a special kernel to proxy messages between host program and the kernel via DRAM, but it incurs \approx1ms latency.
DDR access is performed via PCIe link and raw PCIe latency is merely \approx1$\mu$s. As we improve kernel communication efficiency with channels in place of shared memory, we design a host-kernel communication mechanism with channel abstraction for low latency and high throughput.
We leverage I/O channel in Altera OpenCL and AXI stream in Vivado 高层次综合 to connect the Catapult shell to kernels and built a PCIe bypass switch to arbitrate accesses for on-board DRAM and kernel I/O channel.

A PCIe link is split into multiple logically independent \textit{slots} which can operate in parallel.
One slot is reserved for signals. Remaining slots are assigned to channels between host and FPGA elements, so each channel can transfer data in parallel without head-of-line blocking.
On CPU side, each host element is run on a separate core and receives input flits via PCIe by polling or interrupt.
On FPGA side, each element that communicates with host is connected to an inbound demultiplexer and an outbound multiplexer, where load-adaptive batching is performed to improve peak throughput while preserving low latency under light load.
}

\egg{
With polling model, the latency of the PCIe I/O channel  is $< 2 \mu s$ when message size is small, but the latency will reach to $32 \mu s$
for full batched messages.
The interrupt model, however, will increase the latency.
}

\egg{
, where each slot is assigned to one CPU core. Our PCIe channel has two bottlenecks: (1) PCIe Gen2 x8 interface has 32 Gbps bandwidth, (2) PCIe data width is 128b and the clock frequency of Catapult shell is 200 MHz, which limits PCIe throughput to 25.6 Gbps. PCIe I/O channel offers 1\approx2$\mu$s RTT which translates to 400\approx800K host-kernel transactions per second. Polling mode yields lower latency and higher throughput, while interrupt mode latency can be improved by utilizing more cores and PCIe slots. Four CPU cores are sufficient to saturate maximum throughput for both polling and interrupt mode under 64KB batch size. Effectiveness of batching will be further evaluated in traffic dumper application.
}

}
